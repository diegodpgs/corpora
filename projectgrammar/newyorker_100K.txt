<--------->
Books NOVEMBER 2, 2015 ISSUE

Scenes from a Marriage
Lauren Groff’s “Fates and Furies.”

BY JAMES WOOD
SHARETWEET
 11_02_15
TABLE OF CONTENTS
Need to stop reading?

Enter your email address
 GO
We'll send you a reminder of where you left off.
 Groff’s language is precise, lyrical, rich, at once worldly and epically transfiguring.
Groff’s language is precise, lyrical, rich, at once worldly and epically transfiguring.
CREDIT ILLUSTRATION BY VIVIENNE FLESHER
Formally, Lauren Groff’s new novel, “Fates and Furies” (Riverhead), resembles a bed that long marital use has unevenly depressed: it tells the story of an apparently successful marriage from two different perspectives, the husband’s and then the wife’s, and it explores the fierce asymmetry of the two tellings. Essentially, the man’s view of things (a section titled “Fates”) is happy, open, naïvely victorious, and complacent; the woman’s (“Furies”) is secretive, damaged, less happy, and, accordingly, much less complacent. The story’s form not only promises a stereoscopic account of the mythological monad that is marriage but holds the tempting possibility that the angrier second version might modify the easier first one, forcing it out of untruth with corrective revelation.

Lotto (short for Lancelot) and Mathilde meet at a party, near the end of their time as Vassar undergraduates. The attraction is intense, and they get quickly married, just before graduation. The relationship is puzzling to Lotto’s friends: he is a college god, blessed with charm, intelligence, and riches, strapping and handsome (six feet six), a rising young actor. Mathilde is mysterious. She seems to have no legible past, no obvious context. She had no friends at college, and is thought of as an “ice queen” or worse. She is glamorous, but people can’t decide whether she’s beautiful or “interesting-looking.” Temperamentally, the two seem opposed. Lotto thinks her “the purest person he’d ever met,” and later likens her to a saint. This is a characteristically patriarchal gesture: Mathilde seems to ask for little, and subsumes whatever desire for a career she may have had to his larger claims. But Lotto’s praise of her purity also has to do with the holy hygiene, the devoted erasure of Mathilde’s self-presentation. One morning, we are told, “it struck him hard that she had no family at all”:


The little she spoke of childhood was shadowed with abuse. He’d imagined it vividly: poverty, beat-up trailer, spiteful—she implied worse—uncle. Her most vivid memories of her childhood were of the television that was never turned off. Salvation of school, scholarship, modeling for spare change. They had begun to accrete stories between them. . . . How she’d been discovered for modeling by a gargoyle of a man on a train. It must have taken an immense force of will for Mathilde to turn her past, so sad and dark, blank behind her. Now she had only him.
This is as far as Lotto’s curiosity ever takes him, and it is all we hear, in the novel’s first section, about Mathilde’s origins. To Lotto (and to the reader, who sees Mathilde through Lotto’s eyes in this part), she is a successful American tabula rasa: her real life began, conveniently enough for him, when she met her husband.

The couple move to New York (it is the early nineteen-nineties). They are poor (he has been cut off from the family wealth, a penalty for his spousal choice) but happy, heroically bohemian, erotically enchanted with each other. During his twenties, Lotto struggles to make it as an actor, while Mathilde works at an art gallery, earning the regular money. Though naturally ebullient, Lotto, whose father used to say that he would become President or an astronaut, suffers from depression, and starts drinking. A reversal of fortune occurs on New Year’s Eve, 1999, when, in a kind of drunken trance, Lotto stays up all night and in five hours writes a play, “The Springs,” about his tempestuous family background. Mathilde wakes him to tell him that she has read it, that he has found his true talent, and that she has already started editing the manuscript. Lotto, lucky man, appears to remember nothing of his dusky labor. “The Springs” launches the literary career of Lancelot Satterwhite, who goes on to write a series of celebrated plays, emerging as one of the most distinguished dramatists of his day. Mathilde quits the art gallery, and they move to the country, where she keeps house and manages Lotto’s business interests. Thanks to his wife, Lotto never again scrubs a toilet or pays a bill, and smugly boasts—in public, on a literary panel—that his wife “gave up her job years ago to make mine run more smoothly. She loves to cook and clean and edit my work, it makes her happy to do these things.”


Groff is an original writer, whose books are daringly nonconformist; she has a sharp gift for mimesis, yet she also tends naturally toward imagining semi-autonomous worlds. Admirably, she writes inside and outside history at once, refusing to play safe by merely contouring the known. “Arcadia,” her previous novel, convincingly tells the life story of a boy who grows up in the early nineteen-seventies, in a commune in upstate New York. It follows his development all the way to 2018, as he leaves the community of his childhood and joins the larger world. The enclosed, utopian space of Arcadia, with its cultic leaders and its ragged freedoms, is brilliantly brought to life, the details absorbed by the restless, compound eye of an impressionable child. Likewise, “Fates and Furies” refuses to be a conventional domestic novel. Playing with the Greek commands of her title, Groff enlarges (and also reduces) her protagonists. They are sentenced by fate and charged with fury; they are heroic and doomed, modern and ancient, comic and tragic, dramatic and diminished. This tone, essentially mock-heroic, is extremely difficult to maintain, and it can’t be said that “Fates and Furies” finally succeeds in that maintenance. But the first part of the novel, at least, which glorifies and lays bare its golden hero, Lancelot Satterwhite, is consistently surprising and vital. The ornamented names tell us something at once: Lancelot may have been born in Florida, may be the wealthy heir of a water-bottling company named Hamlin Springs, but, with that name and a father called Gawain, he isn’t going to resemble many contemporary Floridians. Lotto’s life will be closer to some epic chanson than to the gray grammar of novelistic realism. His father dies when Lotto is young; it is his atrocious mother, Antoinette—never more than an operatic villain—who cuts off his inheritance when she discovers that he has married the inappropriate and enigmatic Mathilde. But Lotto triumphs anyway.

Cartoon
“Owing to an unforeseen dip in the fiefdom’s population, we regret that we must once again raise taxes.”
BUY THE PRINT »
Groff sows her text with bracketed authorial interventions, in which she plays the role of omniscient Greek chorus, reminding us that she is measuring the thread for her invented spools. Lotto’s progress is regularly interrupted in this way. When he begins what will be a vigorous erotic career, our chorus murmurs, within square brackets, “[Lust! Old story renewed in young flesh.]” When he considers suicide but resists the notion, the author approves: “[True. It was not his time.]” Elsewhere, a minor character is awarded a Nabokovian flash-forward: “[Her death would be soon and sudden. Ski tumble; embolism.]”

Richer and more interesting is Groff’s unbracketed language, which is thrillingly good—precise, lyrical, rich, both worldly and epically transfiguring. Young Lotto, seen cycling from a distance, is a “mantis on his bicycle”; a dog’s erection is “a tube of lipstick all the way extended.” The sound of a swimming pool—“the pool suckled at its gutters.” A lake is “poxed by the touch of scattered rain.” A bus, lowering itself to let people down, “knelt the passengers off like a carnival elephant.” Bubbles “flea-jump” out of the top of a champagne glass. There are many more examples, on page after page. The prose is not only beautiful and vigorously alert; it insists on its own heroic registration, and lifts this story of a modern marriage out of the mundane. Even Lotto and Mathilde’s sex is grand and yet wittily figured: “his wife posting atop him like a prize equestrienne.” Groff mobilizes these stylistic talents to convey that tricky double sense of characters who (for all we know) may or may not be heroic but are certainly heroic in their own estimation.

So it is an enormous shame that the novel’s second half squanders in quick moments what was slowly accumulated in the first half’s careful pages. Reviewers get coy around narrative secrets: spoilers make them tongue-tied. You might imagine, from this novel’s shy reviews, that the second half of “Fates and Furies” functions as a kind of necessary reality check, in which the wife supplants the epic male vision with a more accurate and un-illusioned perspective. That is wanly true. Mathilde reflects on her invisibility as the wife of a famous writer, on Lotto’s egotistical complacency, on how she quietly rewrote half of his plays (“she would silently steal in at night and refine what he had written”). But these references seem halfhearted and novelistically gestural—we have to take on faith the assertion that Mathilde, like one of the mice in Beatrix Potter’s “The Tailor of Gloucester,” mended her husband’s work at nighttime, because the claim is never more plausibly or solidly rendered. The energy of the novel’s second half is not, in fact, torqued toward a furious corrective analysis of the married state. (Or even toward an unfurious one, which would doubtless be as interesting.) Disappointingly, this part of the book becomes a lurid fairy tale whose heroine is not so much furious as a Fury, not so much disillusioned as a Devil.

Beware: I’m unafraid to host a big spoiler party—a novel that can be truly “spoiled” by the summary of its plot is a novel that was already spoiled by that plot. At the end of the first section, Lotto dies. He is forty-six, the age of his father’s death. In Part 2, we turn to the story of glamorous and inaccessible Mathilde—who, we learn, was born in France, as Aurélie, her mother a fishwife in Nantes, her father a stonemason. When she is four, she effectively kills her brother (by smilingly encouraging him to fall down the stairs), and is banished by her parents: sent first to a chilly grandmother in Paris (where she sleeps in a closet for six years) and then, at the age of eleven, to a nasty uncle in Pennsylvania. This uncle informs her that he won’t often be at home, and that his driver will look after her needs. Alone, Aurélie learns English by watching TV and changes her name to Mathilde: “Like that, all at once, Mathilde grew up over Aurélie’s skin.” All the rooms in the house are locked, save for her bedroom. But one day the uncle accidentally leaves open a small room under the stairs, where Mathilde discovers a beautiful painting that turns out to be a stolen van Eyck. Later, Mathilde will—in short order—pay for her education at Vassar by prostituting herself to a wealthy art dealer; pay for the first performance of “The Springs” by blackmailing her uncle; get pregnant by Lotto and arrange an abortion (because she is convinced that her children will have fangs and claws). And there’s more: after her husband’s death, Mathilde will sleep with a handsome actor named Land, who will turn out to be Lotto’s son, conceived by his first girlfriend, in Florida, when she was seventeen and Lotto was only fifteen.

The point of this cruel outing is not merely to illuminate the heaped incredibilities (which awkwardly subsist within a broadly realist register); or only to suggest that Groff is flailing here, reaching for whatever motifs she can stuff into the vessel—Greek tragedy, “Bluebeard’s Castle” (locked rooms), “The Secret Garden” (horrid banished daughters), “Rumpelstiltskin” (erotic contracts). I find these melodramatic accelerations—“Like that, all at once, Mathilde grew up over Aurélie’s skin”—humanly untruthful (is that how it happens, like that, all at once?) and thus a kind of vandalism of the novel form. But tastes in unreality differ. The acute problem is not so much improbability as eccentricity. Mathilde never told Lotto about any of it (she “made a promise that he would never know the scope of her darkness”)—not the dead brother, or the French childhood, or the rewriting of his plays, or the abortion. The extremity of Mathilde’s suffering makes her repression of it less interesting than a more ordinary version of such self-control (she doesn’t really have a self, so its repression just makes her a double negative); and Lotto’s shortsighted complacency is also suddenly less interesting, because less culpable (we can’t usefully judge him for not knowing what was strenuously kept from him).

Thus the novel hobbles its power to speak of marriage in general. Indeed, far from telling us something suggestive about the desires, different and shared, of two genders, the rapidities and savageries of the second half run the risk of drowning gender in the purest essence of fable: the man belongs to the Fates, the woman (or “devil girl,” as she is called) to the Furies; if it was the man’s fate to have married a Fury—so this narrative logic seems to go—it is the woman’s fate to be a Fury.

The “revelation” of the novel’s second half, far from binding the form in meaning, is the thread that fatally unravels it. Narrative secrets are not the same as human mysteries, a lesson that novelists seem fated to forget, again and again; the former quickly confess themselves, and fall silent, while the true mysteries go on speaking. ♦

<--------->
Books NOVEMBER 2, 2015 ISSUE

The Price of Union
The undefeatable South.

BY NICHOLAS LEMANN
SHARETWEET
 11_02_15
TABLE OF CONTENTS
Need to stop reading?

Enter your email address
 GO
We'll send you a reminder of where you left off.
 Progress in civil rights has been matched by the Southernization of American politics.
Progress in civil rights has been matched by the Southernization of American politics.
CREDIT PHOTOGRAPH BY WALKER EVANS / COURTESY LIBRARY OF CONGRESS
When the Confederate States of America seceded, the response of the United States of America was firm: dissolving the Union was impermissible. By contrast, it took a few more years for the United States to resolve the question of whether it would permit slavery within its own borders, and it took more than a century for the U.S. to enforce civil rights and voting rights for all its citizens. This was mainly because of the South’s political power. In order to become the richest and most powerful country in the world, the United States had to include the South, and its inclusion has always come at a price. The Constitution (with its three-fifths compromise and others) awkwardly registered the contradiction between its democratic rhetoric and the foundational presence of slavery in the thirteen original states. The 1803 Louisiana Purchase—by which the U.S. acquired more slaveholding territory in the name of national expansion—set off the dynamic that led to the Civil War. The United States has declined every opportunity to let the South go its own way; in return, the South has effectively awarded itself a big say in the nation’s affairs.

The South was the country’s aberrant region—wayward, backward, benighted—but it was at last going to join properly in the national project: that was the liberal rhetoric that accompanied the civil-rights movement. It was also the rhetoric that accompanied Reconstruction, which was premised on full citizenship for the former slaves. Within a decade, the South had raised the price of enforcement so high that the country threw in the towel and allowed the region to maintain a separate system of racial segregation and subjugation. For almost a century, the country wound up granting the conquered South very generous terms.

The civil-rights revolution, too, can be thought of as a bargain, not simply a victory: the nation has become Southernized just as much as the South has become nationalized. Political conservatism, the traditional creed of the white South, went from being presumed dead in 1964 to being a powerful force in national politics. During the past half century, the country has had more Presidents from the former Confederacy than from the former Union. Racial prejudice and conflict have been understood as American, not Southern, problems.

Even before the Civil War, the slave South and the free North weren’t so unconnected. A recent run of important historical studies have set themselves against the view of the antebellum South as a place apart, self-destructively devoted to its peculiar institution. Instead, they show, the South was essential to the development of global capitalism, and the rest of the country (along with much of the world) was deeply implicated in Southern slavery. Slavery was what made the United States an economic power. It also served as a malign innovation lab for influential new techniques in finance, management, and technology. England abolished slavery in its colonies in 1833, but then became the biggest purchaser of the slave South’s main crop, cotton. The mills of Manchester and Liverpool were built to turn Southern cotton into clothing, which meant that slavery was essential to the industrial revolution. Sven Beckert, in “Empire of Cotton,” argues that the Civil War, by interrupting the flow of cotton from the South, fuelled global colonialism, because Europe needed to find other places to supply its cotton. Craig Steven Wilder, in “Ebony & Ivy,” attributes a good measure of the rise of the great American universities to slavery. Walter Johnson, in “River of Dark Dreams,” is so strongly inclined not to see slavery as simply a regional system that he tends to put “the South” in quotes.

After slavery had ended and Reconstruction gave way to the Jim Crow system, the Democratic Party was for decades an unlikely marriage of the white South (the black South effectively couldn’t vote) and blue-collar workers in the North. This meant that American liberalism had a lot of the South in it. Ira Katznelson, in “Fear Itself,” adeptly identifies the deep Southern influence on the New Deal era, the country’s liberal heyday, including not just its failure to challenge segregation but also a strong pro-military disposition that helped shape the Cold War. The great black migration to the North and the West, which peaked in the nineteen-forties and fifties, partly nationalized at least one race’s version of Southern culture, and, by converting non-voters to voters through relocation, helped generate the political will that led to the civil-rights legislation of the nineteen-sixties. Once those laws had passed, the South became for the Republican Party what it had previously been for the Democratic Party, the essential core of a national coalition. The South is all over this year’s Republican Presidential race.

I’m a fifth-generation Southerner, though long expatriated, and I know the wounded indignation with which the folks back home react to any suggestion that the South is no longer—or maybe never was—an entirely separate region. What about our hound dogs, our verandas, our charm, our football worship, our slow-moving “way of life”? Outsiders who have visited the South, going back to Alexis de Tocqueville and Frederick Law Olmsted or even further, have usually agreed with the natives about the South’s distinctiveness, though they have often seen it as something to condemn, not admire. How can the South be so American if it feels (and smells, and sounds, and looks) so Southern?

One of the many categories of visitors to the South was concerned liberals during the New Deal, who were primarily interested not in race but in “conditions”—poverty, disease, ignorance. These included the documentary photographers dispatched by the federal government’s Farm Security Administration, who wound up creating most of the familiar images of the Depression, as well as anthropologists, sociologists, journalists, social reformers, artists, and filmmakers. James Agee and Walker Evans’s lugubrious book “Let Us Now Praise Famous Men” is one of the most enduring examples of this tradition. (The 1941 Preston Sturges film “Sullivan’s Travels” manages the nearly impossible feat of poking fun at such visitors while also making it clear that their mission had a powerful moral justification.) During the same period, white Southern novelists produced their own body of work that trafficked in Southern dispossession and dysfunction. William Faulkner was at the head of this class, which also included Erskine Caldwell (who was part of the social-documentary tradition, too, through his professional and personal partnership with Margaret Bourke-White) and, later, Carson McCullers and Flannery O’Connor.

Paul Theroux, the veteran travel writer, seems to have prepared for “Deep South: Four Seasons on Back Roads” (Houghton Mifflin Harcourt), the first of his ten travel books set in the United States, by immersing himself in these works from the second quarter of the twentieth century. The genre in which he is working naturally organizes itself into vignettes rendered with a primary focus on literary artistry, rather than analysis, so he never has to state a full-dress argument, or even say exactly what he was looking for in those four long driving tours. The South remains more rural than the Northeast, but by now, as in the rest of the country, most people live in metropolitan areas. Still, Theroux tells us, “I stayed away from the big cities and the coastal communities. I kept to the Lowcountry, the Black Belt, the Delta, the backwoods, the flyspeck towns.” This principle may have been a way of simplifying his writing assignment: these are places where some people eat squirrels and raccoons, and are obviously unusual in a way that people in the Atlanta suburbs are not. That makes them easier to portray vividly. But Theroux is left trying to evoke the fastest-growing region of the country, where a hundred and twenty million people live, by taking us to a series of poor, deep-rural, depopulated places, like Hale County, Alabama; the Mississippi Delta; and the Ozarks, where the main noticeable changes in the past few decades are outsourcing and the advent of Gujarati Indians as motel owners.

V. S. Naipaul, Theroux’s former mentor, wrote quite a similar book twenty-six years ago, called “A Turn in the South.” Naipaul, never one for sentimentality about oppressed people, wound up celebrating “the redneck” (you have to have pale skin to have a red neck) as the South’s heroic type. Theroux thinks of himself as a liberal, and he doesn’t go anywhere near defending the white South’s politics and attitudes. On the other hand, he also doesn’t want to play the part of the disapproving or sneering Northerner. National culture, these days, seems to connect with the part of the South that Theroux visited through rollicking reality-television carnivals like “Duck Dynasty” and “Here Comes Honey Boo Boo.” Theroux strikes an empathetic, mournful tone rather than a mocking one. The people he visits are older, settled. Many of them either work in or are clients of social-service and community-development agencies. More are white than are black. He often compares the rural South—“rotting, picturesquely hopeless, forgotten”—to the underdeveloped parts of sub-Saharan Africa, which he has been visiting intermittently since he was a Peace Corps volunteer in Malawi, in 1963, and he regularly complains that the South gets far less attention from big philanthropies and the like. (He’s especially annoyed that the Clinton Global Initiative evinces so little interest in the poorest regions of Bill Clinton’s home state.)

Cartoon
“Are we there yet?”
BUY THE PRINT »
In a final, confessional section, Theroux connects the book’s project to his own stage in life. At seventy-four, he finds himself contemplating the past more than the future, and wonders whether the onrushing world has left him behind. Where better to entertain such thoughts than in Allendale, South Carolina, a ghostly town bypassed by the interstate-highway system? But this turn of mind leads him inexorably to an implied theory of the South as, indeed, a region radically apart. Throughout the book, he registers the South’s religiosity and its preoccupation with guns as products of its degraded status, rather than of a culture that has always been more pious and more martial than the rest of the country’s. On one of several visits he makes to gun shows, during which he tries hard to understand rather than to condemn, he observes, “The whites felt like a despised minority—different, defeated, misunderstood, meddled with, pushed around, cheated.” His final judgment on the South, delivered at the end of the book, is this: “Catastrophically passive, as though fatally wounded by the Civil War, the South has been held back from prosperity and has little power to exert influence on the country at large, so it remains immured in its region, especially in its rural areas, walled off from the world.”

Even if you believe the South is that separate from the rest of the country, you might still, if you look hard enough, detect tendrils of Southern influence that extend past the Mason-Dixon Line. Race provides the obvious example. The slave states developed an elaborate and distinctively American binary racial system, in which everybody across a wide range of European origins was put into one category, white, and everybody across a wide range of African origins (including those with more white forebears than black forebears) was put into another category, black. These tendentious categories have been nationalized for so long that they seem natural to nearly all Americans. They are Southern-originated, but not Southern. They powerfully determine where we live, how we speak, how we think of ourselves, whom we choose to marry. They are deeply embedded in law and politics, through the census, police records, electoral polling, and many other means.

A frequent companion of the idea of a simple distinction between black and white is the idea of a simple distinction between racists and non-racists. There can’t be anybody left who believes that racists exist only in the South, but there are plenty of people, especially white people, who believe that racism is another simple binary and that they dwell on the better side of it. Paul Theroux marvels that Strom Thurmond, the old South Carolina arch-segregationist, fathered an out-of-wedlock black child. “Funny that a racist like Thurmond would have an affair with his black servant,” he remarks to someone he’s visiting. Come on! It’s visually evident how often this happened—“racism” as manifest in a sense of sexual entitlement, rather than of revulsion. Theroux himself displays an uncharacteristic electric jolt of resentment on the rare occasions when he contemplates urban black culture. In one passage, he refers to “the obscene, semiliterate yawp and grunt of rap,” and, in another, he describes a well-dressed black-bourgeois group he encounters at an event in Little Rock as being “like a shoal of leathery sharks” who are “suspicious, chilly, with a suggestion of hauteur in their greeting, as if they were still learning how to deal with whites.”

Ari Berman’s “Give Us the Ballot” (Farrar, Straus & Giroux), a history of the 1965 Voting Rights Act, makes for an excellent extended example of the mechanisms by which race in the South becomes race in the nation. The Voting Rights Act followed the better-known Civil Rights Act by a year. It is properly understood as part of a wave of legislation that represents the political triumph of the civil-rights movement, but Berman, like most people, finds a precipitating event in the murder, in June, 1964, in Neshoba County, Mississippi, of three young civil-rights workers, James Chaney, Andrew Goodman, and Michael Schwerner.

Chaney, Goodman, and Schwerner’s mission was voter registration—hence their connection to the Voting Rights Act. It’s sad but true that their murders would not have resonated so deeply if Goodman and Schwerner had not been whites from New York who had come South to participate in Freedom Summer. In fact, the grassroots organizing on behalf of voting rights was substantially black and Southern. Just before Freedom Summer, the congregation of Mt. Zion Methodist Church, in the all-black Neshoba County town of Longdale, had voted to make its church the local headquarters of the movement’s voter-registration efforts. A few days before the murders, the Ku Klux Klan burned the church down, because of the role it was playing. Chaney, Goodman, and Schwerner were on their way back from a trip to Longdale to investigate the fire when they were killed.

“One Mississippi, Two Mississippi” (Oxford), by Carol V.R. George, a history of the Mt. Zion church, makes plain how essential the church was to the local civil-rights struggle. It was organized, with the help of Northern whites, during the period when the citizenship of former slaves was being rescinded, with the end of Reconstruction. For decades, its members were involved in every possible effort to reinstate the rights of blacks in Neshoba County, including the years of relentless activity that preceded Freedom Summer. And, after the church was rebuilt, it was deeply engaged in the long struggle to bring to justice one of Chaney, Goodman, and Schwerner’s killers, Edgar Ray Killen, whom an all-white Neshoba County jury refused to convict in 1967. That took until 2005.

So the passage of the Voting Rights Act was actually a North-South partnership, not an imposition of the North’s will on the South. And it would be a big mistake to think of the act as a great, enduring civil-rights milestone, representing the country’s belated decision to comply fully and everywhere with the Fifteenth Amendment to the Constitution. As Berman demonstrates, the act has been, instead, the subject of half a century of ceaseless contention, leaving its meaning permanently undetermined. Most of the consequential fights about civil rights, beginning with the Reconstruction-era amendments to the Constitution, have been over the federal government’s role in enforcement. The Voting Rights Act gives Washington the power to review local voter-registration practices, and to change the boundaries of election districts in areas that have a history of discrimination or that appear to be drawing district lines so as to minimize the number of black elected officials. But the act, as written, invites conflict because its enforcement provisions come up for periodic congressional review.

Every few years, there has been a serious attempt to discontinue these enforcement provisions. Berman makes a persuasive case that the ongoing battles over the reviews of the Voting Rights Act, beginning with the first one, in 1970, have had a major impact on who has held political power. Periods of aggressive enforcement have produced more black voters and more liberal (especially black) elected officials—including, Berman suggests, Barack Obama—and also the potential for conservative politicians to take advantage of white resentment of the Voting Rights Act.

In August of 1980, Ronald Reagan chose to kick off his general-election Presidential campaign at the Neshoba County Fair, in Mississippi, not far from where Chaney, Goodman, and Schwerner were murdered, and to declare, “I believe in states’ rights.” Once Reagan was in office, there was a battle over the terms of one of the Voting Rights Act’s periodic extensions, in which a significant actor was John Roberts, then a young lawyer at the Justice Department and now the Chief Justice. Berman has found in the National Archives a set of memos that Roberts wrote in 1981 and 1982, demonstrating a passionate opposition to aggressive enforcement of the Voting Rights Act. Three decades later, in the case of Shelby County v. Holder (2013), Roberts led a Supreme Court majority that struck down the major enforcement provision of the act, arguing that the problem the act was passed to correct has long since been solved. This will help Republicans in subsequent elections, including the 2016 Presidential election.

At passage, the Voting Rights Act appeared to be only about the South, but over the years it has regularly been applied elsewhere. Politics is racial, to some extent, in most places; it was impossible to keep such a major law from having national repercussions. Among the states that have now passed election laws in direct response to the Shelby decision are Arizona, Wisconsin, and Ohio. The same dynamic—in which a “regional” issue goes national—repeats itself in just about every realm: not just in politics but also in culture, business, social mores.

“It will become all one thing or all the other,” Abraham Lincoln declared of the beleaguered, slavery-stressed Union, in his “House Divided” speech. In fact, the South and the rest of the nation have one of those hot-blooded relationships—the major one, in American history—which never settle into either trustful intimacy or polite distance. The South is too big and powerful to be vestigial; too married to the rest of the country to stand truly apart; too distinctive in its history to be fully united with the other states. Colin Powell, back in the days when, as Secretary of State, he was voicing skepticism about the Iraq War, used to say, “If you break it, you own it.” That seemed true for a while in Iraq, but, being halfway around the world, Iraq wasn’t so hard to leave. The Union’s defeat of the Confederacy makes for a better example. ♦

<--------->
Books NOVEMBER 2, 2015 ISSUE

Boston Boys
The poetry of John Wieners and John Updike.

BY DAN CHIASSON
SHARETWEET
 11_02_15
TABLE OF CONTENTS
Need to stop reading?

Enter your email address
 GO
We'll send you a reminder of where you left off.
 Wieners was a bard of Boston’s decay, on the hunt for urban transcendence.
Wieners was a bard of Boston’s decay, on the hunt for urban transcendence.
CREDIT ILLUSTRATION BY STUART PATIENCE; SOURCE: CHRIS FELVER / GETTY
The poet John Wieners lived, from the early nineteen-seventies until his death, in 2002, at the age of sixty-eight, at 44 Joy Street, in Boston, on the back slope of Beacon Hill, where disembowelled couches and gooseneck lamps are abandoned curbside on the first of every month. A few blocks over, and a world away, Robert Lowell spent his childhood at 91 Revere Street; up the hill was the site of the house where Henry Adams had grown up “under the shadow of the Boston statehouse.” Wieners was part of a small enclave of dropouts and artists in that part of Boston, now replaced by medical students and first-year associates. New York poets ran a thriving mid-century avant-garde, led by Frank O’Hara; San Francisco had the Beats. Wieners dipped into both scenes and returned to Boston, the backwater where he was born. “The detergent of bourgeois Boston cleans everything,” Elizabeth Hardwick wrote, in “Boston: The Lost Ideal,” one of the great literary disses of a major city. “If there were a Bohemia, its members would indeed live on Beacon Hill.”

I was one of the non-bohemians who lived on the Hill in the nineteen-nineties, and I kept a keen eye out for Wieners, who was by then a folkloric creature, like the yeti or the headless horseman. Pictures of him from that era do not convince me that I never glimpsed him: he looks familiarly lost, generically grizzled, just like the homeless men who foraged under the Charles Street T station and got into wild, operatic quarrels while Wieners lingered “opposite the elevated railroad / tracks / at Cambridge street & Charles / when every hope burns to stinking inconsequence.”


Whenever Wieners was asked what sort of poet he was, he replied, “A Boston poet.” It was a polemical response, since “Boston poet” suggested people like Lowell and his students in the Boston University Creative Writing Program, Sylvia Plath and Anne Sexton: genteel, attractive, straight, unstrung, the so-called confessional poets, who sojourned at the McLean psychiatric hospital, in Belmont, among what Lowell called the “Mayflower screwballs.” Lowell wrote that he and Hardwick, along with their daughter, Harriet, “hog a whole house” on Marlborough Street: it was “nothing if not pretentious,” as he told his friend Elizabeth Bishop. Wieners, who commuted from his parents’ home, in Milton, Massachusetts, to Boston College, was totally unfamiliar with the city’s élite institutions; he was a stranger to Harvard, even though he frequented the Grolier Poetry Book Shop, situated right on its hem. He was bisexual, unruly, squalid, on and off heroin and other drugs for much of his life; the “tracks” he mentions in those lines about Cambridge Street aren’t merely railroad ties. The confessional poets were messes you couldn’t take your eyes off. Wieners was all but invisible.

The power of Wieners’s poems, as the new collection “Supplication: Selected Poems of John Wieners” (Wave Books), edited by Joshua Beckman, Robert Dewhurst, and CAConrad, demonstrates, is partly anthropological, which is not a failing; poetry has a special way—a brilliant way—of doing anthropology. It takes in the social world through the senses and processes it through the emotions. Add passing time to the mix, and you have the peculiar elegiac immediacy of Wieners’s work, which calls to mind a Boston already on its way out when Wieners was chronicling it: seedy gay bars full of “potato chips and boys,” where “James the bartender sweeps / bottles off the bar,” or, gorgeously, a “Howard Johnson’s / beyond the blue horizon.” The voice of the poems is solitary, pining for the “mad weirdos and high-jinx” of New York, while cruising the barren Esplanade by the Charles River, where, Wieners writes, “I was fucked / on the overpass by a student / while hundreds of cars raced by / below, unknowing of our ecstasy.” He is like O’Hara with the lights dimmed, the music shut off, and all his friends (O’Hara among them) living in New York. It makes him permanently desperate for transcendence. He is the bard of feeling marooned in Boston.

The poems are unbelievably dirty, in both senses of the word, and often in the two senses simultaneously. But poems about blow jobs in bathrooms and alleyways are not simply about sex; they survey an alternate urban geography with its own subversive vistas and attractions, from the Esplanade to the Public Garden after dark. These city poems are part scavenger hunt and part missing-persons report, as various lovers drift vividly in and mysteriously out of sight. In “Billie,” a mystical “god” shows up and, like Zeus absconding with Ganymede, takes Wieners’s “girl” and his heart along with him:

They disappeared beside the sea
at Revere Beach as
I aint seen them since.
 
If you find anyone
answering their description
please let me know. I need them
 
to carry the weight of my life
The old gods are gone. What lives on
in my heart
 
is their flesh
like a wound,
a tomb, a bomb.
The poem’s “description” is, of course, emotional and interior. This isn’t a forensics sketch; it’s a lyric poem. Prayers and personal ads are answered only when a person draws himself, his own needs and prerogatives, so movingly that he conjures an “answering” presence where none existed. This is also what poems do, always leveraging old losses to reap the next harvest of fresh and unforeseen gains. The poems about ex-lovers feel as though they were used to attract new recruits, but the tang of heartbreak, the anticipation of loss, colors even Wieners’s happy poems. In his hands, poems are at once “wound,” “tomb,” and “bomb”—sites of injury, elegy, and threat.

His many poems about sex, his celebrations of heroin, peyote, and cocaine, and his reverence—in poems that make gritty and real the heart’s imperatives—for an idealized poetry so different from the facts on the ground make Wieners (born a Catholic, and, like many lapsed Catholics, prone to beliefs that fill in the blank) a devotional poet. The world is materialized spirit; poetry is, in Wieners’s view, “the most magical of all the arts,” because it creates a “life-style for its practitioners, that safeguards and supports them.” It is an odd and utterly circular defense of poetry, a little like saying that the primary virtue of medicine is that it gives doctors something to do, or that ballet was invented because ballet dancers needed it. But, for poetry, the definition actually works: we routinely ascribe to poets an innate capacity for insight and imaginative transport, best but not exclusively expressed by the actual poems they write. Poetry is “magic,” too, because, Wieners wrote, “things change in proximate location” to its “aura” or “romantic glow”; we are enchanted as by a “children’s story.” I don’t see it this way, but I am moved that Wieners, a strange and suffering man, seemed to believe it. It is hard to imagine anything other than poetry keeping him alive for so long. His afterlife exists in the form of these poems, a mental Boston, eerily lit by neon and street lights, through which the rest of us wander.

No two human beings seem more different than Wieners and John Updike, a contemporary and a fellow-denizen of greater Boston, and yet the parcelling of the world into overlapping zones of propriety and perversity, and the use of poetry as a means of travelling between those regions, unite them. It is a common judgment of Updike that he seems to have written poems with one hand tied behind his back, while golfing and having sex with someone’s wife at the same time. There’s no reason to add to the slander and vitriol that get heaped upon Updike’s poems, mainly by upstart brats like me, every time an edition of them appears. Maybe it’s being middle-aged myself, or my memory of seeing the great man walking alone on the beach, not long before he died, looking serene, even beatific, but Updike’s “Selected Poems” (Knopf), edited by Christopher Carduff, strikes me as a book that anybody who loves Updike, or poetry, or Cape Ann—or, for that matter, golf or sex—should read. That should cover more or less everybody.

Perhaps because poetry’s readership is smaller than fiction’s, or because one can always hide behind the screen of forms and personae (or shame someone who peers behind the screen), Updike’s poems seem, if anything, more intimate than his sometimes uncomfortably confiding fiction, as though even the very light disguises of his most revealing books—“Couples,” or the late novel “In the Beauty of the Lilies,” for example—were shed. I suspect that Updike liked the paradox of writing so nakedly in a style nevertheless quilted with exquisite verbiage. If you’ve decided that poetry is really life steeped or fermented in language, you have to find examples of life ever plainer, uglier, more prone to being skipped by literature. But the additive of Updike’s style works in only one way, upping the quotient of mellow beauty or manageable melancholy wherever we go, from Vermont to Waikiki, and whatever is described, whether forsythia or Frankie Laine. His poems sometimes feel like adjuncts of his stories, evincing a fiction writer’s delight in details that exist outside the crewelwork of storytelling; crumbs, one-offs, outtakes. A poem can suspend in its own substance moments in time that, in a short story or a novel, would be jettisoned unless they strengthened the over-all narrative web.

Updike’s poems are not trifles; he could be surprisingly formally ambitious, even experimental. The problem is that all of his poems about strain, discomfort, and regret cheer him, and we don’t associate cheer with great poetry. The poems often feel like the by-products of the happy diversion they provided their author while he was writing them, an effect most striking when it seems least intended. “Midpoint” is his poem of Whitmanesque capaciousness, weaving quotations from Whitman into a fabric made of heterodox elements—typographical spasms, grainy photographs. The poem doesn’t work; the farther Updike strays from his native styles and subjects, the more we suspect that style—in this case, difficult, modernist collage—was something he put on with irritating ease. His best poems are mild evocations of local eccentricity, seasonal anomie, domestic frisson. You never need to imagine their contexts, which tend to get spelled out plainly in the titles. “Topsfield Fair” is about the Topsfield Fair; “Living with a Wife” is about living with a wife. And he didn’t much change his tone when moving from descriptions of poultry to descriptions of women. Animals are “sad to be themselves”: the prize steer is “humble in his stall,” and the turkey is “a turkey even to his wattle.” They are our “fellows in mortality,” which means they’ve been declawed, domesticated. So, too, the husband who muses upon his wife’s “underwear set to soak / in the bowl where I brush my teeth.”

Updike’s poems level our intrinsic ranking of occasions. “The Beautiful Bowel Movement” and “Fellatio” and “Rats” and the Phi Beta Kappa poem, “Apologies to Harvard,” are not so different from one another, while bowel movements, fellatio, rats, and Harvard in fact are. Everywhere, the ingenious adjective turns up to alter its noun, where “adjective” stands for the imagination and “noun” for reality prior to aesthetic transformation. This formula is so consistent as to render its local applications interchangeable. An “unchurched grandma” in a “foursquare house” might as well be a foursquare grandma in an unchurched house. The verbs all seem chosen from a list written in marker on a cinder-block wall, or taken from a word-of-the-day calendar. Vocabulary is the most overrated element of good writing, or so these poems tempt us to conclude.

Updike loved writing so much that he couldn’t help himself from doing it whenever possible. The poems do not slow, or substantially darken, when he learns of his terminal illness, but the formula has a new urgency and poignancy. The bad news strikes in a long sonnet sequence, “Endpoint,” already in progress. Suddenly, Updike’s subject—his true subject, the life of comfort and security and satisfaction that writing brought him—rises to the surface. I remember reading some of these poems in this magazine, and marvelling at their authenticity, as though every other word in those issues was concealing its glee in appearing alongside them. Updike remembers the pleasure of getting checks marked with a Borzoi or an image of Eustace Tilley, in the days before Bertelsmann and Condé Nast:

Back then, my children, in those simpler years
before all firms were owned by other firms,
the checks would come imprinted with a dog,
a bounding Borzoi, or the profile of
a snooty figment, Eustace Tilley. He
was like a god to me, the guardian
of excellence; he weighed my mailed-in words
and paid a grand or so for tales he liked.
 
A thousand dollars then meant we could eat
for months. A poem might buy a pair of shoes.
My life, my life with children, was a sluice
that channeled running water to my pan;
by tilting it, and swirling lightly, I
at end of day might find a fleck of gold.
Any recollection of the writing life that omits this pleasure is on some level dissembling. “Endpoint” is a perfect sonnet sequence, and as great and weirdly transparent an assessment of dying as the final suite of poems left by a far greater poet, James Merrill, in his own last days. Poets die older now than they once did, and their concluding phase is lengthened by machinery and drugs. They’ve come down from Parnassus to the modern hospital room, as in “Needle Biopsy 12/22/08”:

All praise be Valium in Jesus’ name:
a CAT-scan needle biopsy sent me
up a happy cul-de-sac, a detour not
detached from consciousness but sweetly part—
I heard machines and experts murmuring about me—
a dulcet tube in which I lay secure and warm
and thought creative thoughts, intensely so,
as in my fading prime. Plans flowered, dreams.
Updike’s last poems—“not / detached from consciousness but sweetly part”—will not be a footnote to his career; I suspect they’ll be paying royalties far into the future. ♦

<--------->
Books OCTOBER 26, 2015 ISSUE

Shot in the Heart
When Yitzhak Rabin was killed, did the prospects for peace perish, too?

BY DEXTER FILKINS
SHARETWEET
 10_26_15
TABLE OF CONTENTS
Need to stop reading?

Enter your email address
 GO
We'll send you a reminder of where you left off.
 A portrait of Rabin at the November 12, 1995, memorial ceremonies in Tel Aviv.
A portrait of Rabin at the November 12, 1995, memorial ceremonies in Tel Aviv.
CREDIT PHOTOGRAPH BY A. ABBAS / MAGNUM
Assassination is an unpredictable act. Historically speaking, high-profile political killings have been as likely to produce backlashes and unintended consequences as they have been to achieve the assassin’s goal, if he had one. When Lee Harvey Oswald killed President Kennedy, the result was an outpouring of national soul-searching, which Lyndon Johnson took advantage of to push civil-rights and Great Society legislation through Congress. When Syrians conspired to murder Rafik Hariri, the former Lebanese President, in 2005, the result was not continued Syrian domination of Lebanon but a national uprising followed by a humiliating evacuation of Assad’s forces.

Yet the killing of Yitzhak Rabin, the Israeli Prime Minister, in 1995, by Yigal Amir, an Israeli extremist, bids to be one of history’s most effective political murders. Two years earlier, Rabin, setting aside a lifetime of enmity, appeared on the White House lawn with Yasir Arafat, the leader of the Palestine Liberation Organization and a former terrorist, to agree to a framework for limited Palestinian self-rule in the occupied territories; the next year, somewhat less painfully, he returned to the White House, with Jordan’s King Hussein, to officially end a forty-six-year state of war. Within months of Rabin’s death, Benjamin Netanyahu was the new Prime Minister and the prospects for a wider-ranging peace in the Middle East, which had seemed in Rabin’s grasp, were dead, too. Twenty years later, Netanyahu is into his fourth term, and the kind of peace that Rabin envisaged seems more distant than ever.

The story of Rabin’s assassination, told in “Killing a King” (Norton), by the journalist Dan Ephron, inevitably raises the question of what might have been. At the time of his death, Rabin showed every intention of trying to forge a broader peace that would have included ceding most of the occupied territories to the Palestinians, and probably would have resulted in the establishment of an independent state.

Rabin, who was seventy-three when he died, spent most of his life fighting the Palestinians. Born in British-ruled Palestine, he was brought up by secular, socialist immigrants from Eastern Europe. His mother, Rosa, was one of the most important female Zionist leaders of her time; she was apparently so consumed with the cause that Rabin grew up feeling mostly alone. The experience, according to one of his biographers, Dan Kurzman, may have contributed to his intense self-containment, which often made him seem aloof. (Once, at the White House, Jimmy Carter asked him if he would like to listen to his daughter, Amy, play the piano. Rabin replied that he would not.)

When he was a teen-ager, Rabin joined the Palmach, a commando unit of the Haganah, the Zionist militia, and was twenty-five when, in 1947, the United Nations voted to partition Palestine. The partition plan demarcated the boundaries of Jewish and Arab territories; the U.N. envisioned a two-state solution from the start. This led, in May, 1948, to the founding of Israel, which prompted a full-scale attack by the armies of the surrounding Arab states. In battle against the Arabs (and, before that, the British), Rabin proved himself to be a daring and courageous fighter. But he also took part in the expulsion of some fifty thousand Palestinian residents from the towns of Lydda and Ramle, situated between Tel Aviv and Jerusalem. Several hundred villagers were shot during that operation, part of a wider exodus of Palestinians from the new Jewish state. Rabin was also involved in a bloody, and eerily foreboding, incident that took place within weeks of independence, and involved the Irgun, an extremist guerrilla group that had broken off from the Haganah. When a cargo ship carrying weapons for the Irgun tried to dock, Rabin, then a commander in the newly formed Israeli Defense Forces, ordered the soldiers to open fire. Sixteen Irgun fighters were killed; the group’s leader, Menachem Begin—later the Prime Minister—was carried ashore by his men.

After independence, Rabin focussed on building the I.D.F.; his animating vision—like that of many Israeli leaders since—was that peace would be possible only when Israel achieved military superiority over any combination of Arab foes. As a commander, Rabin felt responsible for the lives of his soldiers; he was also physically repelled by the sight of blood. In the run-up to the Six-Day War, in 1967, as the Arab armies were gathering to attack Israel, Rabin, at that time the I.D.F. chief of staff, suffered a nervous collapse. He considered stepping down, but pulled himself together and oversaw Israel’s sweeping victory. “I had to hold his balls,” his deputy, Ezer Weizman, said. The Six-Day War made Rabin a national hero, and left Israel in possession of the West Bank, Gaza, East Jerusalem, parts of Syria, and more than a million Palestinians.

Ephron’s book doesn’t speculate about the degree to which Rabin, in his early years, might have imagined the possibility of a broader peace. He and the Labor Party governments bear nearly as much responsibility as their Likud successors for expanding settlements in the West Bank. Nor does the younger Rabin appear to have considered the likelihood of a Palestinian state. In the late nineteen-eighties, when he was the Minister of Defense, he presided over the response to the first intifada—a full-scale Palestinian uprising—during which he was quoted as ordering the I.D.F. to “break the bones” of protesters. (Rabin denied saying this.) For most of his career, he regarded the P.L.O., which had carried out bus bombings and plane hijackings, as “liars and bastards.” But the experience of the intifada seems to have convinced him that the status quo was unsustainable. “I’ve learned something in the past two and a half months,” Rabin told a group of Labor Party colleagues in 1988. “Among other things, that you can’t rule by force over one and a half million Palestinians.”

The chance to break with the status quo didn’t come until the early nineteen-nineties, when a semi-official group of Israelis, on the initiative of Norway’s deputy foreign minister, reached a tentative understanding with P.L.O. representatives regarding what amounted to a plan for limited Palestinian self-rule in the occupied territories and an Israeli withdrawal from Gaza and part of the West Bank. (Rabin considered most of the settlers who began streaming into the West Bank shortly after the Six-Day War to be misguided, along with government programs designed to encourage them. He did think that some outposts were essential to Israeli security, and therefore important to hold on to.) Rabin, who had become Prime Minister for a second time in 1992, wasn’t told of the Oslo talks until the preliminary teams had agreed to the rudiments of a deal.

Rabin went forward, despite his deep mistrust of Arafat. The Palestinian leader was living in Tunis, having been expelled from Jordan in 1970 and from Lebanon in 1982, following the Israeli invasion. Rabin deputized Shimon Peres, his foreign minister and longtime rival, to help bring the deal to fruition. Viewed in retrospect, the audacity of the agreements that came out of Oslo was breathtaking: Arafat and the P.L.O. would recognize Israel’s right to exist, and Israel would withdraw from Gaza and seven cities of the West Bank, and also allow limited self-rule and the creation of an elected parliament—what we now know as the Palestinian Authority. A majority of Israelis supported the deal, but Rabin clearly had to rely on his reputation as a hawkish military man to reassure them. He was a kind of Israeli Nixon (the two men liked each other); at least in domestic political terms, he had far more leeway to bring about a comprehensive peace than some of his more dovish colleagues might have had. (Rabin may have favored giving the Palestinians their own state, but, if so, he never said so publicly.)

On September 13, 1993, the day of the agreement ceremony at the White House, there were detailed discussions about everything from Arafat’s attire (no gun allowed) to what would happen if he tried to embrace his Israeli counterparts. (He didn’t.) In a photograph from that day of Rabin and Arafat shaking hands in front of President Bill Clinton, Arafat, whom the White House regarded as a terrorist for most of his career, seems overjoyed. Rabin is practically grimacing. After the handshake, Rabin turned to Peres and said, “Your turn now.”

It was a happy story—indeed, as it turned out, a little too happy. The Oslo Accords prompted an unprecedented wave of terrorist attacks by Palestinian groups like Hamas, which sought to inflame the Israeli public and scuttle the deal. But, rather more unexpectedly, the accords ignited a groundswell of animosity from right-wing Israelis, who feared that Rabin intended to give the Palestinians their own state and carry out widespread evacuations of settlements. (At the time that the Oslo Accords were signed, about a quarter of a million Israelis had moved into the West Bank and East Jerusalem.) While many of the deal’s opponents invoked religious justifications for maintaining Israel’s hold on the territories it acquired in the Six-Day War, a large number of the opponents were secular. What united the two groups was their rejection of the notion that any conquered territory should be turned over to the Palestinians, even in the interests of peace.

Yigal Amir was not a settler; he was a law student from the coastal city of Herzliya and the son of ultra-Orthodox Yemeni immigrants. As the Oslo process gathered steam, Amir became increasingly convinced that Rabin was selling out the Israelis and, in particular, the settlers; he organized rallies in the occupied territories to denounce the agreements and even tried to start his own militia. Drawing on tapes and transcripts of Amir’s detailed and unabashed confession, Ephron carefully reconstructs the journey from disgruntled right-wing activist to murderous fanatic. The seed for the assassination was planted about a year before it was carried out, when Amir, quite unexpectedly, spotted Rabin at the wedding of a friend in Tel Aviv. He was stunned at how close he could get to the Prime Minister—and with a pistol “jammed in his belt.” Amir vowed never to let the chance slip away again. “Someday I will be sorry if I do not kill him,” he told himself.

The milieu of right-wing religious nationalists that Amir inhabited will be familiar to anyone who follows the news in Israel today, but it’s striking to see that it was so fully developed two decades ago. Though Amir discussed his plans only with his brother, Hagai, and a friend, he spoke openly and often about the need for Rabin to be killed, and many of his friends and fellow-students had heard him proclaim that he wanted to be the one to kill him. Israeli security forces, focussed on Palestinian terrorism, devoted scant resources to tracking Israeli extremists. Agents of the Shin Bet, the internal-security service, were aware of ominous chatter in extremist circles, but they were not prepared for the threat.

Cartoon
“So, you see, what you were really looking for was a deeper connection with your father, and not the dentist’s office down the hall.”
BUY THE PRINT »
They should have been: little more than a year earlier, Baruch Goldstein, a resident of an isolated West Bank settlement called Kiryat Arba, walked into a mosque at the Cave of the Patriarchs, a contested holy site in the Palestinian city of Hebron, and killed twenty-nine worshippers and wounded a hundred and twenty-five before being beaten to death. Goldstein became a folk hero to many people in the settler community. After the massacre, Rabin considered dismantling a nearby settlement, Tel Rumeida. But settler leaders warned him that such an action could provoke an armed reaction, and a former chief Ashkenazi rabbi commanded Israeli Army soldiers to disobey an evacuation order. Rabin backed down.

The Shin Bet kept a file on Amir that contained no more than a few sentences. Much of the agency’s information on extremist groups was provided by a paid informant, Avishai Raviv, who often joined rallies and told police that he had beaten Palestinian civilians and other backers of the peace process. In the months leading up to the assassination, Raviv heard Amir vow to kill Rabin several times, but apparently did not take him seriously. A former intelligence officer whose girlfriend travelled in the same circles as Amir learned that he was planning to kill Rabin, but didn’t turn him in.

In the weeks leading up to Rabin’s murder, three extremist rabbis from the West Bank issued a written opinion suggesting that it would be acceptable to kill Rabin, on the ground that he had betrayed the Jewish people. The rabbis based their justification on the concept of din rodef, a Hebrew term that describes a person who is stalking a defenseless man. (“Rodef” means “pursuer” in Hebrew.) Under certain interpretations of the Talmud, it is obligatory to kill a rodef in order to save the intended victim. Amir later told his interrogators that he had consulted several rabbis in search of an official sanction but could never find one. (His brother, Hagai, insisted that he had.) As Ephron points out, it apparently never occurred to Amir that he himself was a rodef.

As the Oslo Accords unfolded, and the terror attacks continued, Israeli public opinion began to shift from hope to fear. Rabin and Arafat now saw themselves as partners in a perilous endeavor. Whereas Rabin had once mistrusted Arafat, he now believed Arafat’s claims, buttressed by the Israeli intelligence services, that he was unable to stop Hamas. (Rabin believed that if Arafat did not prevail Hamas would.) In Israel, the extreme wing of the anti-Oslo coalition capitalized on the rising insecurity to excoriate Rabin; some protesters began comparing him to Hitler. As Rabin and the Labor Party’s fortunes sank, those of the Likud and its followers rose, and they stood by as Rabin was vilified. Ephron places Netanyahu at a rally, about a month before Rabin’s murder, where crowds spent two hours chanting, “Death to Rabin.” Netanyahu did nothing to discourage them.

On the day of his death, Rabin considered staying home from a peace rally, because he feared that he’d be embarrassed by a low turnout. The crowd, at Kings of Israel Square, in Tel Aviv, was enormous—about a hundred thousand people—dwarfing anything the anti-Oslo camp had put together. The main fear among the security services was a Palestinian suicide bomber; Rabin himself could not imagine that he would be killed by a Jew. Neither, apparently, could his bodyguards; when the moment came, Amir pushed through the crowd and shot Rabin twice in the back. Later that night, Amir asked the police for a glass of schnapps to toast the Prime Minister’s death. Arafat, hearing of the assassination, wept.

The public revulsion at the news was overwhelming, but it did not translate into a victory for Rabin’s successor, Shimon Peres. Peres waited three months to call an election, figuring that he would first conclude a peace treaty with Syria. But a treaty never materialized, and Hamas kept attacking, while the Likud leader, Netanyahu, vowed to make Israelis safe. Under American pressure, Netanyahu paid lip service to Oslo during his first, three-year administration. But the peace process never really recovered.

It’s jarring to contemplate the assassination of Rabin and then read Dennis Ross’s “Doomed to Succeed” (Farrar, Straus & Giroux), a detailed account of U.S.-Israeli relations since 1948. In four hundred-plus pages, there is almost no mention of the changes that have transformed the Israeli polity in the past six decades, and surprisingly little discussion of the steady growth in the settlement population, which now exceeds half a million. For Ross, who was the State Department’s director of policy planning under President George H. W. Bush, the special Middle East coördinator under President Bill Clinton, and an adviser to Secretary of State Hillary Clinton, the settlements are evidently problematic only insofar as they present an obstacle to a smoothly functioning bilateral relationship. The United Nations and most foreign governments consider them illegal, but for him they are a political difficulty to be finessed. There is no talk of justice. Pressure on Israel—by Palestinians, by Europeans, by President Obama—appears to Ross bewildering and unreasonable.

Ross describes a situation, in 2010, when Mahmoud Abbas, the President of the Palestinian Authority, refused to negotiate with Netanyahu unless he agreed to extend a moratorium on settlement construction, and the Obama Administration tried and failed to broker a compromise. His conclusion: Abbas “showed little flexibility and squandered the moratorium.” And Ross criticizes President Obama for “putting the onus on Israel.” This sort of analysis makes sense only if you regard the expansion of Israeli settlements and the Palestinian objections to them as morally equivalent.

Ross is as impatient with Palestinian efforts to gain a more sympathetic hearing at the United Nations and elsewhere as he is sensitive to the political needs of Israeli Prime Ministers. Yet he says almost nothing about the political realities that have shaped the situation, or how those realities might be changed. He evinces almost no sympathy for similar pressures on Abbas and others at the Palestinian Authority. Only near the end of the book does he bring himself to criticize Israel. Netanyahu’s decision to accept an invitation from John Boehner to address the House of Representatives, thereby defying the White House and inserting himself in a domestic political debate, was, Ross says, “a mistake.” He writes repeatedly that Israeli leaders will make concessions only when they feel secure. This may be true, but where does this leave American policy? And where does it leave Israel?

The highest compliment Ross seems able to pay an American President is to say that he is a “friend of Israel.” But how can an American President help an ally steer away from a potentially disastrous course when that ally, by the nature of its own domestic politics, isn’t able to do so by itself? Ross doesn’t say.

It’s tempting to speculate about whether Israel might have turned out differently had Rabin lived. Dan Ephron plainly thinks that it would have; he says that Rabin had made the fundamental decision to give up most of the occupied territories, even if he never explicitly said so. That meant, almost certainly, the creation of a Palestinian state, or something resembling one. Such a deal, Ephron says, would have “struck a blow for the pragmatists over the ideologues” and helped slow what he calls “the messianic drift” in Israeli society. “Had he lived,’’ Ephron writes, “Rabin might plausibly have reshaped Israel broadly and permanently.”

Ephron is probably right about Rabin making a deal, but he may be overstating the rest. For one thing, allowing for the creation of a Palestinian state, even in the late nineteen-nineties, would have been a politically explosive undertaking. There were some hundred and thirty thousand settlers in the West Bank then, and, even with the broad support of the Israeli public, the government would have had a very difficult time uprooting more than a handful of them. In 1994, after the Cave of the Patriarchs massacre, Rabin could not bring himself to order the removal of a single unauthorized enclave. In 2005, when the Israeli government, led by Ariel Sharon, the hard-bitten former general, ordered the evacuation of about eight thousand settlers from Gaza, their departure was accompanied by entrenched resistance and mass protests. With a broader peace deal, Rabin would have been in for quite a fight.

And it’s far from apparent how much even a comprehensive peace deal would have changed Israeli society. It might have helped open up Israel and the Palestinian areas to each other, but, at least in the short term, it would almost certainly not have brought peace. After all, the Oslo Accords brought more bloodshed, not less. It’s not clear that even Rabin could have persevered.

More important, a deal with the Palestinians, even one that included substantial withdrawals from the occupied territories, would have done little to alter the demographic trends that have been reshaping Israeli politics and society; that is, the growth of the Orthodox and ultra-Orthodox communities. In the twenty years since Rabin was killed, Israel has become more religious, more conservative, and, to borrow a word from Ephron, more “messianic.” Rabin would have found himself increasingly among people to whom he had very little to say.

However slim the chances for a comprehensive peace agreement were in the nineteen-nineties, today they are effectively zero. Until recently, it was heretical to suggest that a two-state solution was implausible. Today, it seems nearly impossible to imagine one at all. What this portends for Israeli society may be disturbing—depending on which estimate you choose, the combined population of Palestinians in Israel and the occupied territories will exceed the number of Jews there as soon as 2020—but it doesn’t make peace any more likely. There are now four hundred thousand settlers in the West Bank, and they are more powerful and more organized than they were when Rabin was killed. Since then, the Israeli center has moved steadily rightward; in 1996, when Yigal Amir was convicted, ten per cent of Israelis said that he should be released; in 2006, thirty per cent said so. (Amir’s brother, Hagai, convicted for being an accomplice, is already out of prison and, as Ephron details, has slipped comfortably back into Israeli society.) Today, so-called price-tag attacks, which aim at punishing not only Palestinians but Israelis who try to impede settlement activity, have gained widespread acceptance in settler councils and are often protected by a popular refusal to coöperate with police. The extremists may still be a minority in the occupied territories, but no Israeli politician hoping to hold national office dares to confront them.

There isn’t much reason to expect anyone in Washington to ride to the rescue. When Netanyahu, during his reëlection campaign earlier this year, declared that he would never allow a Palestinian state, he was scolded by the White House, and then reëlected to a fourth term. In September, President Obama, whom Netanyahu had humiliated in front of Congress only months before, invited him back to the White House.

It’s possible that the course of events in Israel and Palestine might be altered by some extraordinary act of leadership—by some Rabin we haven’t met, or by some crisis we have not foreseen. But, for now, nothing like that seems remotely possible. Tolstoy posited that history is not made by individuals, that it is, rather, the continuously unfolding consequence of innumerable interconnected events. But, if the story of Yitzhak Rabin and Yigal Amir has anything to teach, it’s that individuals matter. Rabin was the right man at the right time, and so, in his perverse way, was Yigal Amir. The opportunity that Rabin was trying to seize—however small—was there for a moment, and it may never come again. ♦

<--------->
Books OCTOBER 26, 2015 ISSUE

Humboldt’s Gift
He was once the most celebrated naturalist in the world. What happened to him?

BY ELIZABETH KOLBERT
SHARETWEET
 10_26_15
TABLE OF CONTENTS
Need to stop reading?

Enter your email address
 GO
We'll send you a reminder of where you left off.
 Humboldt passed along his love for the natural world to his many admirers.
Humboldt passed along his love for the natural world to his many admirers.
CREDIT ILLUSTRATION BY ATAK
On September 14, 1869, the centenary of Alexander von Humboldt’s birth was commemorated in New York—a city Humboldt had never visited—with a parade, a torchlight procession, a proclamation by the mayor, a formal banquet, and the unveiling of a bronze bust in Central Park. The following day, the Times devoted its entire front page to chronicling the festivities. The unveiling was scheduled for 2 P.M., but long before the appointed hour, the paper reported, “an immense throng of people had gathered,” and when the statue was finally revealed “there were not less than 25,000 persons” in attendance. Flags waved from public buildings, military bands played, and homes were decorated with Humboldt’s portrait. The whole city, according to the Times, “seemed to be in holiday dress.”

In Boston, another city Humboldt had never set foot in, the centenary was marked with a two-hour address delivered by Louis Agassiz. This was attended by, among many others, Henry Wadsworth Longfellow, James Russell Lowell, and Oliver Wendell Holmes. After the address, the crowd retired to the Horticultural Hall, where a palm frond that had rested on Humboldt’s coffin was displayed and, in the words of a Times correspondent, “an elegant collation was served.” President Ulysses S. Grant attended the revelries in Pittsburgh, and former President Millard Fillmore presided over the ceremony in Buffalo. Similar commemorations were held in Albany, Chicago, Baltimore, Cleveland, Memphis, and San Francisco. Humboldt mania hit Melbourne and Moscow, not to mention Hamburg, Dresden, and Frankfurt. In Berlin—Humboldt’s home town—eighty thousand people showed up to celebrate in the pouring rain.

What, exactly, was all the hoopla about? At a distance of almost a hundred and fifty years, it’s hard to say, not just because Humboldt’s individual triumphs have faded but because there were so many of them. In 1802, Humboldt climbed nineteen thousand four hundred feet up Chimborazo, in what’s now Ecuador. At the time, the mountain was believed to be the tallest peak in the world, and nineteen thousand four hundred feet was the highest anyone had ever climbed. (In fact, Chimborazo is nowhere near the world’s tallest mountain, although, owing to the globe’s oblate spheroid shape, its peak is the farthest from the center of the earth.) Humboldt was, in this way, the Edmund Hillary of his generation. He was also a naturalist, an inventor, a prolific author, and a republican, in the French Revolutionary sense of the word. Several of his books became international best-sellers. Humboldt’s writings on his adventures in South America inspired figures as diverse as Charles Darwin and Simón Bolívar, who called him the “discoverer of the New World.” As one of his translators put it, “It would need another Humboldt to encompass such a life and its works.”

But Humboldt was, by the time of his death, at the age of eighty-nine, already an anachronism—a generalist in a period of increasing specialization and a Romantic in the Victorian era. Those he influenced quickly went on to overshadow him. Just a few months after Humboldt’s funeral, in May, 1859, “On the Origin of Species” came out. It upended the Weltanschauung that Humboldt had promoted, and his books began to fall out of print. (When I went to the nearest college library in search of some of his thirty-odd published works, all I found on the shelves was a desiccated edition from 1853.) By the time the bicentennial of his birth rolled around, in the English-speaking world, at least, Humboldt had been nearly forgotten.

As his sestercentennial approaches, a new biography has appeared—“The Invention of Nature: Alexander von Humboldt’s New World” (Knopf), by Andrea Wulf, an author and design historian who lives in Britain. Wulf argues that Humboldt’s long, eventful life deserves another look. Indeed, she maintains, the more damage that is done to the world he explored, the more relevant his ideas become.

Alexander von Humboldt was born to a wealthy family in the Prussia of Frederick the Great, and from an early age he chafed at the restrictions of upper-class life. Instead of applying himself to his lessons, like his dutiful older brother, Wilhelm, he roamed the woods, collecting herbs and insects; his parents nicknamed him, not altogether kindly, “the little apothecary.” When Humboldt wrote letters from the family estate, Schloss Tegel, he sometimes used the tagline Schloss Langweil—“Castle of Boredom.”

In his early twenties, Humboldt became friendly with Georg Forster, a German who had sailed to Tahiti with Captain Cook. Forster took Humboldt to London, where he introduced him to the naturalist Joseph Banks, who had also sailed with Cook, and who had assembled the world’s largest collection of plant specimens. On the return trip, the pair stopped in Paris, where preparations were under way for the first anniversary of the storming of the Bastille. Humboldt was hooked—on travel, on botany, on revolution—and he resolved to have his own Cook-like adventure. But Humboldt’s mother—his father had died by then—had no interest in funding adventures. She wanted her son to become a bureaucrat. As a compromise, he agreed to study mining.

For the next half-decade, Humboldt worked as a mine inspector for the Prussian government. Dismayed by what he encountered, he used his own money to open a miners’ school. He also invented a new kind of respirator, designed a better safety lamp, and published a book on subterranean flora. Meanwhile, he began to experiment, even on himself. Fascinated by the work of Luigi Galvani, who’d made animals’ muscles jump by running a current through them, Humboldt cut open his back and stuck wires into the wounds. In the process of these gruesome probes—Humboldt wrote that he was starting to look like “a man who had been running the gauntlet”—he came close to creating the first electric battery. But he neglected to draw the crucial inferences from his own work, and the battery was instead invented, shortly afterward, by Alessandro Volta. According to Douglas Botting, the author of a 1973 biography, “Humboldt and the Cosmos,” he “never forgave himself this failure.”

The death of Humboldt’s mother, in 1796, freed him from her disapproval and, at the same time, provided him with a fortune. He soon signed on to an around-the-world voyage being underwritten by the French government, but it was called off, when the government decided that it needed the money to fight the Austrians. Next, Humboldt set off for Madrid, where he managed to secure a meeting with the Spanish king Carlos IV. A well-known imbecile, Carlos seems to have imagined that sending a mining expert to the New World would yield new riches for the Crown. He gave Humboldt the go-ahead to travel anywhere he wanted in Spain’s American colonies. Equipped with forty-two crates of scientific instruments, including a cyanometer, for measuring the blueness of the sky, Humboldt set sail. His goal, he wrote to a friend on the eve of the voyage, was to discover “the unity of nature.”

This was either a very grand plan or no plan at all. Humboldt thought he was bound for Havana, but, because of a shipboard outbreak of typhoid, he ended up being deposited in Cumaná, in present-day Venezuela. Unfazed, he set off across the Llanos, the vast plain east of the Andes, where he was excited to encounter rivers filled with electric eels. Naturally, he decided to renew his experiments. “If by chance you get a shock before the fish is wounded, or exhausted by a long chase, the pain and numbness are so extreme that it is hard to describe the nature of the sensation,” he observed.

From the Llanos, Humboldt travelled by canoe along the Rio Apure and the Orinoco. The heat was unbearable and the mosquitoes were worse. “People who have not navigated the great rivers of equinoctial America can scarcely conceive how, at every instant, without intermission, you may be tormented by insects flying in the air,” Humboldt wrote. Nevertheless, he was enchanted. Jaguars, tapirs, and peccaries came down to the river to drink:

They are not frightened of the canoes, so we see them skirting the river until they disappear into the jungle through a gap in the hedge. I confess that these often repeated scenes greatly appeal to me. The pleasure comes not solely from the curiosity a naturalist feels for the objects of his studies, but also from a feeling common to all men brought up in the customs of civilization. You find yourself in a new world, in a wild, untamed nature. . . . All kinds of animals appear, one after the other. “Es como en el paraíso” (“It is like paradise”), our old Indian pilot said.
A year and a half after leaving Europe, Humboldt finally made it to Havana. He was planning to sail from there to Mexico when, once again, chance intervened. Humboldt read in a newspaper that the French expedition he’d hoped to join had set off after all, and was on its way to Australia. He reasoned that the expedition would stop in Lima before crossing the Pacific, and decided to catch up with it there. This entailed sailing back to South America, to Cartagena, then trekking across the Andes, a journey of some twenty-five hundred miles. When Humboldt reached Quito, nine months later, he learned that the French expedition had travelled in the opposite direction, around the Cape of Good Hope. “Any other man would have despaired,” Wulf notes. Humboldt’s response was to climb Chimborazo.

He ended up spending five years in South America. Everywhere he went, he took measurements with his instruments, at least those which hadn’t been lost on the Orinoco or smashed in the Andes. These led him to the concept of isotherms—lines connecting points on a map with the same average temperature—and to the discovery of the magnetic equator: the line along which earth’s magnetic field is parallel to its surface. By the time the trip was over, he’d collected some sixty thousand plant specimens. He’d also become convinced of the sophistication of South America’s pre-Columbian cultures and of the evils of slavery, which he felt obligated to publicize.

“It is for the traveler who has been an eyewitness of the degradation of human nature, to make the complaints of the unfortunate reach the ear of those by whom they can be relieved,” he wrote. On his way back to Europe, Humboldt stopped in Washington, D.C., where he met with President Thomas Jefferson. Humboldt sometimes referred to himself as “half American,” and was initially a big admirer of the American experiment. But, as the decades wore on, he grew disenchanted. In the eighteen-fifties, he told the Times’ correspondent in Germany, “I don’t like the present position of your politics. The influence of slavery is increasing, I fear. So, too, is the mistaken view of Negro inferiority.”

The trip to South America had cost Humboldt much of his fortune. Publishing his findings cost him the remainder. Settling in Paris, he wrote and wrote—about his personal experiences, about the landscape he had seen, about the plants he’d collected, about the people and politics of the Spanish colonies. (Humboldt was such a Francophile that he wrote in French rather than in his native German.) His books, much like his travels, were full of energy but, at the same time, unfocussed and digressive.

“You write endlessly,” Humboldt’s good friend and possible romantic interest, the astronomer François Arago, told him. “But what comes out of it is not a book, but a portrait without a frame.” (Humboldt never married, and it’s often speculated that he was gay, though how many—if any—of his intense relationships were sexual is unknown.) Humboldt hired a small army of artists and engravers to illustrate his works. As a consequence, they were phenomenally expensive; in the U.S., a complete edition cost two thousand dollars—something like thirty thousand dollars in today’s money. According to Botting, “Not even Humboldt could afford to possess a set.”

As he scribbled away, Humboldt continued to search for the elusive “unity of nature.” He visited with the naturalists Jean-Baptiste Lamarck and Georges Cuvier, at the National Museum of Natural History in Paris. He helped Joseph Louis Gay-Lussac with his pioneering studies of the behavior of gases. He assisted Arago with his experiments at the Paris Observatory. “Humboldt dashed from one meeting to another and from one dinner to the next,” Wulf reports. Some evenings, he attended as many as five salons. He was known around Paris for his good looks, his breadth of knowledge, and his volubility. A pianist who was asked to perform for him at a party described the invitation as a highlight of his career. But as soon as he started to play, the pianist complained, Humboldt “began to hold forth” and did not shut up for the entire piece.

In 1827, after Humboldt had been living in Paris for more than two decades, the king of Prussia, now Frederick the Great’s grandnephew, insisted that he return to Berlin. By this time, Humboldt depended on a stipend from the king to pay his expenses, so he had no choice but to agree. (It was an irony not unremarked upon by his contemporaries that the great champion of freedom was reduced to being a courtier.) Humboldt had been back in the city for only a few months when he decided to deliver a series of lectures on the theme of, well, everything. He expatiated on meteorology, geology, plant geography, and ocean currents, as well as on fossils, magnetism, astronomy, human migration, and poetry. The lectures, originally given at the University of Berlin, proved so popular that Humboldt delivered them all over again, in a concert hall. There was such a crush to get into the hall that, on the days when he spoke, traffic in the neighborhood practically ground to a halt. He was offered a big advance to publish the talks, but turned it down in order to rewrite them, a process that ended up taking him two decades. The first volume of the resulting work, “Kosmos,” was a huge hit, the second even huger. Booksellers in Hamburg and Vienna pirated shipments to make sure their shelves were stocked. Humboldt delivered the fifth and final installment of “Kosmos” just a few days before he died.

Almost no one actually reads Humboldt anymore. Still, according to his admirers, he has never ceased to be relevant, though the reasons for this have varied over time. During the Weimar Republic, Humboldt was celebrated as a progressive thinker. Then, during the Third Reich, he became the explorer who established German claims in Latin America. In East Germany, he was the revolutionary who labored on behalf of ordinary miners. After reunification, he was recast as a global citizen.

The latest variation on this theme is the green Humboldt. As Nicolaas Rupke, a historian of science at Washington and Lee University, puts it in “Alexander von Humboldt: A Metabiography” (2008), “Humboldt-the-environmentalist” is now “part of the standard narrative.”

This is what Andrea Wulf sees as Humboldt’s claim on our attention. Long before the advent of chainsaws, she notes, he was warning about the dangers of deforestation. And, already in the early nineteenth century, he recognized a connection between forest health and hydrology; when trees were cut down, he observed, evaporation from the soil increased, and the area dried out. “As Humboldt described how humankind was changing the climate, he unwittingly became the father of the environmental movement,” Wulf writes. In her view, he “invented the web of life, the concept of nature as we know it today.”

Humboldt’s love for and fascination with the natural world certainly were profound. And that love, which animated his writings, was passed on to his many devoted fans. When, for example, Thoreau climbed Mt. Wachusett, he claimed to be “with Humboldt” as he “measured the more modern Andes.” (Mt. Wachusett, north of Worcester, has an elevation of two thousand and six feet.)

But Humboldt waxed poetic about many subjects, and the green Humboldt probably reflects our priorities at least as much as it does his. Among Humboldt’s many gifts was that of self-knowledge. He recognized that he had spread himself too thin, that in all his travels and experiments and books and lectures there had been no single great insight or discovery that changed man’s view of the cosmos. What he offered the world was his enthusiasm, which, if a frail basis for an intellectual history, is nonetheless a deeply appealing trait.

“In eight days of reading books, one couldn’t learn as much as what he gives you in an hour,” Goethe said of Humboldt, whom he counted as a good friend. When Darwin finished his own Humboldtian travelogue, “The Voyage of the Beagle,” he nervously sent his hero a copy. “You have an excellent future ahead of you,” the older man reassured him.

“My life has been useful to science less through the little I have contributed myself than through my efforts to let others profit of the advantages of my position,” Humboldt wrote not long before his death. “I like to think that, while I was at fault to tackle from intellectual curiosity too great a variety of scientific interests, I have left on my route some trace of my passing.” ♦

<--------->
Books OCTOBER 26, 2015 ISSUE

Delusions of Candor
How will we remember Gore Vidal?

BY LEO ROBSON
SHARETWEET
 10_26_15
TABLE OF CONTENTS
Need to stop reading?

Enter your email address
 GO
We'll send you a reminder of where you left off.
 “He needs to conquer, to shine, to dominate,” Vidal’s friend Anaïs Nin noted.
“He needs to conquer, to shine, to dominate,” Vidal’s friend Anaïs Nin noted.
CREDIT PHOTOGRAPH BY STEVE SCHAPIRO / CORBIS
In October of 1975, dining in Rome, Gore Vidal told his new friend the novelist Michael Mewshaw that Françoise Sagan was “a magnum of pure ether.” He didn’t stop to clarify, but rigor was beside the point; the Vidalian bon mot was about the speaker, not about the subject. In the course of more than half a century, his quips, aphorisms, insults, and punch lines amounted to a self-portrait, airbrushed so as to highlight his favorite warts: Olympian detachment, patrician hauteur.

It was an act, a put-on—perhaps the most effective double bluff in the history of literary P.R. In 1977, after visiting Vidal at his cliff-perched villa on the Amalfi Coast, Martin Amis observed that “he has little of the paranoia worryingly frequent among well-known writers.” Norman Mailer had been onto something, Amis concluded, when he said that “Vidal lacks the wound.”

“My God,” Vidal told Amis, “what a lucky life.” The official story, as set down in Vidal’s memoirs and essays, and in hundreds of reviews, profiles, and, finally, in his obituaries—he died in 2012—went like this: grandson of Thomas P. Gore, the blind senator from Oklahoma, son of Gene Vidal, a high-school football star whose exploits as an aviation pioneer landed him on the cover of Time, he was born in 1925, at West Point, grew up in Washington, D.C., and studied at Exeter. If asked about his mother, Nina Gore, who had swapped family life for a succession of boyfriends and husbands, Vidal would explain that her desertion—and her alcoholism, and her sexual confessions—hadn’t really bothered him. (A reporter bold enough to press the subject would be silenced with a reference to Freudian quackery.)

At seventeen, Vidal would explain, he “quit schooling” for good and enlisted in the Army, served as first mate on a supply ship in the Aleutian Islands, and then—almost by accident, virtually without sweat, and for the simple reason that he could—became a novelist (“Julian,” “Myra Breckinridge,” “Burr,” “Creation”), essayist (“Homage to Daniel Shays,” “The Hacks of Academe”), playwright (“Visit to a Small Planet,” “The Best Man”), screenwriter (“Ben-Hur”), politician (valiant failed campaigns for Congress, in New York, and for the Senate, in California), actor (“Bob Roberts,” “Gattaca,” “Igby Goes Down”), steel-chinned prime-time brawler (points victories over Buckley in 1968 and Mailer in 1971), and friend to everyone worth knowing (Greta, Tennessee, Eleanor, Orson, Mick, Sting). Yet he remained immune to the seductions of celebrity and clear-eyed about the workings of power. Stepbrother of a sort to Jacqueline Bouvier, he had been a welcome guest at Hyannis Port and the White House until he grew bored with the whole thing and unmasked Bobby Kennedy (notable for his “vindictiveness” and “simple-mindedness about human motives”) in his essay “The Best Man, 1968” and then the Kennedy courtiers in “The Holy Family” and “The Manchester Book.” Later efforts in this truth-to-power vein had titles like “Shredding the Bill of Rights,” “State of the Union, 1975” (it wasn’t good), “State of the Union, 1980” (worse), and “State of the Union, 2004” (don’t ask).

And while his contemporaries—as speared in his essay “American Plastic: The Matter of Fiction”—nervously tracked their positions on the New York literary stock exchange, Vidal lived in regal exile with his partner, Howard Austen, quite impervious to what anyone thought about his writing, his quoted comments, or his sexual proclivities. The one sign of human frailty was his insistence that the hordes of visiting photographers favor his “good” side, the left.

This was the figure known to most, but not to all. At the end of the war, Warrant Officer Vidal was stationed at Mitchel Field, on Long Island, and working part time for the publisher E. P. Dutton. He came into the city whenever he could. On a Sunday in November of 1945, he attended a lecture on love at the Ninety-second Street Y. It was here that he met Anaïs Nin.

Born in France to Cuban parents, Nin, who was forty-two, was writing fiction alongside a diary that she would one day publish. “He has great assurance in the world, talks easily, is a public figure, shines,” Nin wrote, after Vidal paid a visit to her studio. “He can do clever take-offs, imitate public figures.” He is also “lonely,” “hypersensitive,” “insecure.” When Vidal opened up to her—“He dropped his armor, his defenses”—it was not to talk about his grandfather the senator or his father the aviator but his mother the deserter. “Psychologically,” Nin wrote, “he knows the meaning of his mother abandoning him when he was ten, to remarry and have other children.”

At first, Vidal was thrilled by the connection. Returning from a trip to Washington, D.C., he told her, “You have cast a spell on me. What I once accepted, I now do not like. I found my grandfather, the senator, boring.” But the spell soon wore off. In March of 1946, Vidal invited Nin to a dinner at the PEN Club. “Was shocked by the mediocrity of the talks,” she wrote. “A ‘literary’ world so thoroughly political, intriguing, and commercial, but a world Gore intends to conquer.” The next month, she writes, “Gore in the world is another Gore. He is insatiable for power. He needs to conquer, to shine, to dominate.” In November, she notes that Vidal’s letters—he had then retreated to write in a Guatemalan monastery that he had acquired for a pittance—“sound attenuated, diminished, dulled. Lack of faith, of responsiveness to surroundings and people. A blight.” By December, she admits defeat: “Whatever Gore was with me, whatever side he showed me, was not the one he was to show in his life and in his work.”

“The Diary of Anaïs Nin, Volume Four: 1944-1947” was published in 1971, and Nin’s use of the past tense carries a hint of retrospect, as if she were taking account of later developments. In 1970, the composer Ned Rorem, another diarist friend, described the “cynical stance” that Vidal had perfected over the previous quarter century: “Those steely epigrams summing up all subjects resemble the bars of a cage through which he peers defensively. ‘It’s not that love’s a farce—it doesn’t exist.’ . . . Rather than risk being called a softy, he affects a pose of weariness.”

Jay Parini, in his authorized biography, “Empire of Self: A Life of Gore Vidal” (Doubleday), wants to give us the real Gore, but he keeps on falling for the pose. Although Parini exhibits some skepticism toward his subject, and notices that Vidal’s claim of indifference to the world’s opinion was at odds with the many framed magazine covers and threats of libel suits, he begins each chapter with an epigraph culled from Vidal’s table talk and publicity spiel. When it comes to telling the story of the life, Parini proves content to deliver the strapping, self-assured, untouchable Vidal, the builder and overseer of a well-protected, many-colonied “empire of self”—a phrase repeated throughout the book, in a dizzying range of connections.

As Parini approaches Vidal’s later years, his defensive instincts go into overdrive. He praises an essay on John Updike—ten thousand words of ill-argued bile—as “a kind of cultural service,” and declares Vidal “more relevant than ever” in the years after 9/11, when he was in the habit of writing things like: “The unlovely Osama was chosen on aesthetic grounds to be the frightening logo for our long-contemplated invasion and conquest of Afghanistan.”

Vidal is the book’s leading witness, though not a reliable one; his testimony is undermined by what the novelist Adam Mars-Jones called “delusions of candour,” and possibly by delusions of a different sort. Though Parini believes that Vidal gave more interviews than any writer “in the history of literature,” his notes, which are far from comprehensive, contain thirty references to interviews conducted when his subject was in his eighties. Five of the conversations took place in 2010, the year that Vidal began to suffer the effects of Wernicke-Korsakoff syndrome, or “wet brain,” which Parini calls “a stage in alcoholism when the drinker begins to lose touch with reality.”

The underlying problem is a lack of distance. Parini met Vidal in the mid-nineteen-eighties, and the two became great friends. They spoke on the phone every week—“for periods on a daily basis”—and spent time together in a dozen cities. Parini cannot resist playing Boswell any more than he can resist making the Boswell comparison, and it has a damaging effect on his role as biographer. A sentence from a passage ostensibly dealing with the early days of Vidal’s relationship with Austen, whom he met in 1950, begins, “A key memory of their relationship (for me) dates to the late eighties.” It seems unlikely that Vidal would have become the subject of one of Parini’s books—alongside Melville, Tolstoy, Faulkner, and Jesus—if not for the personal connection.

Yet it would be hard to imagine a less intimate biography. Parini loved spending time with the worldly, woundless Vidal, and he seems eager to perpetuate Vidal’s myths about himself. In a letter from the late forties, Vidal wrote that psychoanalysis is “quite a frightening experience,” and that “it’s not a pleasant thing to see oneself.” But when Vidal tells Parini that his experience of therapy failed because “I have no unconscious,” the biographer doesn’t pause to comment.

Cartoon
BUY THE PRINT »
The book’s use of Anaïs Nin is particularly disappointing. Parini quotes Nin’s initial description of Vidal (“clear and bright” and “luminous and manly”) but little else, and his account of their relationship reveals limited acquaintance with what she wrote. Parini says that Vidal tried to interest Dutton in Nin’s fiction but failed, because Dutton—“a manly house”—“shied away from anyone like Nin, who exuded both femininity and exoticism.” But any reader of Nin’s diaries would know that, in December of 1945, Vidal offered her a thousand-dollar advance for her novels, and that Dutton published “Ladders to Fire” the following year.

Parini justifies his brisk treatment of Nin by saying that the published version of her diaries tells “only a bit of the story.” This is no doubt true. But, given that Parini did not visit Nin’s archive, he might have spent more time with them, not only because of their tender closeup portrait of Vidal in his early twenties but because they help to solve the central problem of any literary biography: how to connect the life and the work.

Considering Vidal’s failure to become a poet, Parini accepts his subject’s glib explanation: “The Muse passed over my doorstep.” Nin offers another perspective. In her account, all Vidal’s shortcomings were rooted in the refusal of feeling. After reading “Williwaw,” his first novel, about his Navy experiences, she wrote, “I am startled by the muted tone, the cool, detached words.” In the following weeks, she was given reason to believe that Vidal shared her assessment. “I’ve never written this way, impulsively, directly, and without plan,” he tells her about a play he is working on, and explains that he is, in Nin’s recounting, “aware of the conventional mask of his first novel.”

The culmination of this process should have been Vidal’s third novel, “The City and the Pillar” (1948), about a homosexual relationship that ends in murder. At the time of its publication, the novel was both admired and disparaged for its frankness. Nin’s response foreshadowed its later reputation as self-loathing. After reading the manuscript, she wrote him, “I am going to try and tell you what was destroyed by your novel.” She called “The City and the Pillar” a “book without illusion, without feeling, and without poetry”:

Jim, in your story, kills Bob because Bob has not romanticized the sexual relationship they once had, has looked upon it flatly as a mere sexual incident of no importance. . . . So he kills him. Jim kills the legend in himself, but actually there was no legend, just Jim’s need of idealizing reality.
She presented the book’s shortcomings as human failures:

Everything in your eyes is diminished and uglified. . . . You always focus on the faults, on what can be satirized. . . . To see only the ugliness, that is what people do when they do not love. . . . You are not aware that when you paint only cruelly, underlining only faults or weaknesses, you are the loser.
Vidal continued to write about his own experience: among his novels in the years after “The City and the Pillar” are “The Season of Comfort” (1949), in which a young man struggles with his alcoholic mother, and “The Judgment of Paris” (1952), about a young American on a makeshift Grand Tour. But these efforts were also considered shallow. Norman Mailer, in his essay “Evaluations—Quick and Expensive Comments on the Talent in the Room,” written in 1959, argued that Vidal’s “narcissistic explorations . . . do not go deep enough into himself, and so end as gestures and postures.”

At the time of Mailer’s essay, Vidal hadn’t published a novel in five years. Strapped for cash, he spent much of the fifties writing for television and the movies: filling weekly slots on “General Electric Theater” and “Studio One,” adapting Tennessee Williams’s loopy play “Suddenly, Last Summer,” sprinkling homoeroticism over “Ben-Hur.” But in 1964, the year his play “The Best Man” became a film, with Henry Fonda as a Presidential candidate—Vidal said that an agent’s suggestion that Ronald Reagan play the part had been laughingly rejected—Vidal published “Julian,” a vast, fine-grained portrait of the apostate Roman emperor, which marked Vidal’s first appearance at the top of the Times best-seller list. It also marked his long-awaited breakthrough.

The failed reckoning with painful feelings was over. Vidal had found a form that exploited the virtues he was comfortable displaying in public, notably, worldliness, erudition, and cynicism:

The first official to greet me was Arbetio, who had been consul in the year I was made Caesar. He is a vigorous, hard-faced man of forty; born a peasant, he became a soldier, rising to commander of cavalry and the consulship. He wants my place, just as he wanted Constantius’s place. Now there are two ways to handle such a man. One is to kill him. The other is to keep him near one, safely employed, always watched. I chose the latter for I have found that if someone is reasonably honest and well-meaning—though he has treated one badly—he should be forgiven. When men are honest in public life we must be on good terms with them, even though they have treated us badly in a private capacity; while if they are dishonest in public affairs, even though they are personally devoted to us, they must be dismissed.
This is not what Anaïs Nin meant by literature—it is not poetic or psychoanalytic or Lawrentian. But it suited Vidal. In trading halfhearted, gestural, posturing novels about love and pain for full-bodied novels about diplomacy and power, Vidal realized Mailer’s professed hope that he would “turn the prides of his detachment into new perception.”

In 1967, he followed “Julian” with “Washington, D.C.,” the opening volume of a sequence of novels variously known as “American Chronicles” or “Narratives of Empire,” which he pursued intermittently, and in strange order, over the next three decades. Then came “Myra Breckinridge,” a comic splurge about a movie-besotted transsexual “whom no man will ever possess.” It took a month to write—and a month to sell two million paperbacks. Vidal henceforth divided his energies between historical “reflections” and satirical “inventions”; between Founding Fathers and violent sex fiends; between the worlds of Aaron Burr, the hero of the second (and juiciest) American Chronicle, and Raymond Burr, one of many pop-culture names dropped throughout the pages of “Duluth” (1983), a deranged fantasia in which life imitates bad television.

If the history novels, on the whole, work better than the satires, it is partly because Vidal gravitated toward historical subjects that came ready-laden with themes he wanted to explore, and partly because an amused-detached perspective is better suited to the Machiavellian than to the libertine. Even in a book as outwardly impulsive as “Myra Breckinridge” Vidal found opportunities for cool analysis:

It is the wisdom of the male swinger to know what he is: a man who is socially and economically weak, as much put upon by women as by society. Accepting his situation, he is able to assert himself through a polymorphic sexual abandon in which the lines between the sexes dissolve, to the delight of all.
Parini admires Vidal’s novels from the sixties and seventies, especially the historical ones, but he cannot decide how to praise them. At first, he says that the “radical subjectivity” of “Julian” “anticipates the postmodern turn in fiction” in its (very implicit) skepticism about the idea of history and truth. Later, we meet the writer who kept faith with “the so-called historical novel” (the novel that believed in history) at a time when “the postmodern novel” (the novel that didn’t) had rendered it in many eyes a “déclassé genre.”

Cartoon
“You will continue to perform the same repetitive tasks that you have always performed.”
BUY THE PRINT »
Both impulses were present. As a character puts it in “Washington, D.C.,” “History is gossip, but the trick is determining which gossip is history”—a line that accommodates postmodern skepticism and traditional empiricism. The appeal of a novel like “Julian,” which is presented as the Emperor’s diary, is that its narration has the status of both personal testimony and official record. And his portrayal of historical figures throughout the American Chronicles shows a similar struggle between impious mischief and trembling reverence. “What balances him is the power to rebel against authority,” Anaïs Nin wrote. “Emotional rebellions offset the power-loving side.” The appeal of a novel like “Lincoln” (1984), in which the President contracts a venereal infection, derives from these mixed impulses. The old myths about Lincoln’s piety and perfection come tumbling down, and yet he remains worthy of debunking.

Parini gives a better account of Vidal’s nonfiction writing, which started “by chance,” but soon became “a full-blown sub-career.” As he worked on “Julian,” Vidal also wrote most of the reviews, essays, and profiles that appeared in his first collection, “Rocking the Boat.” (“Gore Vidal is now a critic, which means he is cremating people,” Nin wrote.) Starting in 1963, Vidal was a regular contributor to The New York Review of Books, which published about three of his review-essays every year, and rejected, as too boring or too contentious, some of his most substantial work in this form—“French Letters: Theories of the New Novel,” “The Holy Family,” about the Kennedys, and “Pink Triangle and Yellow Star,” originally called “Some Gays and the Jews.” (The beneficiaries were Encounter, Esquire, and The Nation, the magazine where much of Vidal’s later political writing appeared.)

Parini nicely describes the “lofty intimacy” of Vidal’s style, and makes a strong case for Vidal as one of the critics who helped to “enlarge and redefine” the book-review essay. But, in calling Vidal’s book reviews “his very own Harvard,” Parini misses both their strength and their weakness. Apart from a few exercises in self-education—such as “French Letters,” “American Plastic,” and a review of the Times best-seller list—Vidal tended to write on subjects in which he was already expert. The virtue of his best essays—the source of their fluent authority and zesty phrasing, along with the frequently heavy provision of gossip—is that he knew the subjects backward and forward, either as a reader (“Tarzan Revisited,” “On Rereading the Oz Books,” “The Golden Bowl of Henry James”) or as an acquaintance (“Remembering Orson Welles”), or both, in the case of Christopher Isherwood, whose “I am a camera” conceit Vidal scrutinized in an essay from 1976:

Because of those four words he has been written of (and sometimes written off) as a naturalistic writer, a recorder of surfaces, a film director manqué. Although it is true that, up to a point, Isherwood often appears to be recording perhaps too impartially the lights, the shadows, the lions that come within the area of his vision, he is never without surprises; in the course of what looks to be an undemanding narrative, the author will suddenly produce a Polaroid shot of the reader reading, an alarming effect achieved by the sly use of the second person pronoun. You never know quite where you stand in relation to an Isherwood work.
Like many before him, Parini says that Vidal is a better essayist than he is a novelist. It’s a position that tends to emphasize the shortcomings of Vidal’s fiction—all the things he didn’t try to do—while overlooking the vices of his essays. Martin Amis, writing, as he later admitted, from scant familiarity with Vidal’s fiction, said that Vidal was too clever to write novels but not too clever to write essays, because “you can’t be too clever for them.” But you can be too angry and too anxious, too cut off from the taproot of your own strong opinions.

Every essayist is a product of his own hobbyhorses, but few claim as vehemently as Vidal to be offering not a view but the Truth. If his novels turned his public persona into an aesthetic, his essays tried to turn his private anxieties into points of intellectual integrity. Sometimes he succeeded. In an exchange of letters in The New York Review of Books, he advised John Bayley that “The Golden Bowl” is about force, not about love, invoking a reading of Jamesian irony and making it work, just about. At other times, his arguments look merely like animus.

Parini says that Vidal worried that “being exclusively gay . . . interfered with” his theories about sexuality. But the reason Vidal came up with those pansexual theories was so that he could tell the world he was not exclusively gay. It may be true, as Vidal frequently maintained, that “gay and straight” are “nonexistent categories,” but the point was to establish that although he lived with a man, and had sex only with men, he was not, and never could be, merely a “homosexual.” (Vidal told Amis that he had been reading D. H. Lawrence: “Every page I think, Jesus, what a fag. Jesus, what a faggot this guy sounds.”)

A general distaste for authority underpinned a number of Vidal’s positions. For years, he slammed practical criticism for what he called its “slow killing of the work through a close textual analysis.” Yet, in 1979, in an attack on another bugbear—the emphasis, beyond academe, on a writer’s personality—he enlisted the New Critics to his cause, and regretted that “these paladins of the word have long since faded away.” And his writing on America (the “national security state” at home, the empire abroad) was beholden to a dark, declinist view. In the age of the Pentagon Papers, Watergate, and the Iran-Contra affair, he looked canny. But when the facts changed he didn’t change his mind, because his mind had not been formed by facts to begin with. Vidal’s view of the world was the opposite of supple, and someone who says, “It always comes down to money,” is bound to be wrong at least some of the time. Proof positive that Vidal’s journalism was anything but a voyage of discovery came in 1971, when, at the request of the Los Angeles Times, he reviewed a new book that he had known about for more than twenty years. “If there is one theme to Volume Four, it is Anaïs’s formidable will to power,” Vidal wrote, and though he referred in passing to Nin’s inaccurate portrait of young “Lieutenant Vidal,” he used the occasion to establish once again what he believed were their differences in philosophy, knocking her “contempt for intellect . . . her mystical belief in Love”—as if slamming an old friend’s book in her home-town newspaper (Nin had lived in L.A. for years) did anything but show that love might be a useful thing to have around.

The Nin review was a classic instance of what Michael Mewshaw, in his recent memoir “Sympathy for the Devil: Four Decades of Friendship with Gore Vidal” (Farrar, Straus & Giroux), calls “the chilly, uncaring misanthrope portrayed by the press.” The man he knew was eager to please and sensitive to slights—and not always happy to show it. His late memoirs, “Palimpsest” (1995) and “Point to Point Navigation” (2006), Mewshaw writes, “contained strikingly little introspection, few truly intimate revelations . . . almost nothing about the people he mixed with on a daily basis”—and no hints that Vidal “was generous, hospitable, loyal to friends.”

Reprising Nin’s response to “The City and the Pillar” half a century earlier, Mewshaw spends a few pages scratching his head over “Palimpsest,” an account of Vidal’s first four decades. He grew bored by Vidal’s presentation of himself as “the yoga master of world-weariness” and expressed bafflement as to “why so many of the emotions I had witnessed in him over the years had been deleted from this draft of his life.” (Writing about “Point to Point Navigation,” Parini admires the way Vidal “restrains his emotions.”) Mewshaw recalls reading “Palimpsest” when he was living in London and Vidal was in town on a promotional tour, and noting the disparity between the book’s announcement of personal happiness—“I am past all serious desire for anything. . . . The Buddha was right: To want is to suffer”—and “the man who just the night before at the Connaught had drunk himself to the brink of unconsciousness.”

In the opening pages of “Sympathy for the Devil,” Mewshaw expresses the hope that the noise around Vidal has died down enough to “allow for” an “alternative assessment.” So far, the evidence seems mixed. In a five-hundred-word “lightning raid” that appeared on the Vanity Fair Web site, James Wolcott—the author of “The Gore Supremacy,” a Kindle Single published after Vidal’s death—objected to the offscreen, life-size figure in Mewshaw’s book, especially his “glassy-eyed, falling-down senior moments.” Vidal shouldn’t be remembered that way, Wolcott said. He was suave and smooth. He amused without effort. He didn’t care what people thought—and he didn’t care who knew that he didn’t care what people thought. Wolcott consoled himself with picking out the “hilarious aperçus and asides” recounted in the book, as when Vidal challenged Mewshaw to name the three saddest words in the English language, and supplied the answer: Joyce Carol Oates. This is the image of Vidal that seems destined to endure: cartoonish, two-dimensional, other than human. But, although Vidal made “harsh critical remarks” about Oates at every opportunity, Parini explains, he once caught him reading a volume of her essays, and—“he admitted”—enjoying them. ♦

<--------->
Books OCTOBER 19, 2015 ISSUE

Sorry Not Sorry
Reading Dalkey Archive Press’s Library of Korean Literature.

BY ED PARK
SHARETWEET
 10_19_15
TABLE OF CONTENTS
Need to stop reading?

Enter your email address
 GO
We'll send you a reminder of where you left off.
 The best South Korean fiction coats the country’s existential tumult in dark humor.
The best South Korean fiction coats the country’s existential tumult in dark humor.
CREDIT ILLUSTRATION BY HANNAH K. LEE
A casual reader of the news from South Korea could be forgiven for wondering whether Koreans apologize more than other people do. Public expressions of contrition abound. Last year, President Park Geun-hye apologized for the government’s mishandling of the Sewol ferry disaster, and JoongAng Ilbo, one of the country’s major newspapers, ran a full-page apology for its sensationalist coverage of the tragedy. When, earlier this year, the MERS virus spread through Seoul’s Samsung Medical Center, the minister of health apologized “for causing concern and anxiety” by underestimating the disease’s contagiousness, and the heir apparent to the Samsung Group did the same, bowing deeply from the waist on national television. Then, there was last year’s “nut rage” incident, in which a Korean Air Lines executive went berserk after a flight attendant served her macadamia nuts in a bag rather than in a bowl. She demanded that he beg her forgiveness, only to apologize herself, later, as did her father, the company’s chairman, and her sister, who had threatened to seek vengeance on whistle-blowing employees. And Korean-Americans might recall that the country’s Ambassador to the United States called on them to “repent” after it was discovered that the gunman who carried out the Virginia Tech massacre was born in Korea, proposing a thirty-two-day fast, one day per victim, to prove that Koreans were a “worthwhile ethnic minority in America.”

Hierarchy—social, corporate, political—is the major organizing principle of Korean life, and apology is one of its crucial mechanisms. When those lower down the chain screw up, decorum demands that they apologize to those higher up; when those higher up wrong those lower down, apology functions as an affirmation of accountability, an expression of responsibility of the few toward the many. South Korea perennially demands apology from Japan, its former colonizer, which in 1993 acknowledged forcing women (many of them Korean) into sexual slavery during the Second World War. There are practical reasons for wanting such repeated reassurance; the rise of aggressive nationalism in a neighbor that has invaded you countless times across the centuries is certainly a distressing trend. But South Korea’s insistence on fresh acknowledgment of misdeeds long past, and its distress when such acknowledgment fails to come, also stem from the quintessentially Korean concept of han, a mélange of sadness, rage, and despair—a condition born of a sense of oppression and grievance, and impossible to assuage by apologies alone.

The Korean apology is satirized to harrowing effect in “At Least We Can Apologize,” a darkly comic 2009 novel by the South Korean writer Lee Ki-ho, published in this country by Dalkey Archive Press as part of its Library of Korean Literature series. The narrator, Jin-man, is equipped with a literal mind and a disconcerting lack of curiosity, and lives at “the institution,” a disreputable mental ward that doubles as a sock-packaging plant. Fluorescent lights burn around the clock, and the staff subdues residents with daily cocktails of pills. “When I first entered the institution I was beaten almost daily,” Jin-man recounts, in Christopher J. Dykas’s translation. “I was beaten in the morning, beaten at lunchtime, and beaten before bed.” As he goes through the menu of brutality, a certain giddiness sets in:

I was beaten with a pointer, beaten with a steel pipe, slapped, punched, kicked with a booted foot, and even beaten with a thick book. I was beaten with a chair, beaten with a trashcan, beaten with socks, and beaten with a shovel. After being beaten like this for some time, one day I looked over and there was Si-bong. He had both arms wrapped around his head as he was being beaten. That was the first time that Si-bong and I met. After that, we were beaten together every day. We were beaten together under our beds, beaten together in the hallway, beaten together after being called into the office, beaten together in the workroom, beaten together on the hill behind the institution, and beaten together in front of the main gate. Being beaten together like that for so long, we became friends.
Falsely confessing to random wrongdoing—swearing at their superiors, throwing out medication—results in milder punishment, so Jin-man and Si-bong learn to game the system:

Si-bong admitted to cursing the caretakers again and was beaten repeatedly in the thighs with a steel pipe. The caretakers said that committing the same wrong again was an even greater wrong. So we had to come up with new wrongs every day. Some of them became “wrongs,” while others became “greater wrongs.” On days we committed wrongs, we were beaten less, on days we committed “greater wrongs,” we were beaten a lot, and on days we admitted to nothing, we were beaten repeatedly all day long.
Jin-man and Si-bong are honest liars. They always make sure to commit their offenses after admitting to them, proving so adept at the racket that their caretakers put them in charge of collecting the apologies of the other inmates. This equilibrium is interrupted by the arrival of a new guy, “the man with the sideburns,” who despairs at his confinement and tosses messages over the institution’s fence in an attempt to reach the outside world. Jin-man and Si-bong start to copy him, packing their notes—“We are being held captive. If you find this note, please report this to the police. The man in our room said that you will be generously rewarded”—into sock crates. The messages hit their mark. The institution collapses in scandal, and Jin-man and Si-bong emerge to a media frenzy. Camping out at the dingy apartment of Si-bong’s sister and her pimp boyfriend, they fruitlessly hunt for jobs until they hit on the idea of marketing their sole indisputable skill: apologizing for someone else’s sins.

One of their first customers is a ten-year-old boy who has stolen money from his mother’s purse. Jin-man and Si-bong accompany him to his mother’s small food shop, where the irate woman threatens to “break this little bastard’s wrist.” Primed by their impeccable training at the institution, the newly minted businessmen spring into action, offering their own bodies up for abuse. As mother and son look on in horror, Si-bong takes a pipe and whacks Jin-man repeatedly on the wrist.

“An apology means that you say you’re not going to do the same thing that you did before,” Si-bong explains to another customer. “That’s all it is. There’s nothing we can do about your feelings, sir.” By outsourcing a gesture whose only value comes from the intent behind it, Jin-man and Si-bong turn the apology, that most civilized of interactions, into a mercenary performance, a backstreet Grand Guignol. It’s a lucrative one, too. “There are wrongs upon wrongs out there,” the pimp says, with growing excitement at the new business’s possibilities. “That means the apologies will just keep coming.”

“At Least We Can Apologize” is divided into three sections, whose titles—“Finding Wrong,” “Creating Wrong,” and “Cultivating Wrong”—describe a surefire, if unmistakably cynical, business strategy. What started, at the institution, as a simple means of survival becomes, in the outside world, an industry with the promise of limitless growth.

For American readers, literary evocations of Korea have come, for the most part, in the form of dystopian novels written by people without any direct connection to the country. Adam Johnson’s Pulitzer Prize-winning novel, “The Orphan Master’s Son,” is set in the harsh confines of North Korea; at the other extreme, David Mitchell’s “Cloud Atlas” features a futuristic South Korea-inspired “corpocracy,” a hotbed of clones, plastic surgery (“facescaping”), and insurrection. With few exceptions, novels by actual Koreans have not registered here. Kyung-sook Shin’s “Please Look After Mom” briefly appeared on the Times best-seller list in 2011. (She made headlines this year amid charges that she once plagiarized passages from a Yukio Mishima story, for which—yes—she later apologized.) Kim Young-ha’s “I Have the Right to Destroy Myself” (2007) and Hwang Sok-yong’s “The Old Garden” (2009) both received a trickle of reviews, and Yi Mun-yol’s story “An Anonymous Island” appeared in these pages in 2011. That’s about it. Happily, Dalkey Archive’s series, launched in 2013, in collaboration with the Literature Translation Institute of Korea, provides a panoramic view of Korean fiction, in all its strangeness and variety, from the nineteen-thirties to the present.

Cartoon
“You can lie to me, you can lie to your trainer, you can even lie to yourself, but you can’t lie to your Fitbit.”
BUY THE PRINT »
That’s a significant span in the life of any country, but all the more so in the case of Korea. Two decades ago, when I spent a year in Seoul, the city my parents came from, after I graduated from college, I couldn’t have fathomed that South Korea would become an epicenter of state-of-the-art anything; there was hardly any evidence that a new, high-tech, high-speed civilization was on the way. Things changed after the Asian financial crisis of 1997. As Euny Hong detailed last year, in her book “The Birth of Korean Cool,” the South Korean government, reeling from the recession, decided to invest in pop culture as a prime export, resulting in the wildly popular boy bands and girl bands and soap operas that went on to make up hallyu, the wave of Korean culture that has swept over Asia, and, increasingly, the rest of the world. These days, South Korea is famous for being among the most wired countries in the world, with whip-fast Internet speeds and a smartphone in every hand. Thousands fill stadiums to watch video-game tournaments, and plastic surgery seems as common as hair dye. It sounds like science fiction.

Such breakneck change can’t help but come at a price. The titular mother in “Please Look After Mom,” for instance, travels from the countryside to Seoul to visit her grown children, only to get lost in the subway. The novel captures the unsettling dislocation of the country’s rapid rural-to-urban transformation, and the transition from an elder-venerating Confucian hierarchy to a youth-focussed culture obsessed with physical beauty. This degree of change has left a deadly legacy: as Kim Young-ha noted in a Times Op-Ed last year, South Korea’s suicide rate has been the highest in the industrialized world for eight years running.

The novels in the Library of Korean Literature series are populated with the broken and the dispossessed, young drifters, like Jin-man and Si-bong, looking to carve out a place for themselves in an ungraspable, shifting world. Another such character introduces himself in the first sentence of Jang Jung-il’s novel “When Adam Opens His Eyes,” translated by Hwang Sun-Ae and Horace Jeffery Hodges: “I was nineteen years old, and the things that I most wanted to have were a typewriter, prints of Munch’s paintings and a turntable for playing records.” The nameless narrator (he’s called Adam by a lover, in honor of his being her first man) hasn’t scored high enough on the standardized exam to get into the university of his choice, so he plans to spend a year cramming.

Naturally, he doesn’t lift a finger to accomplish that goal—which isn’t to say that he does nothing. A hundred pages later, he buys a typewriter, and with it the promise of a different, differently programmed life. “If I write a novel, I will begin by depicting the portrait of my 19th year this way,” he says, and then quotes the book’s first paragraph nearly verbatim. This seems an optimistic conclusion—the narrator has made something of himself, and we’ve just finished reading the evidence—but, on the next page, Jang violently drops us into the novel’s wildly discordant final section, “The Seventh Day.” If the book’s first stretch was a study in passivity, “The Seventh Day” is all action: sex, lots of it, between an unnamed man and woman, graphically described and mixed with literary chat. “No virgin finds climaxing easy in her first experience,” Jang deadpans. “Except that this is a porno novel.” (The transgressive 1999 film “Lies,” which might be retitled “Fifty Thousand Shades of Grey,” was based on another of Jang’s novels.) Like the coda to Don DeLillo’s “The Names” or Wong Kar-wai’s “Days of Being Wild,” the end of “When Adam Opens His Eyes” seems spliced in from a different work. Who are these nameless, insatiable characters? Maybe they are yet another product—concentrated, unbearably intense—of the narrator’s typewriter, the vision that comes with Adam’s newly gained knowledge of the world.

“When Adam Opens His Eyes” was published in 1990, before South Korea’s great pop boom; the narrator’s typewriter and cassette player are practical necessities, not ironic totems of a bygone age. But a number of more recent novels betray a certain nostalgia for an earlier, less technological time, when life didn’t have to be constantly mediated by a screen. No computers show up in “At Least We Can Apologize,” and when Jin-man and Si-bong make calls they do it strictly via pay phone. A similar analog atmosphere can be found in “No One Writes Back,” by Jang Eun-jin, also published in 2009, and translated by Jung Yewon. “I left home with an MP3 player and a novel in an old backpack,” the novel begins. The speaker is Jihun, who for three years has moved from motel to motel with his late grandfather’s faithful, though blind, guide dog. He spends his time looking for places to stay, carrying on a one-sided correspondence with the people he meets on his rambles, and skirting his own vast, withheld sorrow. “I write letters because I want to convey to someone the stories of these people,” he explains, “but also because I want to let someone know that a day had existed for me as well.” One gets the sense that the immediacy of text messaging and e-mail would be too much for Jihun to handle; he wants to make contact with other people, but not at the expense of keeping his distance.

“No One Writes Back” is composed of short, numbered chapters, its progression echoing Jihun’s own peripatetic existence. As if to avoid the complications that could come from any budding intimacy, Jihun assigns numbers rather than names to the people he writes to. “My name is . . . ,” one of the people he encounters, a writer selling her novel on the subway, starts to tell him. He cuts her short: “ ‘I don’t want to know,’ I say, because I fear that we really will have to get to know each other once we start calling each other by name.”

The book’s centerpiece is Jihun’s letter to his sister, who has become a cosmetic-surgery addict. “With scissors in hand, you cut up all the photos with your face in them, and even burned up the photos of your hundredth day celebration and your first birthday party,” he writes. The letter is a heartbroken critique of a society gone insane with images. Seen through Jihun’s eyes, the Korean craze for such facescaping starts to seem a sort of unconscious sacrifice: in order to be properly absorbed, the dramatic changes visited on the nation need to be visited on the body as well.

The most appealing novels in the Library of Korean Literature capture the existential turbulence of han while keeping a sense of humor about it. The didactic moments in Yi Kwang-su’s “The Soil,” a social-realist tome originally serialized in 1932 and 1933, are balanced with wry observations of customs and people, such as the modern man who has internalized Japanese values and looks down his nose at his country’s educational system: “Yes, there’s the Department of Korean Literature. I really don’t know what students learn there. I think literature is useless anyway. And to study Korean literature? Even worse.” (Yi, the most famous writer in the series, was one of the country’s first modernists and a leader of the Korean independence movement, though he was later tarred as a Japanese collaborator.)

More recently, Park Min-gyu’s “Pavane for a Dead Princess,” set in the late nineteen-eighties and translated by Amber Hyun Jung Kim, tracks the doomed romance of its handsome narrator, a valet at a fancy shopping mall, and his co-worker, a shy, intelligent woman who is mocked for being homely—“the world’s ugliest woman.” Though she had the best grades at her vocational school, she’s never promoted; Park is blunt about the unfairness of a society wrapped up in surfaces, in which the unlovely are confined to a kind of permanent underclass, at least until they go under the knife. “Pavane” is a bildungsroman that veers into metafiction, bristling with footnotes and multiple endings. There’s also plenty of comic relief, such as this sterling career advice for a new valet, turning the impulse to apologize on its head:

“Now let’s suppose there’s been an accident. This is what you have to do, so listen and learn. First, take off your armband and cap. Next, run back to the office without looking back. If the supervisor’s there, knock him out. Open the second drawer of his desk and look for your employee record. Either tear that into shreds and swallow it or burn it. Then run straight home. Then start looking for another job. Is that clear?”
When you do something wrong, flee the scene: this would be bad business for Jin-man and Si-bong, inverting the bleak social order that they aim to exploit. Later in “At Least We Can Apologize,” Jin-man and Si-bong are recaptured by the sinister caretakers of the institution; the only way for Jin-man to escape is to sacrifice his friend. “I had committed a wrong against him, but I missed him very much,” Jin-man thinks. “That was all.” Apologies are only a partial salve for wrongdoing; they acknowledge, but do not reverse, the harm that’s been done. Jin-man, it turns out, has a conscience. This discovery recalls a line from the start of the novel, the attempt by the man with the sideburns to open Jin-man and Si-bong’s eyes: “ ‘Look at you! You guys are fine and you’re locked up in here!’ ” Maybe Jin-man and Si-bong were never crazy to begin with—no crazier, in any case, than the country awaiting them outside the gates. ♦

Explore books in The New Yorker Collection on iBooks. (iTunes and iBooks are advertising partners of The New Yorker.)

<--------->
Books OCTOBER 19, 2015 ISSUE

Rebirth of Venus
Robin Coste Lewis’s historical art.

BY DAN CHIASSON
SHARETWEET
 10_19_15
TABLE OF CONTENTS
Need to stop reading?

Enter your email address
 GO
We'll send you a reminder of where you left off.
 Lewis’s catalogue of Western depictions of black women spans forty thousand years.
Lewis’s catalogue of Western depictions of black women spans forty thousand years.
CREDIT ILLUSTRATION BY TONY RODRIGUEZ
Robin Coste Lewis’s début poetry collection, “Voyage of the Sable Venus and Other Poems” (Knopf), derives its title from a notorious eighteenth-century engraving by Thomas Stothard, “The Voyage of the Sable Venus from Angola to the West Indies.” The image was slave-trade propaganda: it shows an African woman posed like Botticelli’s Venus on a weirdly upholstered half shell. She glides serenely across the Middle Passage, attended by an entourage of cherubs and dolphins and escorted by a predatory Triton, who looks as though he’d read the poem on which the engraving is based: Isaac Teale’s “The Sable Venus, An Ode,” which celebrates the pleasures of raping slave women, since black and white—Sable Venus and Botticelli’s Venus—are, after all, the same “at night.”

“The Voyage of the Sable Venus” has made its own voyage—that word’s bitter irony, lost on its original audience, is now its meaning—and ended up in this arresting book, whose title subtly transforms it. Titles establish property; change the title, and you’ve wrested from the history of racism a powerful symbol for the emergence of black women as the depictors of their own lives. But a voyage requires both an origin and a destination, and so the eighteenth-century engraving and the twenty-first-century book operate as two shores in a trip from the lurid past, in which African women were transported to be sold into slavery as property, to the current day, when an African-American woman like Lewis can recast history in her own brilliant, troubling terms. Google the phrase “voyage of the sable venus,” and you will see how these two works are now linked, in both the old and the new senses of that word.

Lewis’s volume, nominated for a National Book Award, takes its place in a line of important reclamations of “Venus” from its use as a humiliating designation for black women and for depictions of them, including Rita Dove’s “The Venus of Willendorf” and Elizabeth Alexander’s “The Venus Hottentot.” (You could argue that Venus Williams, perhaps the most famous Venus since the Roman goddess, has, with tennis as her medium, made her own aesthetic intervention.) Poems can provide the effaced interiority of these caricatures, but the backlog of silenced persons is daunting and the history by no means safely concluded. And so Lewis’s book begins with an aftermath, a morning-after poem, “Plantation,” in which two lovers awake “embracing on the bare floor of a large cage,” bound together by an intimacy that is concomitant with confided, and temporarily pardoned, shames:

To keep you happy, I decorated the bars.
Because you had never been hungry, I knew
 
I could tell you the black side
of my family owned slaves.
 
I realize this is perhaps
the one reason why I love you,
 
because I told you this
and you—still—wanted to kiss
 
me. We laughed when I said plantation,
fell into our chairs when I said cane.
Those “bars,” reflected in the couplets on the page, stand for the innate possessiveness of our gaze; the “you” whose happiness depends on the cage being made pretty is, partly, the reader, who is lured by beauty to the site of pain, and whose scrutinizing presence there turns a bedroom into a prison, or perhaps a zoo. Sex, far from being a reprieve from the humiliations of the past, expresses them; it promises to soar above social and historical identity, but it’s more like an early Wright brothers plane, skipping and wobbling from one degradation to the next: the lover who laughs one minute, changes “every now and then” from “a prancing black buck”—the white stereotype for black male sexual threat—to a “small high yellow girl: pigtailed, / patent leather, eyes spinning gossamer, begging / for egg salad and banana pudding.” Experience and innocence, the “black buck” and the light-skinned girl, are elements of a single racist trope, whose tensions well up in every act of tenderness:

And then you were fourteen, and you had grown
a glorious steel cock under your skirt. To brag
 
you rubbed yourself against me. Then your tongue
was inside my mouth, and I wanted to say
 
Please ask me first, but it was your
tongue, so who cared suddenly
 
about your poor manners?
The poem gets progressively more nightmarish: the “tongue . . . inside my mouth” is an image for speech and for its effacement under circumstances that here seem ecstatic, there violent. Are these lovers, or successive versions of a single person, her sexuality tainted by history? By the end of the poem, both parties have suffered a grotesque, Ovidian transformation: “You said, The bars look pretty, Baby, / then rubbed your hind legs up against mine.”

Lewis is a Ph.D. candidate at the University of Southern California with a fellowship in Poetry and Visual Studies, and those disciplines join together in her book’s title poem—seventy-nine pages long, including notes and appendices—which is itself made up entirely of the “titles, catalog entries, or exhibit descriptions” of objects in Western art that depict the black female form, going back to 38,000 B.C. The sheer bulk of material that Lewis turned up in her research, and the relentlessness of the descriptions, suggests that the history of the black female body is inextricable from the institutions that claim ownership of its depictions; the subjugation is translated into symbolic terms but never undone. Two haunting epigraphs hang over the poem: an announcement, from 1939, for the Metropolitan Museum of Art’s “Employees’ Association Minstrel Show and Dance,” and an inquiry by one “Mrs. B. L. Blankenship,” who writes that she is “anxious to buy a small healthy negro girl—ten or twelve years old.” The latter confronts us with the outright barbarism of slavery and its attendant transactions, while the former suggests how thin a disguise racism must wear at any given moment, in any given cultural institution, to pass for what we optimistically call “culture.”

Poems describing works of art are nearly as old as poems and works of art. The name for this hoary genre is ekphrasis, though in Lewis’s hands its conventions are scrambled. The problem isn’t that the works of art are silent and need a voice; it’s that we encounter them inside institutions that title and describe them for us, pre-assigning them meaning. Poetry can’t imagine these artifacts from scratch, since their labels adhere so tenaciously to them. And so Lewis followed some stringent, self-imposed rules in composing this long poem, altering nothing about her source language except its punctuation. This puts extraordinary pressure on the fundamentals of prosody. Line breaks, stanza shapes, the management and distribution of words and silence—this small repertoire of formal options is here weaponized for maximum impact. The poem moves chronologically; this is a passage from one of its early sections, “Catalog I: Ancient Greece and Ancient Rome”:

Statuette of a Woman Reduced
to the Shape of a Flat Paddle
 
Statuette of a Black Slave Girl
Right Half of Body and Head Missing
 
Head of a Young Black Woman Fragment
from a Statuette of a Black Dancing Girl
Lewis’s technique returns the humanity to these anonymous women, which, in turn, makes the objects depicting them feel like examples of, and even instruments of, real historical violence. In a gallery, it seems perfectly natural to see a woman missing her head and half of her body. Here, in a list that feels like a catalogue of atrocities, it’s nearly unbearable.

A later, formally inventive section divides women from the objects that embody them by a method of formal panning, where the residue of personhood is extracted and isolated on the page:

water jar
 
bowl
 
ointment spoon
in the form of swimming
black girl
 
mirror
with handle
in the form of a carved standing
black girl
Since form is essentially neutral, its effects are wildly variable; in some instances, the technique redeems the women, who were literally objectified, but only, after all, by objectifying them in a different form. Separated from their subjects, these objects that have taken the shape of black women become formless: the poem’s own form corrects the problem by building a sombre temporal interval between object and person, as our eyes make their own voyage from the left to the right margin.

First books usually spend longer in the chrysalis than later books, and often feel more like finish lines than like starting blocks: a writer never again gets so much time off the clock. “Voyage of the Sable Venus” was made over roughly forty thousand years, if we take seriously Lewis’s continuum of historical and autobiographical time. In its final sections, many of the art works listed are by black women, including a “Venus of Compton,” whose presence suggests not only the tennis star but also Lewis’s own birth in that city. The effect is magical, a little like Hermione’s abrupt transformation, at the end of “The Winter’s Tale,” from statue to living woman. All those women made into serviceable, mute paddles and spoons, missing their limbs and heads, are, by the miracle of verbal art, restored. This may be one reason the poem is dedicated to “the legacy of black librarianship, and black librarians, worldwide”; something of their oblique, channelled genius, expressed in the social space of the library as a “collection” of books by others—black and white, dead and living—is here transferred to the aesthetic space of Lewis’s own many-chambered and remarkable collection. ♦

Explore books in The New Yorker Collection on iBooks. (iTunes and iBooks are advertising partners of The New Yorker.)

<--------->
Books OCTOBER 12, 2015 ISSUE

The Time of Broken Windows
New York from punk to Trump.

BY LOUIS MENAND
SHARETWEET
 
TABLE OF CONTENTS
Need to stop reading?

Enter your email address
 GO
We'll send you a reminder of where you left off.
 Crosby Street, SoHo, in 1978.
Crosby Street, SoHo, in 1978.
CREDIT PHOTOGRAPH BY THOMAS STRUTH
To cut unceremoniously to the chase, yes, as you might suspect, or fear, Garth Risk Hallberg’s new, much promoted, nine-hundred-and-forty-four-page novel, “City on Fire” (Knopf), is about four hundred pages too long. Hallberg is a gifted writer, so the pages go by pleasurably. His book is never flaccid or flat, but it does not leave you wishing for more. He tried to squeeze too much juice out of the apple.

“City on Fire” is basically a detective story, from a recipe that goes back to Dickens: an apparently random event—in this case, the shooting of a punky N.Y.U. student named Samantha in Central Park on New Year’s Eve—turns out to be a thread that, when pulled, unravels a web of intrigue that stretches from a midtown office tower to an abandoned building in the East Village. The novel features a dozen or so characters from varied walks of New York life—a cop, a reporter, a schoolteacher, a punk rocker, a fireworks-maker, an assistant in a gallery, an investment banker, and so on. As the secrets are revealed, each of them turns out to be one degree of separation or less from the others.

It’s all highly implausible, of course. Contrary to what newcomers to the city may imagine, New York is a place where circles almost never intersect, except transactionally—at co-op meetings and parent-teacher conferences, or on jury duty. New Yorkers circulate mostly within their own class and occupational orbits. Manhattan is a hundred small towns unevenly distributed over some twenty-two square miles of city space.

But the genre that “City on Fire” belongs to requires a suspension of disbelief on this point. Implausibility is part of the design. The plots of Dickens’s big “condition of England” novels are implausible in the same way. In the London of “Bleak House,” a connection between Lady Dedlock and Jo the street-sweeper didn’t have even a transactional basis. The aim of these novels is not to mimic actual city life, where people tend to be like hamsters in their own cages. It’s to dramatize a hidden interdependence, to show that we are all, each according to our abilities, turning the same big socioeconomic wheel inside the same spatiotemporal cage.

The spatiotemporal cage in “City on Fire” is New York in the nineteen-seventies. The main action takes us from Christmas, 1976, to July 13, 1977, the night of the New York City blackout. (The night, to be precise, of the second New York City blackout; the first was in 1965. A third blackout happened in 2003.) The book opens with a bang. The setup, introducing the main characters and ending with the shooting and the start of the police investigation, is expertly handled on every level: characterization, setting, pace, suspense. It takes up only a hundred and thirty-one pages.

Then the backstories start. There are also flash-forwards, plus several “interludes,” consisting of documents printed in a variety of fonts (a New Wave zine, the typescript of a magazine piece, an artist’s notebook). In all, the novel has six of these interludes, ninety-four chapters, a prologue, and a postscript. Relationships get reshuffled and a lot of family history accumulates, but the book has just the one plot. The style is naturalistic—no Pynchonian flights of fancy—with some bravura passages (inside the mind of someone on heroin, for instance) and bits of life wisdom appropriate to the characters’ personalities and perspectives.

It’s rude to speculate about an author’s motives, so let’s just say it’s as though Hallberg had had an idea for a great, “Chinatown”-like whodunit about New York but felt some sort of ethical or professional duty to turn the book into an art novel. (In fact, although “City on Fire” is being advertised as Hallberg’s first novel, he has published an art novel, “A Field Guide to the North American Family,” a cross-indexed mixture of prose and photographs.) Hallberg has the talent to bring a character to life in a few pages, and that’s really all he needs for the purposes of his plot. The backstories and the rest iterate more than they complicate.

Another reason for the length is an overreliance on the technique known as free indirect style—the focalization of the narrative through the mind and voice of a character. It’s a great device for representing experience from the inside, but it carves the world into a series of perspectives. Here is the first paragraph of the first chapter:

A Christmas tree was coming up Eleventh Avenue. Or rather, was trying to come; having tangled itself in a shopping cart someone had abandoned in the crosswalk, it shuddered and bristled and heaved, on the verge of bursting into flame. Or so it seemed to Mercer Goodman as he struggled to salvage the tree’s crown from the battered mesh of the cart. Everything these days was on the verge. Across the street, char-marks marred the loading dock where local bedlamites built fires at night. The hookers who sunned themselves there by day were watching now through dime-store shades, and for a second Mercer was acutely aware of how he must appear: a corduroyed and bespectacled brother doing his best to backpedal, while at the far end of the tree, a bedheaded whiteboy in a motorcycle jacket tried to yank the trunk forward and to hell with the shopping cart. Then the signal switched from DON’T WALK to WALK, and miraculously, through some combination of push-me and pull-you, they were free again.
It’s a nicely crafted curtain-raiser. It signals a time (the de-institutionalized homeless—the “bedlamites”—began showing up on the streets in 1965; the WALK/DON’T WALK signs got phased out starting in 2000) and a place (Eleventh Avenue loading docks plus prostitutes equals Hell’s Kitchen), and it introduces two major characters. Mercer is a nerdy African-American from Georgia who teaches in a private school for girls in the Village, loves nineteenth-century novels like “Lost Illusions” and “The Red and the Black,” and is trying to write a novel of his own (not, as it turns out, this one). His boyfriend, William, is the estranged scion of a banking family, an ex-punk-band leader, public-restroom cruiser, and heroin addict. He is trying to finish a painting.

But the “lens” on the encounter in a crosswalk between a Christmas tree and an empty shopping cart (a tidy image of economic dissonance) is Mercer. Everything in the scene is as he sees it and thinks it. “Bedlamite,” an obsolete Britishism, is there because it’s a word that a literary guy like Mercer would know. Even the description of Mercer is Mercer’s description. Every chapter is in this mode. We get the picture from one point of view at a time. There are few wide-angle shots. The story is assembled in slices.

Hallberg has an M.F.A. from N.Y.U. and lives in New York, but he was born in Louisiana, grew up in North Carolina, went to college in Missouri, and was not alive in the nineteen-seventies. To a person who did live in New York in the nineteen-seventies—to wit, this person—his powers of evocation are uncanny. Hell’s Kitchen, the Bowery, Central Park West, the subway, the L.I.R.R.—it’s as though he’d once walked those streets, ridden those cars.

He acknowledges the help of several books on New York, including Ken Auletta’s classic “The Streets Were Paved with Gold,” published in 1979, and Jonathan Mahler’s more recent “Ladies and Gentlemen, the Bronx Is Burning.” Slips are minimal. (The tag of the pioneer graffiti artist Taki was Taki 183, not Taki 8, for instance.) But “City on Fire” is not overstuffed with period detail. It’s not that kind of historical novel.

There is virtually no mention of the Yankees, for instance, although they won the World Series in 1977, under the guidance of the thin-skinned sourball Billy Martin, or the campaign for mayor, at the end of which, to the dismay of liberals, Ed Koch defeated Mario Cuomo. Son of Sam, then known as “the .44-calibre killer,” is not in the book, although he killed five people between January and July, 1977, and was finally captured that August.

It’s not the facts that bring the nineteen-seventies to life in “City on Fire.” What Hallberg is after is an atmosphere, and he gets it. He gets the assaultive feeling the city had in those bombed-out years. He gets the ubiquitous defacement of public surfaces, the shuttered shops and derelict street people, the soul-destroying round-the-clock noise. His New York is a city that never sleeps not because there’s always more fun to be had but because it has insomnia.

Cartoon
“Do we have to use our inside voices through clenched teeth, like you, Ms. Baker?”
BUY THE PRINT »
But he is (like Dickens) a romantic about human nature. “City on Fire” will probably be compared with Tom Wolfe’s big, class-intersecting novel about New York in the nineteen-eighties, “The Bonfire of the Vanities.” You read it here first that this would be a mistake. “The Bonfire of the Vanities,” as the allusion to Thackeray tells us, is a satire. “City on Fire” is not remotely satirical. The good guys are truly good, or, at least, they have honorable intentions and suffer remorse when they fall short. The few characters who are without a conscience had tough childhoods. The system is not to blame, it seems, nor is human folly. It’s just that some people manage to transcend their family mess (every family in the novel is some sort of mess), and some can’t, in which case they might deal with their pain by doing bad things, like burning down the South Bronx.

Hallberg is also a romantic about the nineteen-seventies. That may seem a strange species of nostalgia. The decade between 1972 and 1982 was the worst extended economic period since the nineteen-thirties. There were two oil crises, the first in 1973, when the price of a barrel nearly quadrupled, and the second in 1978-79, when it tripled. The stock market crashed in grim slow motion. Between 1972 and 1974, the Dow Jones Industrial Average lost almost half its value, and the market did not get back to 1972 levels until 1982. In 1975, unemployment jumped to 8.5 per cent. The inflation rate exceeded ten per cent. By late 1980, the year Ronald Reagan was elected President, the prime rate was twenty per cent.

In New York, it was the same only worse. From 1970 to 1976, the city lost more than six hundred thousand jobs. By 1976, unemployment stood at eleven per cent, and one in every seven New Yorkers was on welfare. The city was broke and had to be bailed out. “FORD TO CITY: DROP DEAD” was the famous Daily News headline, but Ford did not in fact say those words, and Congress eventually provided the financial assistance the city needed to climb back out of the red.

The price for that aid was austerity measures, and the reduction in city services, as well as poor decisions about things like the allocation of firehouses, led to areas of the city, like the Lower East Side and the South Bronx, becoming wastelands of drugs, abandoned buildings, muggings, robberies, and arson. Signs of blight were everywhere, a kind of urban eczema.

New York felt empty—there were so many parts of it where people didn’t want to go—and out of control. It was the time of broken windows. But, in part because of the collapse, the city also felt open, liberated, available. Anything seemed possible, especially to people who didn’t have much to begin with—avant-garde artists and performers, New Wave musicians, experimental writers, advanced students of the society of the spectacle. It wasn’t quite Saint-Germain-des-Prés after 1945 or Berlin after 1989, but Manhattan in the nineteen-seventies had a kind of locally grown cultural magnetism.

The piece of the downtown subculture that Hallberg takes on is the music scene. It was centered on two clubs, C.B.G.B., at Bowery and Bleecker, and the renovated Max’s Kansas City, on Park Avenue South. Those clubs were where groups like the Ramones, Television, and Blondie—groups at first known only by word of mouth—performed. It cost a dollar to see the Ramones at C.B.G.B. (You had to pay for drinks.)

The main downtown characters in “City on Fire” are the members of a punk band called Ex Nihilo and assorted hangers-on. The most successful of these is Charlie, a geeky Long Island teen-ager who is making the difficult taste transition from David Bowie to Patti Smith. To the extent that “City on Fire” is a novel of education, like the nineteenth-century novels that Mercer is obsessed with, Charlie is the hero. As a detective story, though, the book has two main actors: the leader of Ex Nihilo, who uses the stage name Nicky Chaos, and his uptown counterpart, Amory Gould, a businessman who has a scheme to make money from urban blight and might require a little surreptitious help with the blight part.

Nicky is a practitioner of the Situationist art of negation. He is a firebug who collects Herb Alpert records and talks about Marx and Nietzsche. He describes himself as a “post-Humanist,” and says things like “Choice isn’t the same thing as freedom—not when someone is framing the choices for you,” and, “This is the ’70s now, the death trip, the destruction trip, the internal contradictions rumbling and grumbling, the return of the repressed. It’s the system, having swallowed everything, having indigestion.”

He also says, “No more art. No more trying to change the culture with culture.” He’s a revolutionary who thinks that people won’t rise up unless their lives get materially worse. Underlying his arrangement with Amory is a bet over whether increasing the blight will bring on class warfare or simply hasten redevelopment.

The climax of the novel is set during the blackout. July 13, 1977, was a very hot day. At 8:37 P.M., during a thunderstorm, power lines in Westchester carrying electricity to the city’s grid were hit by lightning. Con Ed was unprepared to cope with the demand, there were more lightning strikes and a chain reaction, and by 9:36 P.M. all five boroughs (except for portions of Queens) and a few northern suburbs were without electricity.

The looting and arson began almost immediately. It was twenty-five hours before power was restored, and by then a thousand serious fires had broken out, more than fifteen hundred businesses had been looted or set on fire, and thirty-seven hundred people had been arrested. Rioters did not touch upscale businesses on the Upper East Side or, for the most part, in midtown. They looted and burned the mom-and-pop stores and bodegas in their own neighborhoods. About half of those arrested were unemployed, but this meant that about half were not. It was a frenzy of self-destruction.

Hallberg’s blackout is a (hundred-and-twenty-page) Walpurgisnacht of rage, craziness, and anarchy, but he also treats it as one of those suspensions of the conditions of ordinary life that allow things, under cover of chaos, to sort themselves out. By the next day, the mystery of the shooting has been solved and the web of intrigue revealed, and the dozen or so characters, having been brought together, now begin to separate, off to fresh treadmills in cages new.

The rioting killed any Situationist dreams of a new Commune. As the writer Luc Sante, then a graduate student living on the Lower East Side, later put it, “The looters were exemplary Americans, whose immediate impulse in a crisis was to see to the acquisition of consumable goods. They had no interest in power. Neither did anyone I knew. We just wanted power to go away.” The blackout did not kill the arts scene, though. The damaging blow came in 1981, at the very end of that rough decade, with the first signs of the AIDS epidemic.

It may strike some readers as a little odd to find that the novel’s sympathies seem to lie, ultimately, with the uptown businesspeople, and not with the punks. Amory was right: the recession may have driven many middle-class families out of the city, but it was a great business opportunity. One cause of the arson was landlords choosing to burn their buildings for the insurance. This meant that there was plenty of distressed property, all over the city, for bold speculators to snap up.

That’s how Donald Trump got his start. In 1974, when he was twenty-eight and had never built a thing in Manhattan, he set out to acquire property owned by the bankrupt Penn Central railroad. One of those pieces was the Grand Commodore Hotel, on Forty-second Street, next to Grand Central. According to Trump’s account, in his first book, “The Art of the Deal” (required reading for those of us preparing for a Trump Presidency), he got the city to agree to let him pay taxes on the property based on its assessed value in 1975. Trump renovated the hotel, and, in 1980, it reopened as the Grand Hyatt. The tax abatement was good for forty years, and is estimated to have been worth sixty million dollars in its first decade.

A great number of the jobs lost between 1970 and 1976 were manufacturing jobs. The city was going through a painful and inadequately planned-for transition from an economy based on manufacturing to one based on financial services. It needed the markets to recover, but, when they did, all the amenities of urban professional life recovered with them. Gentrification drove the S.R.O.s out and real-estate values up. The old Times Square, a squalid realm of peep shows and B movies, was turned into Disney World Manhattan.

Upscale New Yorkers today may complain about the gentrification and the commodification and the tourists. But not many of those people would have lasted long in the city of 1975. They would have found it so unhealthy. People smoked in restaurants and did not clean up after their dogs. Forget about cell phones and Wi-Fi; most people didn’t even have cable. If you parked your car on the street, someone would steal the radio. There were no espresso bars in the nineteen-seventies, no Mario Batali or David Chang or Dan Barber. There was Chock full o’ Nuts, and people ate in places like Lüchow’s and Mama Leone’s, huge barns of fat and cholesterol. Gyms were for pumping iron, not for doing Pilates. No one had ever heard of Pilates.

It was another era, remote not just in time but in spirit, and now that we know how it all came out it’s nice to have a book that brings a little of it back. You can always close a book. ♦

<--------->
Books OCTOBER 5, 2015 ISSUE

Naked Cities
The death and life of urban America.

BY ADAM GOPNIK
SHARETWEET
 10_05_15
TABLE OF CONTENTS
Need to stop reading?

Enter your email address
 GO
We'll send you a reminder of where you left off.
 New York’s grid wasn’t the result of a long-range plan but more of a big “Why not?”
New York’s grid wasn’t the result of a long-range plan but more of a big “Why not?”
CREDIT PHOTOGRAPH BY MATTHEW PILLSBURY / BENRUBI GALLERY
Cities can’t win. When they do well, people resent them as citadels of inequality; when they do badly, they are cesspools of hopelessness. In the seventies and eighties, the seemingly permanent urban crisis became the verdict that American civilization had passed on itself. Forty years later, cities mostly thrive, crime has been in vertiginous decline, the young cluster together in old neighborhoods, drinking more espresso per capita in Seattle than in Naples, while in San Francisco the demand for inner-city housing is so keen that one-bedroom apartments become scenes of civic conflict—and so big cities turn into hateful centers of self-absorbed privilege. We oscillate between “Taxi Driver” and “The Bonfire of the Vanities” without arriving at a stable picture of something in between.

Has it ever been acceptable to regard a big city as admirable through and through? Maybe in books about Paris and London from around 1910 to the Second World War, and in books about New York in the years just after the Second World War, before the Dodgers moved and the big fractures began. For the rest, whether it’s Victorian London or post-sixties New York, pop novels and scholarly urbanism are most often voiced in a tone of complaint or querulous warning. (The outlier is the architectural historian Reyner Banham’s 1971 “Los Angeles: The Architecture of Four Ecologies,” still the best book ever written about an American city, its happiness fuelled by an Englishman’s perversity: Everyone says L.A. sucks? I’ll show you it shines.) Nothing urban would be more likely to evoke disgust than a study promoting a benign picture of Bloomberg’s New York—even though, in reality, that city was relatively peaceful (and self-healing from the worst war wound in its history) and prosperous (if more and more unevenly so), with the parks restored or expanding and the subways so safe that they became crowded at two or three in the morning. Those of us who dreamed of the High Line as an improbable public benefit, and then saw it come true, had to accept that it would next become a subject of ridicule, as a cynical developer’s amenity, a green-tinted scam.

The reason that perceptions of cities switch so radically is twofold. Cities are the contradictions of capitalism, spelled out in crowds. They are engines of prosperity and inequality in equal measure, and when the inequality tips poor they look unsavable; when it tips rich, they look unjust. And then cities enfold a subtler contradiction—they shine by bringing like-minded people in from the hinterland (gays, geeks, Jews, artists, bohemians), but they thrive by asking unlike-minded people to live together in the enveloping metropolis. While the clumping is fun, the coexistence is the greater social miracle, though not one that lends itself to stories. Greenwich Village and Park Slope and Southie count as homes and get reverent treatment; a musical might be made of hipsters and Hasidim learning to live together in Williamsburg. But a movie about the lives of the people in a single car on the 6 train would trail off into inconsequence, since the point is that city kinds and lives are so different that contiguity is their only coinciding point. (The one proviso of the local story is that the neighborhood must be under assault and the narrator must side with the old ways, even if he or she is representative of new ones. And so Ray, in Lena Dunham’s beautifully observed, Brooklyn-based “Girls,” runs for the local community board as a champion of preservation, not transformation, though he is utterly typical of the transformative kind.)

The things that give cities a bad conscience are self-evident: seeing the rise of 432 Park Avenue, the tallest, ugliest, and among the most expensive private residences in the city’s history—the Oligarch’s Erection, as it should be known—as a catchment for the rich from which to look down on everyone else, it is hard not to feel that the civic virtues of commonality have been betrayed. Every day brings news of old favorites closed, familiar neighborhoods homogenized, ethnic enclaves turned over to the legions of Capital, not to mention Oberlin and Bard.

Yet the social crises that cities face are remarkably consistent, country to country and town to town. Very little that is going on in New York, from plutocratic excess to outlying gentrification, is not also going on, with different emphases and origins, in London: the same tales of people who drink wine and lattes buying the property of those who drink whiskey and beer. At the same time, cities are local. Saying that Manhattan and central London share the same problems is like saying that a man dying of drink in London is like one doing the same in Manhattan. It’s true, but all the local conditions—what he’s drinking, where he drinks it, who takes him home, and what kind of home he goes to—are so different that a story about the drunk in either place becomes a story about the place. Cities are at once the most cosmopolitan and the most particular of subjects; they require, and rarely receive, a view sufficiently wide-eyed as to become effectively double.

The foundation of the city is its spatial organization, the way its streets meet and the way its citizens travel on them. Gerard Koeppel’s “City on a Grid” (Da Capo) tells the too little-known tale of how and why Manhattan came to be the waffle-board city we know. He shows us that the grid, far from being a long-range plan imposed by a class of managers, was the result more of a shrug, an inconclusive meeting, and a big “Why not?” Koeppel reproduces the key paragraph of the Gouverneur Morris report of 1811:

Whether they should confine themselves to rectilinear and rectangular Streets, or whether they should adopt some of those supposed Improvements by Circles, Ovals, and Stars, which certainly embellish a plan . . . they could not but bear in mind that a City is to be composed principally of the Habitations of men, and that strait sided and right-angled Houses are the most cheap to build and the most convenient to live in.
Koeppel argues, convincingly, that the show of hardheaded rationality here is merely a show. There was no good commercial reason to make a thrifty city of intersections at right angles. London, the model of an imperial commercial city, had its ovals and organic oddities and still prospered. Philadelphia had lovely squares interrupting its own version of the grid. Straight-sided and right-angled houses can be built in circles as well as on street corners. The details of New York’s grid turn out to be surprisingly haphazard and improvisational in their origins. As Koeppel points out, no one has ever provided a good explanation for why the wide two-way streets were chosen to fall where they do—at Fourteenth, Twenty-third, Thirty-fourth. In general, he persuades us, the impulse behind the grid was less the rationalizing impulses of the Enlightenment than the eternal desire of a bureaucratic commission to finish its report, accented, later, by the eternal real-estate developers’ urge to have regularized lots to develop.

A lost city of stars and ovals may appeal to us more, but one wonders if it would have altered the city’s story much. The history of the grid suggests that its character is determined by its uses more than the other way around. Mansions that arose within it had a fenced, forbidding look, as in pictures of early Fifth Avenue; the clustering of poor immigrants seemed to create crowded streets and slums not terribly different from those in Paris or Chicago.

Koeppel certainly recognizes the ambiguities of the grid, but he seems unsure what to make of them; early in his book, he darkly insists that the dead hand of the rectilinear grid “favors private interest over public convenience,” and cites a German urban planner who claimed that “mystic” peoples favor organic cities over regularized ones. But any town that has Walt Whitman as its bard can hardly be accused of forcing narrowly straight-sided views on its singers. Rectilinear the grid may be, but it twists and turns in our imaginations as much as any winding road.

The grid, useful as an accelerant for pedestrians and horse-drawn vehicles, ended up being unintentionally well-adapted to the imperialism of the car; a short ride in a London cab can take forever, while taxi- and Uber-drivers race up and down the midnight Manhattan avenues at hyper-speeds. Evan Friss’s forthcoming “The Cycling City: Bicycles & Urban America in the 1890s” (Chicago) wants, in turn, to show us a forgotten parenthesis when the city had not yet yielded to the car. But he ends up showing mainly how terrific research and a feeling for detail can be undermined by the pieties of the contemporary social sciences. Common sense wins, barely, but not without the author taking many frightened-looking glances over his shoulder to see if the consensus of the discipline is gaining on him.

The consensus of the discipline takes a dim view of common-sense considerations (say, that people rode bikes because they were the best way to get places before cars). More sinister Foucauldian épistèmes must be shown to govern social life: any social explanation that can’t be expressed as a conspiracy theory involving bourgeois society stamping out Difference is inadequate to the phenomenon, even if the phenomenon is on two wheels with gears and going many different places at once. Still, Friss has a good story to tell. In the late nineteenth century, bicycles were not just a sweet means of romantic transport—“Daisy, Daisy, give me your answer do,” and all that—but a technological triumph creating fanatical followers and interest groups. The bicycle was more like a personal computer than like a love seat. There were “dozens of exclusive bicycle clubs dotting America’s leading cities. . . . Libraries, card rooms, and billiard tables kept members busy while dumbwaiters shuttled food from kitchen hands to hungry cyclists.” Women considered them “an almost utopian instrument,” Friss says, and quotes a contemporary source: “Now and again a complaint arises of the narrowness of woman’s sphere. For such disorder of the soul the sufferer can do no better than to flatten her sphere to a circle, mount it, and take to the road.”

Friss is a demon researcher, and his book is full of revelatory facts: who knew that the bicycle lobby played a key role in the Chicago mayoral election of 1887? Yet one feels impatient as he torturously tries to track academic concepts of class and mentalité onto what are, clearly, the inevitable inner squabbles of fan clubs and interest groups. Friss illustrates, without quite articulating, the central Trollopean social insight: like-minded people with similar passions typically end up fighting among themselves far more than they do with their class or intellectual opponents. Cyclists fight cyclists, as union leaders fight union leaders. To take one instance, Friss shows that the American biking community itself split, violently, in the eighteen-nineties, between those who were in favor of dedicated bike paths and those who mistrusted any segregation of the biker from the common highway.

Although Friss concedes that “bicycle mechanics became automobile mechanics” (as the Wright Brothers sprang from a bicycle shop into the air), he still insists that bikes were defeated not by cars but by a growing fear of the potentially radical effects, particularly on women, of the popular bicycle. The decline in cycling had to do with “its loss of social and cultural appeal,” Friss writes. “As more and more varieties of people began to ride, others no longer found bicycles so appealing.” Class panic, in this view, was central: “The smart set and their followers no longer found that their machines served as a social marker. The bicycle could not sustain itself as a fashionable social tool and also as a utilitarian tool.” Well, why not? Cars do. Bikes do again today, with sleek architects racing to their ateliers on the gearless kind and underpaid deliverymen pedalling through the rain with Chinese food. Scanting the obvious technological history, one can also overstate the determinisms of class warfare.

Surely many things, including bikes, fall in and out of fashion for reasons that have more to do with fashion than with reason. Hemlines do not rise and fall because of changing attitudes toward sexuality, unless attitudes toward sexuality change radically every five years. They rise because they had previously fallen and fall because they once rose. Fashion is not a subsidiary idea but itself an explanatory one. Any New York student of styles of transport and recreation will have seen, for instance, the rise and fall of roller skates, a craze that claimed the cover of this magazine more than once in the nineteen-seventies, and its replacement by in-line skating, and then, eventually, the decline of both. All of this had less to do with changing social visions than with the inevitable pull of tides and time.

Cartoon
“What did we promise Mommy about leaving our rejected manuscripts and empty bourbon bottles on the stairs?”
BUY THE PRINT »
How much do the physical arrangements of cities—their alleys and streets, their transportation infrastructure—actually affect their character? The grid expresses something about a common New York ideal of busyness and intersection—mercantile capitalist order as a devouring Dionysian force—more than it enforces that ideal. The greatest celebrations of the grid are the émigré Piet Mondrian’s two New York paintings: “Broadway Boogie Woogie” and “Victory Boogie Woogie.” They are part of a forties-New York efflorescence, the blinking, stable, dynamic, and yet still rectilinear energy—an image of energy that breaks with the usual organic forms of ecstatic spirals and gyres. They show the grid as metaphor, and a metaphor, after all, is a cell with a view: the bars in the window bend, and you leave as, and when, you want to.

If city tales begin with grids and transportation, they end in ruins: there are no more moving or frightening images in urban history than those of contemporary Detroit. The photographs of Michigan Central Station now be-weeded, or of Mishkan Yisroel synagogue abandoned, are presented on the Internet as if Piranesian and romantic, until one recalls that this Rome fell not after five hundred years of Vandals and Christians but after a mere few decades of neglect and decay and social change. The journalist David Maraniss has written a book about the fall of Detroit, and done it, ingeniously, by writing about Detroit at its height, Humpty Dumpty’s most poignant moment being just before he toppled over. Maraniss’s “Once in a Great City” (Simon & Schuster) is an encyclopedic account of Detroit in the early sixties, a kind of hymn to what really was a great city. (Maraniss spent his early childhood in Detroit, and its old monuments still have a childlike glow for him. I feel the same way about Philadelphia, a similar place with a similar fate and a happier rebound.)

Maraniss begins with twinned disasters that at the time no one saw as portents. In 1962, the Ford Rotunda, a now forgotten but formerly world-famous example of high-tech architecture, designed by the visionary modernist Albert Kahn—once one of the five leading tourist attractions in America—burned to the ground in an hour after a stupid roofing accident. The same day, another Kahn building, a model black-owned-and-operated hotel called the Gotham, was raided on charges of housing a gambling ring; it was shortly doomed to demolition, to make way for a parking garage, as much of black Detroit was being steamrolled for expressways. Both structures were monuments of civic optimism in their day, and their destruction dramatically illustrates all that could be lost.

We then meet a panorama of social players and types: from Berry Gordy, Jr., then emerging as a tycoon of rhythm, to Wilfred X, Malcolm’s older brother, a significant character in the era and the town. The Motown chapters, to any lover of American music, are especially engrossing. Maraniss dramatizes one of the most compelling of all historical questions: How did a line of geniuses suddenly emerge in this one industrial town—Smokey Robinson and Marvin Gaye and Stevie Wonder and Holland-Dozier-Holland, and so many more that Aretha Franklin, across town, was snobbishly kept out of the line by her preacher father, who didn’t want her to mingle with crass Motown? Was it because there was a local hit-making music business, or because there was, by serendipitous chance, a special gathering of geniuses? The obvious answer, that it was a little of both, is not that helpful, since we want to know how much of each, a question that applies equally to Florence in 1400. The art historian E. H. Gombrich, the best student of the process, once identified the central engine of such renaissances as in-group competition, and, indeed, the engine of Motown’s genius seems to have been the morning meetings, when all the composers would have to go head to head and demo to demo, and the competitive level was so high that Gordy’s sister Esther generally sided with Smokey over Berry.

We are also reintroduced to the man who ought to be on the twenty-dollar bill, the great Walter Reuther, the president and a founder (with his brother) of the United Auto Workers, tragically little remembered now, especially compared with the thug Jimmy Hoffa, who betrayed the labor movement to organized crime. We witness Reuther’s heroic past—he was a social democrat who worked in the Soviet Union and denounced Stalinism; a labor leader who survived attempts by the owners not just to intimidate but to assassinate him—and his visionary nineteen-sixties present. He thought that autoworkers needed not only higher wages but less stressful work, and proposed regular sabbaticals for them, as for college professors.

We get a deservedly sympathetic portrait of Henry Ford II, who knew perfectly well what a louse his old man had been, fired the goons who had tried to kill Reuther and his brother, recognized the necessity of unions, and worked hard to accommodate their concerns. There are also some lovely and funny intertwinings: Dr. Martin Luther King, Jr., a saint but no fool, got into a squabble with Gordy over potential royalties for an LP of his “I Have a Dream” speech. (Maraniss notes that there were multiple “Dream” speeches—Gordy was ready to release the one delivered in Detroit, before the March on Washington, which Reuther helped organize.)

The display of municipal energies is so impressive that every page haunts us with the questions What went wrong? How could so much go so wrong so rapidly? How did a city of so many fruitful tensions and monuments and intermediary institutions turn into the ruins we see now, with scarcely a third of its 1950 population remaining and so many of the sites that Maraniss mentions ruined or destroyed? The answers, more hinted at than spelled out, are depressingly familiar: the riots of 1967 and the upward swoop of crime that dissolved old neighborhoods and drove their residents to the suburbs; the separation of the suburbs as a tax base from the city that they depended on; and, above all, the simple and inexorable decline of Detroit as a manufacturing base, under the pressure of Asian competition. (It is exactly the historic arc of change that John Updike captured for all time in his “Rabbit” books.)

Reading about Maraniss’s Detroit in isolation, though, one would have only a vague idea that pretty much the same things happened in Gary and Buffalo and Cleveland and Camden—if Detroit got it worse, it was partly because it had it better. What’s more, the same thing (minus the gun violence, an American specialty) happened in the North of England. Liverpool (which also had a pop efflorescence), Manchester, and Leeds all saw similar depressions. Read the English novelist Keith Waterhouse on what downtown Leeds was like, splendid and civic, during his Yorkshire childhood in the forties, and the Detroit agony seems universal.

The rise in urban violence certainly played a decisive role in the American disaster. Maraniss details the first significant clash between police and citizens in the sixties, caused by the killing of a prostitute known as St. Cynthia, in recognizably muddled circumstances. The local community’s suspicion of the police was not assuaged by the possibility that in this particular case the police were not at fault, or by what Maraniss records as the genuinely good intentions of the recently appointed police commissioner. His book reminds us that amnesia about the effect of crime on middle-class voters is a dangerous narcotic for liberal politics. It was crime and the fear of violence, however paranoid or overstated, that impelled the rise of Richard Nixon, and of George Wallace, and fuelled the paranoia about cities that became a staple of the American political diet for decades. Maraniss shows us that periods of progressive politics—and one of the most heartbreaking things in his chronicle is the certainty, of Dr. King among others, that Detroit was at the onset of an upward-moving arc—coincide not with periods of heightened despair but with periods of rising expectations. (Black Lives Matter rose under Obama because a reminder of residual bigotries met an increasing expectation of equality of treatment.) Walter Reuther was well to the left of Bernie Sanders, but he understood that his union needed a better Ford, in both senses. The U.A.W. could get more power for itself and benefits for its members when there was more to share. Someone once called anti-Semitism the socialism of fools, meaning that imagining that Jewish financiers were responsible for inequality was a half-witted way of explaining it. Sentimentality about urban violence is the progressivism of fools, a half-witted insistence that the American middle class, itself plagued by economic insecurities, is more likely to pay tender attention to cities if they are made unlivable.

Granting that what looks peculiar to a place usually turns out to be endemic to a kind, has anyone found a cure for the common modern urban ills? The two best books about American cities—Reyner Banham’s on Los Angeles and Jane Jacobs’s “The Death and Life of Great American Cities,” which is essentially about New York—share an oddly double-headed ideology. Both Banham and Jacobs believe in unplanned, organic, emergent cities. Banham’s ecstatic expressway is an expression of L.A.’s endless appetite for elsewhere; Jacobs’s beloved street corner expresses New York’s nightly celebration of community. Their ideas are conservative and bottom up, in one sense, but also progressive and top down, inasmuch as both writers believe that intelligent government decisions create the places where the unplanned can happen. The expressway is Jacobs’s enemy and Banham’s friend, but they both know that expressways are not self-made. So there is a kind of tension in the way they describe the cities they love, and that tension may be built into what cities are: highly organized and planned—gridded and gated—yet unpredictable in their unfolding pattern.

What we need, obviously, and find hard to make, are stable pots and beautiful flowers, good plans producing open forms. We mourn the small stores lost and the neighborhood neutered, even as we recognize that cities depend for their future on new ways of selling and buying and living. Cities often produce whatever the next wave of social change is going to be, and then violently reject it for altering the nature of the city. The tech kids clustering in San Francisco depend on the special virtues of the old San Francisco—contiguity, character, charm—which they cannot help but diminish. The old city recoils, even as it is, inevitably, remade. As city people, we are our own pathogens and our own patients.

Can we be our own doctors, too? Certainly, the chief way that cities have renewed and restored themselves in recent times is through the process that has the ill-given name of gentrification—ill-given because it is dehumanizing to fix under the label “gentry” the mixture of social types who reënter the urban arena, ranging from real-estate keeners to young gay couples to painters seeking space, just as it is to label a similar mixture of social types an “underclass.” (And the gentry are, often enough, the ambitious locals, as in Harlem, where Calvin Butts’s Abyssinian Baptist Church is the single most potent agent of change.) D. W. Gibson’s “The Edge Becomes the Center: An Oral History of Gentrification in the Twenty-first Century” is a fair example of how the topic often gets treated. The book’s aim is to highlight the voices of the dispossessed and the often rightly aggrieved, with one meek lawyer there to say that the High Line has become that scam. The problem, Gibson suggests, is with property itself; reading a pamphlet titled “Squatting in New York City,” he’s captivated by the phrase “the sin of property.” (Proudhon thought it merely theft; the Catholic imprecation is newer.) This is odd for someone who presumably wishes to preserve the New York City that immigrants built and loved. They did not believe that property was sin. They believed that property was salvation—that a small house or a corner store was the path out of immigrant poverty. At the end, Gibson holds out the fear that New York may yet become Detroit, but the truth is that stagnation strikes cities in many ways, and that the homogeneity that threatens New York is entirely a problem of unexpected civic success—too many people from too many places chasing too little property—not, as in Detroit, a civic failure so large that it causes the government, in effect, to give property away.

Cities change. It is their nature. Those which stop changing stop being cities. Cities that change entirely, though, cease to be themselves. If there are sane grounds for hope, they lie in how resilient the social capital accumulated in cities turns out to be. Detroit today is, all agree who work there, a harsh place, haunted by the past, but one with real civic resources that are being called on for its renewal. The clashes between local people and new arrivals, in inner-city Detroit as elsewhere, are real, and a fit subject for a novel or a film or a real oral history; but they are not poisonous or intractable.

If there is one book that might fill your heart with small but reasonable hope about the possibility of rebuilding social capital through civic instruction—in reinforcing organic community with social engineering—it is a 2008 study of dog shit. “New York’s Poop Scoop Law,” by Michael Brandow, tells the story of the Koch-era initiative to make people pick up after their dogs. We forget now that, while crime raged on the streets, dog shit lay beneath the feet of muggers and victims alike. Brandow is a dog lover, who, for much of his book, sneers at the absurdity of the law and at those who protect trees instead of accepting dogs, only to confess that, on the whole, it has worked wonderfully well. A combination of, so to speak, ground-up enterprise—citizens’ groups demanding cleaner streets—intelligent legislation, and a surprising surge of civic good will made into a durable social convention a law that most thought would be unenforceable or neglected. It arose from a perfect interaction of laws and manners, top-down instruction and bottom-up consensus. Civic groups acting to save their city inspired politicians to pass intelligent laws, and the laws became traditions so entrenched that no one now walks his poodle without a plastic bag. The streets of the city may not be paved with gold. But they are clean of dog poop. It’s a start. ♦

<--------->
Books SEPTEMBER 28, 2015 ISSUE

Washington Scribe
The diaries of the ultimate D.C. insider.

BY THOMAS MALLON
SHARETWEET
 09_28_15
TABLE OF CONTENTS
Need to stop reading?

Enter your email address
 GO
We'll send you a reminder of where you left off.
 The columnist Drew Pearson sought direct policy impact, often at the Presidential level.
The columnist Drew Pearson sought direct policy impact, often at the Presidential level.
CREDIT ILLUSTRATION BY OLIVER MUNDAY / SOURCE: COURTESY ESTATE OF DREW PEARSON
Last May, George Stephanopoulos disclosed that, while working at ABC News, he had donated seventy-five thousand dollars to the Clinton Foundation, headed by his former boss. For a couple of news cycles, political operatives and journalists argued about whatever line might have been crossed, furrowing their brows over the well-established migratory patterns of their two species.

While a few individuals, like David Gergen, move between the professions in a sort of commute, more frequently a short career in politics now leads to a longer, permanent one on television, as with Tim Russert and Chris Matthews and Stephanopoulos himself. Rarer—if one discounts newspeople who become Presidential press secretaries (Pierre Salinger, Tony Snow)—are those who depart journalism for the top inner precincts of the political realm they used to cover. In his memoir “The Clinton Wars” (2003), Sidney Blumenthal, once this magazine’s Washington editor, describes with boyish wonder how it felt to leave The New Yorker for the West Wing. “The decisive moment had arrived when I could become a wholehearted political participant,” he wrote, adding, “Being on the outside in whatever capacity was never the same as being in.” The satisfactions of “access” can never quite equal the thrill of agency.

The day-to-day porosity between politics and journalism has closed up in recent decades. Arthur Krock, who from F.D.R.’s era to L.B.J.’s wrote the Times’ In the Nation column, worried in his “Memoirs” (1968) about whether he might have been compromised by orbiting the Kennedy family for decades. He had written glowingly of Joseph P. Kennedy, Sr.; helped polish for publication the senior thesis of young John F. Kennedy; and, later, even recommended the man who became the President’s valet. He spends a stretch of his book trying to assure readers, and himself, that he was able to maintain his detachment when writing about the Kennedy White House.

Robert Novak, known as “the Prince of Darkness,” records in his autobiography of the same name how the social connections of his writing partner, Rowland Evans, sometimes put their column in the tank for J.F.K. Things got even more complicated with the President’s brother Robert. In 1966, unbeknownst to Novak, Evans, over lunch at the Sans Souci, helped New York’s junior senator draft a statement calling for a coalition government in Vietnam. When the proposal was issued, Novak wrote a column attacking it—in keeping with the policy position that he and Evans had already established. Evans let the column run with only a little softening and then headed to Hickory Hill to apologize to Bobby. The little affair nearly ended the Evans-Novak double byline.

The power of individual Washington commentators has contracted radically in recent decades. A general contempt for the press may still be the Presidential norm, but mid-twentieth-century Chief Executives especially hated having to curry favor with a small handful of syndicated pundits. Even so, in their heyday, the influence sought by columnists and newspaper publishers with American Presidents tended to be incidental, unprompted by ideological zeal. When Arthur Krock privately suggested to Kennedy how he might get a better handle on the C.I.A., he was operating in a manner similar to that of Philip Graham, the co-owner of the Washington Post, who recommended Douglas Dillon to Kennedy for Secretary of the Treasury. Each piece of advice was a chance to participate directly in the game, to feel modestly helpful and somewhat important. Joseph Alsop, the Post’s owlish, irascible, and closeted columnist, also recommended Dillon to Kennedy, but, as the years went on, his advice developed a particular doctrinal urgency. According to his biographer, Robert W. Merry, Alsop eventually “aimed the column directly at a single person,” Lyndon Johnson, in the hope that he would adopt a Vietnam policy that was even more hawkish than the one he was carrying out.

From the thirties through the sixties, no one crossed the journo-politico line in search of real policy impact with greater fervor than Drew Pearson, the author of the syndicated newspaper column Washington Merry-Go-Round. Accompanied by Pearson’s mustachioed thumbnail image, it ran so widely and for so long that its purveyor became a figure in the popular culture. A volume of Pearson’s diaries from the nineteen-fifties, published more than forty years ago, added context to his public exploits and exposés. These included being choked by Joe McCarthy in the coatroom of Washington’s Sulgrave Club (the newly elected Senator Richard Nixon, Pearson’s fellow-Quaker and an object of his special loathing, broke things up); and, with access to an official investigator’s hidden microphone, helping to get President Eisenhower’s chief of staff, Sherman Adams, fired for accepting a businessman’s gift of a vicuña coat. Only now are we getting a second volume of diaries, “Washington Merry-Go-Round” (Potomac), edited by Peter Hannaford. It runs from 1960 nearly up to Pearson’s death, in 1969, seven months into Nixon’s long-delayed Presidency. This new installment shows even more convincingly the extent of Pearson’s direct involvement in politics, often at the Presidential level, and the degree to which it derived not just from standard elements of ego and competitiveness but also from an emotionally committed world view.

“The vote went against us, as I expected, but we polled twelve,” Pearson writes on January 31, 1962, sounding more like a senator than like a reporter as he expresses disappointment over the confirmation of a C.I.A. director. “Senator Case of South Dakota switched his vote and opposed McCone. This means that I will have to support him for re-election.” The speeches and the private memorandums that Pearson wrote for senators sometimes consumed more of his energies than the column. He maneuvered and whipped legislators as if he were in the leadership: “I telephoned [the Missouri senator] Tom Hennings and told him that if he could filibuster for a day on his [liberalizing] southern primaries amendment, I could probably pick up twenty-five votes for him,” he writes at the beginning of 1960. The strategy succeeded.

Jack Anderson, Pearson’s collaborator and the inheritor of his column, wrote, in “Confessions of a Muckraker” (1979), that his boss “was as much the political activist as the reporter” and that he himself didn’t always enjoy being Pearson’s “ward heeler.” According to Anderson, Hubert Humphrey, a Pearson favorite, may have had “a firmer grasp on journalistic propriety than Drew,” whose methods sometimes left Anderson feeling queasy. (Not that Anderson was shy in the investigative department. It was he who listened to the gleanings of that hidden microphone during the Sherman Adams affair, and became such a thorn in Nixon’s side that G. Gordon Liddy volunteered to kill him.) What Anderson finds most remarkable about his boss is the way Pearson took “infinite pains to inculcate his convictions on the moral objectives of the newspaper column and the just society.” Pearson was particularly outspoken on behalf of civil rights, and spent twelve years directing the Big Brothers of Washington, D.C. “You’re so much pleasanter in person than in print,” McGeorge Bundy, the national-security adviser to Kennedy and Johnson, once told him.

Pearson had thought of becoming a diplomat before arriving in Washington at the end of the Coolidge years. For the rest of his life, he offered himself as a sort of freelance envoy, organizing a post-Second World War “Friendship Train” of relief supplies to hungry Allied populations and later dispatching “Freedom Balloons” full of wholesome propaganda into the Eastern Bloc skies. Certain that he knew more than the ambassadors the U.S. sent to Moscow, he grasped any chance to serve as a back channel to the Soviets. He hosted Russian editors and students, and appears to have dined with the longtime Soviet Ambassador, Anatoly Dobrynin, more often than with any high-ranking U.S. official. He visited Khrushchev at his dacha and described him, in Mrs. Thatcher’s later, famous words about Gorbachev, as someone “we can do business with.” As a gentleman farmer back home in the States, Pearson was pleased to urge the General Secretary to try “sorghum instead of corn for the more arid areas of the Soviet Union.”

He took credit for a slight softening of American public opinion toward the Russians, but his accommodationist writings and activities got him picketed in anti-Castro Miami and attacked by Senator Strom Thurmond. Assaults from the other side were welcomed. “Radio Moscow has taken a crack at me—thank God,” Pearson writes in the diary on April 4, 1962, before recalling a promise made to him by Khrushchev’s son-in-law, Aleksei Adzhubei, the editor of Izvestia, “to write some critical stories.” A year earlier, Pearson had asked Pierre Salinger for a domestic version of the same: “I suggested that when the going got tough and I got too much hell from Republican editors, I would ask Kennedy a favor—namely, that he do to me what Harry Truman did: blast me. This would really set me up with the press. Salinger said that when the time was desperate to call on him.”

Pearson arrived in the capital too late for Teapot Dome and departed life too early for Watergate, but he covered nearly every smaller scandal in between. His generally straitlaced nature allowed him little tolerance for even lovable rogues. “There is a streak of insanity in the Long family,” he writes in the diary, unwilling to give a pass to either Huey’s son or brother, the bibulous Senator Russell and the randy Governor Earl. For all his championing of civil rights, he withheld full approval of Martin Luther King because of what he knew about King’s extramarital affairs, though by the courtesies of the time, sex, like drinking, was more a matter for the diary than for the column. Pearson noted, first for himself and then for posterity, that J. Edgar Hoover and the newest husband of Marjorie Merriweather Post were “in the same category” as Walter Jenkins, the aide to Lyndon Johnson who was arrested in a Y.M.C.A. men’s room during the 1964 campaign. He privately records Lady Bird Johnson’s expression of annoyance at the departure of her husband’s most recent mistress: “What does Mary Margaret mean by leaving without breaking in someone to take her place?”

The most flagrant dalliances belong, of course, to John F. Kennedy, who is “laying every girl in sight.” On March 4, 1961, the journalist Ernest Cuneo and his wife come to Drew and Luvie Pearson’s for dinner: “We spent most of the evening discussing the favorite topic of conversation: the sex life of the president of the United States.” Yet no one reports on it. Even now, this new volume of diaries carries a dramatis personae but no footnotes, and some half-told stories require a bit of digging by the reader interested in their completion. “Kennedy shacked up with the female singer who had entertained the [White House Correspondents’ Association]. He had never met her before but sent for her.” Newspapers from February, 1961, provide, alas, less than conclusive information. It was probably Julie London but could also have been Dorothy Provine.

Cartoon
“Somebody tweeted.”
BUY THE PRINT »
Financial corruption, which could be publicly exposed, repelled Pearson even more than canoodling and boozing did. If Sherman Adams was his biggest catch of the fifties, a decade later the Connecticut senator Thomas J. Dodd presented himself as a kind of iniquitous twofer, a man sometimes too drunk to accept delivery of his bribes. Pearson was following the money long before Woodward and Bernstein, conducting green-eyeshaded examinations of documents questionably obtained. In pursuing the Dodd case, he faced a civil suit (he won it) as well as a possible indictment (he avoided it). As a frequent defendant in libel trials, Pearson learned to confuse juries by gratefully shaking the hand of whatever witness had just testified against him.

Had he lived to see it, Pearson, like Nixon, would have loved the Internet; he believed that nearly all human behavior, public and private, could be explained by tracing the links in every chain of friendship and enmity. Here is how he spells out, on April 20, 1965, the way a tax bill beneficial to DuPont came to pass in the late fifties:

The DuPonts had hired Clark Clifford and probably paid him a million dollars over a period of ten to twenty years. Clark, in turn, sold Sen. Bob Kerr of Oklahoma on carrying the ball for the tax giveaway. Clifford had been on Kerr’s strategy board in promoting him for President. Kerr, in turn, worked with Allen Frear when the Democratic senator from Delaware put across the tax concession. Kerr siphoned $27,000 through Bobby Baker’s bank account into Frear’s political campaign. . . . Kerr also would develop oil wells, sell them to Frear for ten cents on the dollar. They turned out to be very profitable wells. Because Frear was carrying the ball, the Republican senator from Delaware, John Williams, balked at the DuPont-General Motors tax bill and helped to kill it. He hadn’t been consulted. Later he was brought in on the act and helped to carry the ball.
One imagines Pearson fingering these links late into the night, the way other people tell their rosary beads or count sheep. But the diary’s display of this particular chain—a series of connections and events explained to Pearson by Bobby Baker, Lyndon Johnson’s secretary and “protégé”—skips over one link. Left out is any mention of how, while the DuPont maneuvers took place, Pearson was engaged in one of his backstage lobbying campaigns, this one to derail President Eisenhower’s nomination of Lewis Strauss, the great foe of J. Robert Oppenheimer, to be Secretary of Commerce. In “Confessions of a Muckraker,” one can find Anderson recalling how “for several months we had been knee-deep in things that journalists should never do,” which inevitably set a politician’s quid in search of a journalist’s quo:

Allen Frear of Delaware, home of DuPont, passed a message to Drew . . . that he might vote against Strauss if Drew would refrain from attacking Frear’s special tax-avoidance legislation for DuPont; Drew did not respond to Frear, but nonetheless, we should never have been in a posture to receive such an offer.
The column certainly made no mention of Frear’s approach.

Pearson didn’t enrich himself—a year before he died he was still hustling along the lecture circuit to pay his bills—but he kept track of every kind of currency he was owed. For all that Strauss’s treatment of Oppenheimer may have bothered him, so did Oppenheimer’s ingratitude: “After I had gone to bat day in and day out when Oppenheimer was on trial, he turned me down for a TV interview and went on Ed Murrow’s program instead.” Pearson regarded the Kennedys as especially ungrateful to everyone, including him, but he remained friendly enough toward J.F.K. to contemplate helping him during his first live press conference: “I had planned a question about the Free University of Cuba but couldn’t get hold of Salinger to coach Kennedy in advance.” Seven years later, when NBC airs three showings of a Bobby Kennedy press conference, Pearson can only “wonder what Bobby had on NBC.” The knowledge that everybody has something on somebody creates an informational barter economy and a reputational balance of terror, a small-scale version of the Doomsday-avoidance mechanism being used by the U.S. and the Soviets. The “gimlet-eyed cold young man” who serves as his brother’s Attorney General calls off an investigation of New Hampshire’s corrupt Senator Styles Bridges after “a very high Republican” threatens to expose the President’s sexual infidelities.

By 1968, Pearson disliked R.F.K. so much that the diary pays him the highest anti-compliment possible: “If Bobby were nominated, I might well vote for Nixon.” In the event, Pearson voted for Humphrey and held back one of the biggest items he ever had, news that Nixon had received psychotherapy from a New York doctor named Arnold Hutschnecker. He killed the story himself a week before the election: “I wasn’t sure Hutschnecker was telling me the truth, and I have [in the past] had such hell from editors that I decided to play it safe.”

Lyndon Johnson, the President with whom Pearson had the longest, closest, and most complicated relationship, is the one who kept him tied in ethical knots. Back in 1956, Johnson agreed to support the Presidential hopes of Pearson’s preferred candidate, the Tennessee senator Estes Kefauver, if Pearson backed off from investigating the tax advantage Johnson had secured for a Texas construction company. By 1964, Tyler Abell, Pearson’s stepson, was working in Johnson’s White House, while Pearson’s daughter-in-law, Bess Abell, served as Lady Bird Johnson’s social secretary; Pearson himself was asked to help write the State of the Union address.

Pearson had to rationalize all these things as being part of his service to the greater progressive good; in 1968, L.B.J. told him that the Administration’s Fair Housing Act would never have passed without his public drumbeat on its behalf. The column may have taken shots at the Administration, but some were in the nature of that Russian criticism of Pearson: camouflage for an alliance. According to the diaries, Leonard Marks, the head of the United States Information Agency, tells Johnson that Pearson “has to needle you occasionally to keep his impartiality,” and then assures Pearson that “the president agreed.”

He had good reason to. On November 13, 1967, Pearson records being told by Johnson, “You might write a paragraph showing what Wilbur Mills and Jerry Ford are doing to the country. They entered a conspiracy to prevent new taxes until there was a cut in spending. I only asked for $4 billion in new taxes this year to help pay for the war and head off inflation, but they are adamant.” It took Pearson nine days to comply. A search of his columns turns up, on November 22nd, a paragraph reporting the “inside fact” that “the President was sore as blazes” at Mills and Ford “for conspiring together to block the President’s request for a tax increase.” Newspaper readers didn’t get to see how L.B.J. urged the information on the columnist, just as they weren’t privy to a diarized record of Pearson’s reaction to a Johnson TV appearance: “Lyndon did fairly well at his press conference, though I can see why the country is turning against him. He drawls along in a ponderous manner like a hick farmer standing in a pulpit pretending to be God.”

As with most diaries, the greatest pleasures to be had from Pearson’s tend to be fast, peripheral ones occasioned by minor characters who are out the door a moment after they’ve arrived. Oh, look, it’s Nancy Pelosi’s father—“Tommy D’Alesandro, who put across the Kennedy blitz in Maryland, was his usual backslapping self”—and, a page later, there’s the first Mrs. Nelson Rockefeller, revolted by her husband’s recent surrender to his rival: “ ‘Nelson has just come into the room wearing a big Nixon button, and I could throw up.’ ”

The diaries also contain one or two unexpected antipathies, motifs of contempt that are never really explained. Pearson is much harsher on Jacqueline Kennedy than on her philandering husband, describing her, variously, as “pretty tough and conceited,” “a cold gal who deep down doesn’t have much sympathy for the aims of her husband and wouldn’t know a social reform when she saw one,” and “just plain lazy.” During the last days of 1963, at the height of the widowed First Lady’s public deification, Pearson writes, with more wishfulness than evidence, that “resentment against Jackie is mounting” among Washington ladies who lunch.

Apart from the more cerebral Walter Lippmann, the only columnist more famous than Pearson was the other Walter—Winchell, whom Pearson spotted at the 1964 Democratic Convention “manufacturing big items out of trivia.” But Pearson never lost the conviction that his own items were purposeful and history-making. On every page of the diary one senses how much it all matters to him. Jack Anderson says that, two weeks before his death, his boss told a friend, “We’ve got to live a long time. We’ve got so much to do.”

Of course, the reading of any diary distorts the life being chronicled. Each small entry becomes, for the latter-day reader, an accelerated particle in a narrative now moving faster than the speed of life. The form can cut the most portentous diarist down to size, and it may be especially brutal to politicians and journalists, who, as they circle and use and become one another, only rarely exchange their habitual sense of emergency for the longer, calmer view. In 1861, William Howard Russell, the London Times’ American correspondent, found himself playing cards with the New York Times’ Henry J. Raymond and Secretary of State William Seward. When Abraham Lincoln entered the room, Seward genially urged the President to seize an opportunity: “Here, Mr. President, we have got the two Times—of New York and of London—if they would only do what is right and what we want, all will go well.” Lincoln, looking for a moment beyond the endless dance of pols and pressmen, merely replied, “If the bad Times would go where we want them, good Times would be sure to follow.” ♦


 student working on a seminar paper about the mechanics of the Rwandan genocide of 1994 sees his father reading “Black Earth” (Tim Duggan/Crown), the Yale historian Timothy Snyder’s new book on the Holocaust, and asks the unaskable question: Do we really need one more book on the Holocaust? The facts are in and clear, he says, while so many other human horrors demand our historical understanding and get so much less: how many new books have been published this year on the Belgian genocide in the Congo? Doesn’t endlessly retelling the story of the murder of the Jews of Europe let us give ourselves the appearance of moral seriousness while immunizing us to the urgencies of actual moral seriousness? Piety is the opposite of compassion, which is better directed toward those who need it now than toward those who were denied it then.

The student turns away in exasperation before his father can reply that Snyder has framed this book in order to respond to that question. It’s why he has given it the subtitle “The Holocaust as History and Warning.” Snyder’s point is that if we really understood what happened in Ukraine in 1941 we would begin to understand what happened in Rwanda in 1994—and might prevent something like it from happening elsewhere next year. He even argues, against the grain of the usual historian’s practice, that there are recurrent patterns in history and that the bad ones can be identified and perhaps undone.


Though Snyder’s goal is to clarify history, he is certain (and here he is like most academic historians) that one can clarify history only by complicating it. As one might expect, an extremely complicated story, formed in the field of Holocaust studies, trails his new effort. His previous book, “Bloodlands,” was an effort to historicize the Holocaust—to remove it from the stock black-and-white imagery, accompanied by minor-key cello music, in which it had come to reside, at least within the popular imagination. In particular, he sought to re-center our attention on the “forgotten” Holocaust, on the reality that at least as many Jews were killed in mass actions in Ukraine, Poland, Belarus, and the Baltic states as were dispatched in the death factories of Birkenau and Treblinka. Soldiers machine-gunning people on the edge of a pit that they’d dug themselves and that already held the bodies of their families—that was the true image of the Holocaust, more so than trains running on time to industrialized gassings and burnings. Auschwitz, in this view, is, to put it brutally, almost a tourist trap for historians. What distinguished the horror from other horrors was not lists on graph paper and bureaucratic requisitions for Zyklon B gas. It was a soldier writing home to his wife about killing Jewish babies in Belarus: “During the first try, my hand trembled a bit as I shot, but one gets used to it. By the tenth try I aimed calmly and shot surely at the many women, children, and infants. . . . Infants flew in great arcs through the air, and we shot them to pieces in flight.”

Historicizing anything risks diminishing it—history, after all, is what used to happen—and critics complained that Snyder, by robbing the horror of its industrial modernity, had made it a folkloric and merely regional tragedy. Worse, they complained, by blending the crimes of the war with Soviet crimes that began in the thirties—the Ukrainian famine, for instance—he was explaining away the enthusiastic participation in the mass killing of Jews by the locals, Ukrainians and Poles in particular. Instead of seeing crazed German fanatics who communicated their pathogen to hardened Jew-haters, he asked us to see what happened in the forties as a blind scything through a harsh landscape, ignorant armies clashing by night and killing millions in the darkness, a scene from Bosch more than from Kafka. “Mass violence of a sort never before seen in history was visited upon this region,” he wrote in “Bloodlands.” “The victims were chiefly Jews, Belarusans, Ukrainians, Poles, Russians, and Balts, the peoples native to these lands.” By making the massacres part of a geographic tragedy of invasion and counterinvasion, and of victimized native populations who suffered in various ways along with the Jews, Snyder could be accused of playing down the role of ideological and indigenous anti-Semitism. “The gesture of a finger across the throat, remembered with loathing by a few Jewish survivors,” Snyder wrote with great delicacy, “was meant to communicate to the Jews that they were going to die—though not necessarily that the Poles wished this upon them.” It is certainly not the way that Claude Lanzmann’s “Shoah” represents the gesture. Elsewhere, Snyder wrote that the “victims of Auschwitz were more likely to be bourgeois and thus suitable targets of comfortable identification,” although “comfortable” is surely the last feeling anyone reading about their suffering would ever have. By treating Slavs, Balts, and Jews all as victims together, his critics claim, he was obscuring a tragic core truth. Slavs and Balts died along with Jews, but Slavs and Balts killed Jews as well, in a way that Jews did not kill Slavs and Balts.


Snyder maintains that too much emphasis has been placed on the ferocity of indigenous anti-Semitism; that it was instead the destruction of Eastern European states by the Nazis, and then the skillful “political” exploitation of their wretched recent history, which made their lands into killing grounds and a small number of their people into executioners. Without the state apparatus that had long accepted, however grudgingly, ethnic coexistence, and with the murder of Jews made into a sign of renunciation of “Judeobolshevism,” intolerable pressure was placed on the native population. He points out that, while the Jews of Estonia died almost to a man, the Jews of Denmark largely survived, and that this was not because Estonians hated Jews and Danes did not but because the Estonian state—though it had had only the briefest of existences before—was destroyed, and the Danish one left mostly intact.


Snyder’s new book is meant, in part, to respond to the criticisms of “Bloodlands,” and a reader familiar with the controversies will note in the text more sublimated anger than might at first appear. Angry authors, even when their books are not explicit replies, end up incorporating unsent responses into the web of the text, where the angry red threads stand out. If the complaint about “Bloodlands” was that Snyder made the Holocaust a local event, this book is meant to universalize it again, with the understanding that what is universal in human experience is what is local and political.

Why do we need any new books on the extermination of the Jews? The Shoah, it seems, has come to be read for portents and interpretation as much as for history itself. Yet one reason that the small scholarly details matter is that they provide an arsenal for whatever argument you want to make about the present. If you believe that the mass extermination of the Jews was already implicit in the orders given for the June, 1941 invasion of Russia, then you are likely to see it as proceeding according to a long-standing fixed plan of Hitler’s; if you believe that the Final Solution, properly so called, was a panicky, confused improvisation arrived at in December, 1941, after the German failure at Moscow and the Russian counterattack, then you will probably see it as a response made by a mostly disordered and dysfunctional evil. If June, you are likely to believe that bad people do what they say they will; if December, you believe that the worst things happen when bad people get cornered by their own bad behavior. How you feel about things as seemingly remote as Iranian deals and Putin’s aggressions is shaped by—or shapes—your judgment of the historical micro-details.

More broadly, if you believe, with Snyder, that the Second World War was really about subject peoples robbed of their states and their identity at a moment of environmental crisis and pitted against one another by a brutal colonialism, you are likely to see in Rwanda a similar kind of tragedy, as Snyder does—and you are likely to be sympathetic, as Snyder is, to protecting small-state identities and encouraging their nationalisms. If, with the Israeli historian Alon Confino, whose recent “A World Without Jews” is written in quiet opposition to Snyder’s views, you see in the Shoah the vengeance of atavistic tribalism on liberal modernity, you are likely to worry about all incantations of authenticity. Far from being sympathetic to revived nationalisms as bulwarks of the oppressed, you are likely to be suspicious of them (possibly even extending to the renascent Jewish kind).

Snyder begins the new book with an unorthodox and provocative account of Hitler’s thinking. He stresses two arresting elements: Hitler’s skepticism about using agricultural science for increased food production and (usefully discomfiting for an American readership) his dependence on an American model of development. Hitler, Snyder tells us, was obsessed with the question of growing enough grain to feed the German population and, for various crackpot reasons, didn’t believe that modern agronomy could make it happen on native soil. He saw himself doing in Eastern Europe, and in Ukraine especially, what Americans had done in the Great Plains: extinguish or exile the natives while taking over the land to feed the metropolis. Lebensraum meant “living space,” but in a different sense from the way we normally understand it: a place to grow grain rather than a place to put Germans.


Snyder’s Hitler was not exactly convinced that the Germans were a superior race. He was convinced that they might become a superior race, given their bloodlines and their numbers, but they would have to prove it in competition with other races on the world stage. Startling as it is, this view explains many aspects of Hitler’s character: his physical distance from the ideal he espoused (they aren’t like me, but I will midwife a superior race that I do not belong to); his unappeasable appetite for war; his rage at his compatriots for losing his war; his readiness, at the end, to see German land destroyed, German cities burned, German women raped—his manifest desire for a bonfire of the Germans. He had given them every chance to show themselves a superior race, and, since they had failed the test of history, they must suffer the consequences.

Snyder’s Hitler is no doubt made more neatly uniform in purpose than he really was. Revolutionary ideas tend to be rigorous: if you are plotting a socialist utopia, a blueprint, however unreal, is called for. Reactionary ideas, forged in rage, tend to be emotive and incoherent. We miss their appeal if we search them for regularities they don’t possess. It takes a purpose to illuminate a plan; it takes only one high passion to set fire to many more.

As Snyder moves toward the specifics of the German invasion of the Soviet Union, in 1941, he reveals again that, while no apologist for the indigenous murderers, he is, certainly, a partisan of the peoples of Eastern Europe. He hates the way that the Ukrainians, the Latvians, and the Poles have been made into peasant demons, with Western sages nodding and saying, well, the Nazis “unleashed the old hatreds.” He writes:

It is tempting to imagine that a simple idea in the minds of simple people decades past and thousands of miles away can explain a complex event. The notion that local east European antisemitism killed the Jews of eastern Europe confers upon others a sense of superiority akin to that the Nazis once felt. These people are quite primitive, we can allow ourselves to think. Not only does this account fail as an explanation of the Holocaust; its racism prevents us from considering the possibility that not only Germans and Jews but also local peoples were individual human agents with complex goals that were reflected in politics.
Ukrainians may have massacred their Jewish neighbors. But this was not because the Ukrainians had always hated Jews; it was because the famine of the thirties had led the Ukrainian people to fear Soviet power, and the Nazi invocation of Judeobolshevism as the cause of their miseries provided a pat and plausible enemy. (The Soviet administration did employ Jews “disproportionately to their numbers,” Snyder observes, although most Soviet collaborators weren’t Jewish.) Some did terrible things, but they did them out of political desperation and misrouted nationalism, not enduring hate.

Snyder’s regular invocation of “politics” is meant to illuminate this devil’s dance of impossibilities. The local Slavs or Baltic peoples, having previously collaborated with the Communists, could reclaim their national inheritance by murdering Jews, and cleansing themselves of the stain of collaboration. Neat trick. By supplanting “ancient hatreds” with contemporary politics, Snyder wants to situate the massacres within the specific logic of a time and place. Ukrainians and Polish Catholics don’t just hate Jews and kill them any chance they get. You have to put them in extremis first. The local populations got caught up in the killing because it was the prudent thing to do, given the context of the occupation.

Yet if the concept of “politics” is to be explanatory it should show how power gets dispersed and rebalanced among contending groups. Politics is how people adjust to one another’s needs and potential for violence. In the circumstance where one party has all the power, though, the invocation of politics seems unhelpful. The politics of a slaughterhouse is not really politics, at least not to the pigs; it is just a division of the labor. To coöperate or not is a political choice, made every day in prisons; to obey or die is not.

The real end of Snyder’s relentless invocation of “politics” is, one comes to feel, not without its politics. Snyder does not want the Putinists of 2015 to be able to discredit Ukrainian nationalism by pointing to Ukrainian participation in the Holocaust: he wants to make it clear that the Ukrainian nationalists were, in the hackneyed phrase, “victims, too.” But they were victims of a peculiar kind, and one can cheer their emancipation today without looking past their history. Snyder asks us not to blame the Lithuanians and the Latvians for what they did to the Jews without first blaming the Soviets for what they did to the Lithuanians and the Latvians. Surely one can blame all the evil actors without having to take sides with any. Going state by state and people by people through the Stalinist and Nazi destruction of local authority in all the occupied smaller nations, Snyder certainly shows that those local populations, whether Poles or Latvians or Ukrainians, could not be instantaneously motivated to rise up and murder Jews; they could only be very quickly motivated to do it. He seems to find more consolation in this distinction than it may possess.

What would be the opposite of Snyder’s view? First, that the “bloodlands” was not a geopolitical ground that generated its own events. The worst of the killings happened there, certainly, but there is no discernible difference in Nazi or, for that matter, Soviet behavior elsewhere. An entire village was murdered in central France, Jews were slaughtered en masse on the banks of the Danube. Numbers alone, not actions, made the bloodlands as bloody as they were. Snyder emphasizes that, where the state was destroyed, the Nazis got at their victims more easily. It’s certainly true that, the wider the moat, the harder it is for the tiger to get at its victims. In France, recently arrived eastern Jews, without friends or history, were easier to get at and deport than native French ones. But plenty of those went to the ovens, too. Picasso’s intimate friend, the poet and Catholic convert Max Jacob, an ornament of French culture and as French as any man could be, died on his way to Auschwitz, and his brother and sister were gassed on their arrival. The tiger’s appetite, not the width of the moat, is still the story. Denmark, the seeming counter-example, was the site of a relatively benign occupation, in Nazi terms, but the benignity was influenced by a sense of racial affinity, the vast irrational forces of racial hallucination seeming as powerful as the local political forces.

Snyder is admirably relentless in making the reader feel the horror of the Soviet mass killings, without waving them away or moving them to the margins. We meet, or, rather, shudder to have heard of, Vasily Blokhin, the N.K.V.D. executioner—hard to credit as a real person and not an Ian Fleming invention—who in one night could murder two hundred and fifty Polish military officers. This is not being even-handed. It is being clear-eyed. But if Snyder’s thesis is that, without the previous ten years of Soviet brutality, the peoples of the “bloodlands” would not have been complicit in the Nazi nightmare, then one would want more evidence—a correlation between Soviet brutality and genocidal eagerness, a direct relation between the two, something—to make the correspondence more robust. In Hungary, the Arrow Cross killed with mad vengeance, and the Béla Kun Communist period was far in the past. Vichy passed anti-Jewish laws, and hastened its Jews toward Drancy almost before they were asked for, and in France the Soviets were only a spectre.


Snyder is certainly aware of all this, and thinks that his account explains it: “Where Germans obliterated conventional states, or annihilated Soviet institutions that had just destroyed conventional states, they created the abyss where racism and politics pulled together towards nothingness.” But another view would see the obliteration as the auxiliary act and the abyss as the central moral landscape. Politics and procedures obviously enabled the killings; we owe Snyder a debt for his realism about this. But the desire to maim and murder had its roots in a disease of the mind so powerful and passionate that to call it political or procedural hardly seems to capture its nature, or its prevalence.

The explanation of the human appetite for mass murder obviously does not lie in the peasant simplicity of the Eastern Europeans. (Is there a single historian or journalist who holds this view?) But it does seem to lie in the enduring power of old hatreds, and our capacity for turning group hatred into massacre, given opportune circumstance. Just as we can’t pretend that Stalinist crimes were unrelated to absolutist Enlightenment habits of mind in which class enemies easily become nonpersons, we can’t pretend that the Hitlerian crimes can be released from an anti-Semitism deeply rooted in European Christianity. The great and sympathetic historian of Christianity Diarmaid MacCulloch writes that it is still “necessary to remind Christians of the centuries-old heritage of anti-Semitism festering in the memories of countless ordinary twentieth-century Christians on the eve of the Nazi takeover. In the 1940’s, this poison led not just Christian Germans, but Christian Lithuanians, Poles and many others gleefully to perpetrate bestial cruelties on helpless Jews who had done them no harm.”

And anti-Semitism was surely different in kind from the other ethnic hatreds of the time and place. The Jew was doubly evil, as both an agent of modernity and the possessor of an occult mystery. Jews were cosmopolitans, bankers, merchants, middlemen; the same family passed at will, from day to day, as German, French, or Russian. Jews were also possessors of an ancient text, a secret language, a lore kept hidden and unavailable; they welcomed no converts, teaching their ancient language reluctantly. This compound suspicion helped make anti-Semitism so virulent. It was, as Confino shows, reflected in the sadistic Jewish parades in which, in the thirties, helpless Jews were forced to participate: the Torah was burned, with the enthusiastic participation of German Christians.

Postwar people who fastened onto the story of Anne Frank, as has been observed before, were not in any sense sentimental to do so. That a modern state would send the police out to entrap a fifteen-year-old girl and then send her across Europe to her death because she was a Jew was an evil genuinely new in the world in a way that horrific massacres (and counter-massacres) were not. For that matter, when Martin Amis returned to the question of Auschwitz in last year’s novel “The Zone of Interest,” it was because he knows that it represented something genuinely new in the practice of evil: an institution set up with all the normal domestic appurtenances of middle-class life whose essential purpose is the mass murder of human beings.

Snyder offers his own view of the right lessons to draw in a final chapter of “Black Earth,” ambitiously called “Conclusion: Our World.” The college student’s Rwanda arrives here rapidly: just as Hitler’s world view derived from his bizarre response to an ecological crisis—the threat to a Germany deprived of land for growing grain—what happened in Rwanda happened, in part, because of the exhaustion of arable land. Africa, in Snyder’s view, may become the world’s new bloodlands, where ecological crisis is capped by mass slaughter and where ethnic explanations of killings (those Hutus always hated the Tutsis) conceal the political manipulation of power.

Surely Snyder is right when he implies that the well-meant “Godwin’s law,” which, beginning as an observation about Internet arguments, has come to be shorthand for the rule that the Nazis should never be introduced into ordinary political arguments, is miscast. In fact, we should keep the image of the Germans and the Nazis in front of us—not to show how close the people on the other side of an argument are to unutterable evil but to remind ourselves that we, too, can become that close in a shorter time than we like to think. In a period of fear and panic, it is the easiest thing in the world to talk ourselves into the idea that bad things we do are necessities of human nature. The Germans listened to Mozart and Beethoven and then murdered children, and this was not a cognitive dislocation from which we couldn’t suffer but the eternal rationale offered by the terrified: we can’t protect what really matters if we don’t do things that we wish we didn’t have to.

War makes ordinary people do horrible things. If there is a point that perhaps Snyder does not underline enough—it comes through vividly in Antony Beevor’s books on the Battle of Stalingrad and other campaigns—it is that the Germans who killed were dying, too, in increasingly vast numbers and in cold and fear of their own. Wars make atrocities happen. Americans have still not come to terms with My Lai, a Vietnam atrocity not unlike the acts of the Einsatzgruppen on the Eastern Front, though thankfully more isolated. To engage in political or procedural or even geographic explanations of these histories misses their history. Once panic sets in, for an army or an occupier, then the persecution—indeed, the slaughter—of the population seems a necessity for survival. Frightened soldiers in foreign lands murder the locals without mercy or purpose. One wishes that this happened rarely. In truth, it happens all the time.


Yet another truth also rises here. Hitler ruled for twelve years. The worst of the horrors occurred during four of them. Stalin’s reign was twenty-five-odd years. Hell on earth is possible to make but is hard to go on making. The human appetite for social peace, if not for social justice, eventually asserts itself. This is of no comfort to the victims. But it should be of some comfort to the survivors, their inheritors, and us. Those who think that the horrors of the nineteen-thirties and forties were eclipses of the sun, rather than an eternal darkness of the earth, are invariably mocked as Panglossian. But Dr. Pangloss, Voltaire’s fatuously optimistic philosopher, is an unfairly reviled man. The Enlightenment philosophers who insisted that the world could be improved were right. Voltaire was one of them. The mistake was to think that, once improved, it couldn’t get worse again. Voltaire’s point was not that optimism about mankind’s fate is false. It was that, in the face of a Heaven known to be decidedly unbenevolent, it takes unrelenting, thankless, and mostly ill-rewarded work to cultivate happiness here on earth, no matter what color the soil. That was the lesson Dr. Pangloss and his students had yet to learn. 
<-------->
In  January of 1842, Ralph Waldo Emerson’s firstborn child, Waldo, contracted scarlet fever and died within a week. He was five. He had been his father’s exuberant companion, who had, Emerson wrote, “touched with his lively curiosity every trivial fact & circumstance in the household.” Henry David Thoreau, who had lodged with the Emersons, “charmed Waldo by the variety of toys whistles boats popguns & all kinds of instruments which he could make & mend.” The death was a shock to the entire village of Concord, Massachusetts. When the nine-year-old Louisa May Alcott came to the Emersons’ door to ask about Waldo, she was greeted, she wrote, by an Emerson “worn with watching and changed by sorrow.” All he said was “Child, he is dead.” Alcott called it her “first glimpse of a great grief.”

But the grief did not feel real, or real enough, to Emerson. “I chiefly grieve that I cannot grieve,” he wrote in a letter the following week. The loss of Waldo spurred an essay, “Experience,” that contains one of the most startling passages in American literature:


The only thing grief has taught me, is to know how shallow it is. That, like all the rest, plays about the surface, and never introduces me into the reality, for contact with which, we would even pay the costly price of sons and lovers. Was it Boscovich who found out that bodies never come in contact? Well, souls never touch their objects. An innavigable sea washes with silent waves between us and the things we aim at and converse with. Grief too will make us idealists. In the death of my son, now more than two years ago, I seem to have lost a beautiful estate,—no more. I cannot get it nearer to me. If tomorrow I should be informed of the bankruptcy of my principal debtors, the loss of my property would be a great inconvenience to me, perhaps, for many years; but it would leave me as it found me,—neither better nor worse. So is it with this calamity: it does not touch me: some thing which I fancied was a part of me, which could not be torn away without tearing me, nor enlarged without enriching me, falls off from me, and leaves no scar. It was caducous. I grieve that grief can teach me nothing, nor carry me one step into real nature.
This seems nearly callous, but that’s the point. Emerson had suffered tragic deaths before, and, partly as a result, had developed a theory of spiritual profit and loss: surely the greatest costs led to the richest benefits? When “a great man,” he wrote in “Compensation,” an earlier essay, is “pushed, tormented, defeated, he has a chance to learn something; he has been put on his wits, on his manhood; he has gained facts; learns his ignorance; is cured of the insanity of conceit; has got moderation and real skill.” But Waldo’s death was so profound that it went uncompensated, even by grief. It taught Emerson “nothing”; it was almost as though his son had never existed. Unluckily, swiftly, even happily, life goes on, only mildly “inconvenienced” by the most devastating loss imaginable.

“Experience” has a knife’s-edge, emergency intensity that is nowhere to be found in Emerson’s poems, collected in “Ralph Waldo Emerson: The Major Poetry” (Harvard), edited by Albert J. von Frank. Prose was a zone of fruitful conflict for Emerson, who began his public life writing sermons. He entered Harvard Divinity School in 1825, at the age of twenty-one, to prepare for a career in the Unitarian ministry. Soon he was struck by a painful eye disease, likely caused by tuberculosis, and submitted to two cataract operations. According to Robert D. Richardson’s “Emerson: The Mind on Fire,” Emerson’s reading in Hume, and his knowledge of the “brilliantly clever arguments of Cicero,” began to erode his faith. His days were “slipping past him, one by one, in an irrevocable procession,” Richardson writes. He knew that the “proper emotion” wasn’t humility or even skepticism but “wonder.”


Emerson’s essays are like wonder handbooks: they tell you where to find it, how to use it, what to do when it fails you. “Nature,” “The Poet,” “Self-Reliance,” “Circles,” “Experience”: you can use these essays to become enchanted; many dejected secular people have gone to them regularly to see the world in renewed and refreshed terms of beauty. They outfit you for a walk in the woods or an ordinary morning. They are modular: you can remember bits of one, bits of another, mess up the order, mix and match. Their authority comes not from the Church or the ministry but from the power of their prose. Emerson must have realized that half of the people in church were there to hear language electrified by the preacher; his essays are, as Harold Bloom put it, “interior oratory,” free-range sermons that make their own occasions.


Emerson also wrote a poem about Waldo, “Threnody.” It is often quite beautiful, but it is dressed almost entirely in period costume. The period was the eighteen-forties, and the costume was woodsy, “native,” and politely anti-European. Henry Wadsworth Longfellow, who returned from Europe to teach modern languages at Harvard, was considered a boldly American poet: he wrote epics about Miles Standish and the expulsion of the Acadians from Nova Scotia. To Emerson’s contemporaries, experimentation in poetry meant writing about “the bobolink and the humble-bee” rather than the English nightingale and the skylark, which, as Thomas Wentworth Higginson, Emily Dickinson’s correspondent and early advocate, wrote, Americans “might never have seen or heard anywhere.”

By these standards, “Threnody,” with its tableaux of American village life, is a masterpiece. Instead of nymphs and dryads, here are the rudiments of New England hill, garden, and scrub forest:

On that shaded day,
Dark with more clouds than tempests are,
When thou didst yield thy innocent breath
In birdlike heavings unto death,
Night came, and Nature had not thee;
I said, ‘We are mates in misery.’
The morrow dawned with needless glow;
Each snowbird chirped, each fowl must crow;
Each tramper started; but the feet
Of the most beautiful and sweet
Of human youth had left the hill
And garden,—they were bound and still.
This is one of the first inventories in verse of a distinctly New England winter, its “needless glow” awakening the tinny chirps of native snowbirds. Those unforgettable “birdlike heavings” of the child’s final breaths puncture a frigid silence known to anybody from Emerson’s neck of the woods. The image is borrowed from his journals, roused to this new formal occasion yet maintaining its fringe of untransformed anguish. But the poem quickly stifles its desperation in the prescribed comforts of noble sentiment and regular music.

The listlessness of Emerson’s poetry is surprising, given the veneration he expressed for the art. Some of his best prose is devoted to lobbying for the special advantages of poetry. These works are thrilling because they are written in thrilling sentences. This does not necessarily imply that Emerson’s poetry will be thrilling, though he must have intended his large claims for poetry to be tested on his own work. Like many of his essays, “The Poet” was printed with an original short lyric as its epigraph. The mediocrity of these poem-epigraphs is often emphasized by the essays’ attempts to honor them as superior forms of expression. It makes for a strangely rigged contest between turbocharged prose and the rickshaw verse it ostensibly reveres. Emerson’s “poet”—a “complete man,” a “man without impediment,” a “sayer” and “namer,” like Adam—would not have printed the lacklustre verses appended to “The Poet,” which venerate “Olympian bards” and “divine ideas” with rhymes as bouncy as a Super Ball.

In “Merlin I,” written, like “The Poet,” in the eighteen-forties, Emerson plays the unwinnable game of arguing in metre against metre and in rhyme against rhyme:

Thy trivial harp will never please
Or fill my craving ear;
Its chords should ring as blows the breeze,
Free, peremptory, clear.
No jingling serenader’s art,
Nor tinkle of piano strings,
Can make the wild blood start
In its mystic springs.
Emerson kept an Aeolian harp in a window of his house. He intended to build in verse its equivalent, an instrument that nature could play. But the instrument itself was old-fashioned, gaudy, and domestic.

Emerson’s ideas were obviously badly served by the rickety verse structures he built for them. Seeing them strain and buckle under the weight of his mind and ambition led him, in “The Poet,” to call not only for a new kind of poem, which, at least in theory, he could have written, but for a wholly new kind of person, a person he wasn’t and didn’t want to become. His best poems—“Each and All,” “Brahma,” “The Rhodora,” “The Snow-Storm”—are refinements of oratory to the special rhetorical technologies of poetry. But his quicksilver prose was poetry, its sentences like signal flares launched one after another into the ether. What he says about the poet is truer of those astonishing prose performances:

For it is not metres, but a metre-making argument, that makes a poem,—a thought so passionate and alive, that, like the spirit of a plant or an animal, it has an architecture of its own, and adorns nature with a new thing. The thought and the form are equal in the order of time, but in the order of genesis the thought is prior to the form. The poet has a new thought: he has a whole new experience to unfold; he will tell us how it was with him, and all men will be the richer in his fortune. For, the experience of each new age requires a new confession, and the world seems always waiting for its poet.
This passage, like so many in his great essays, describes itself, its own idiosyncratic “architecture.” This is what Emerson meant when he called for a literature of “insight and not of tradition.” Each sentence is an innovation, “a new thing.” Emerson didn’t want to write poems about the New World. He wanted poems to make the world new. It is fascinating, therefore, to see how he arranged for his own swift obsolescence. His poems sometimes feel intentionally slight, as though making way for the accelerating future, still at his back but quickly gaining on him. His prose was poetry by other means, calling for its own mirror image, a poetry whose “argument” trumped its forms.

Emerson was not the poet he had in mind in “The Poet.” In 1840, Alexis de Tocqueville had prophesied an American poetry free of “legendary lays,” “old traditions,” “supernatural beings,” masks, and personifications. Americans led “petty” and “insipid” lives, “crowded with paltry interests”: their lives were “anti-poetic.” The only subject possible for an American poet was humankind; luckily, as Tocqueville wrote, “the poet needs no more.” Emerson, who spent most of his life cultivating the aura of an elder, called for “a brood of Titans” who would “run up the mountains of the West with the errand of genius and love.”

In July of 1855, Emerson got the poet he’d been calling for. He picked up a parcel from the Concord post office which contained the first edition of “Leaves of Grass,” sent anonymously from Brooklyn by its author. The book was unsigned, though there was a frontispiece portrait, the name “Walter Whitman” on the copyright page, and, inside, the jubilant line “Walt Whitman, an American, one of the roughs, a kosmos. ” After a little hunting, Emerson found Whitman’s name and the address of his distributor in a newspaper advertisement. He then wrote his famous letter to Whitman, welcoming him to immortality: “I greet you at the beginning of a great career, which yet must have had a long foreground somewhere for such a start.”

In response, Whitman published the letter in the book’s next edition, along with twenty new poems and his own open letter to Emerson of several thousand words celebrating “that vast basis of the supremacy of Individuality—that new moral American continent” whose “shores you found”:

I say you have led The States there—have led Me there. I say that none has ever done, or ever can do, a greater deed for The States, than your deed. Others may line out the lines, build cities, work mines, break up farms; it is yours to have been the original true Captain who put to sea, intuitive, positive, rendering the first report, to be told less by any report, and more by the mariners of a thousand bays, in each tack of their arriving and departing, many years after you.
Whitman was a fact of American life from that moment forward. It took a little longer for an equally important disciple to surface: Emily Dickinson, who treasured an edition of Emerson’s poems given to her by an admirer, and whose brother and sister-in-law, Austin and Susan Dickinson, had hosted Emerson many times at their handsome house, the Evergreens, just across the field from her home. Of course, Dickinson’s poems sound nothing like Emerson’s. He provided, for the wild synaptic activity of his protégés, the framework. He was their server. If Emerson’s poems had been just a little better than they were, we might not have American literature as we know it. Our greatest writers, seeing their own visions usurped, might have been content to remain his readers. 
<-------->
The world you have to live in is // the world that you have made,” writes the American poet Linda Gregerson, whose dauntless, serrated work is collected in “Prodigal: New and Selected Poems, 1976-2014” (Mariner). Gregerson’s poems examine worldly wonder and danger in a single bifocal view. She writes an authentic American georgic, focussed not on Virgilian oxen or olives but on the processes by which, say, cancer cells metastasize or endangered cranes find patches of endangered maize. These operations are terrifying; once catalyzed by human folly, they are mindless, unstoppable, and morally neutral.

But Gregerson is a poet of praise: her poems constantly renegotiate the terms of human happiness and safety in light of randomized peril. “The fault’s in nature,” she writes, “who will // without system or explanation / make permanent / havoc of little mistakes.” Her great subject is coincidence, both its cruelties and its windfalls; though the poems seem unusually exposed to the “havoc” they describe, their brilliance marking them for calamity, they nevertheless delight in chance and seize its unbidden opportunities.


Gregerson, a professor of English at the University of Michigan, is a scholar of the Renaissance and was once a professional actor. She learned from Shakespeare and Thomas Wyatt, among others, what she has called “open voicing,” the probationary “lapses and interruption” that make those five-hundred-year-old voices seem so vulnerable and real. The mind in her poems operates without a map, generating itself as it goes along. I prize in Gregerson a natural drift and shamble, but what sutures everything together is her syntax, gathered in part from the Elizabethans but as sinewy and precise as any in contemporary writing. “I think of grammar as a social contract,” she has said; without it, relations of all kinds cannot be expressed. This is the jumbled world of text messages, or of concordances, where, to quote Elizabeth Bishop, everything is “only connected by ‘and’ and ‘and.’ ” Here is the opening of “Salt”:

Because she had been told, time and
again,
not to swing on the neighbors’ high hammock,
 
and because she had time and again gone
back, lured
by the older boys and their dangerous
 
propulsions, because a child in shock (we
didn’t know
this yet) can seem sullen or intran-
 
sigent, and because my father hated his life,
my sister
with her collarbone broken was spanked
 
and sent to bed for the night, to shiver
through the August
heat and cry her way through sleep.
The controlled detonations of those serial “because”s mark the postulates of the child’s life, her father’s self-hatred just another adverse environmental given. A single sentence builds through its multiple modifying clauses, across lines and stanzas that seem here to flinch from anticipated punishment, toward the final, painful disclosure.

Gregerson’s syntax acts as a strong forward current, carving a jagged path through the stony resistance of her lines and stanzas. Her best-known poems are written in the form of “Salt”: a three-line helix-like stanza with a corseted middle line, a shape that she invented and which Gregerson, not given to hyperbole, says “saved my life.” Her first book, “Fire in the Conservatory,” had intellectual force without that generative formal opposition. Her breakthrough came with her next book, “The Woman Who Died in Her Sleep,” whose opening poem, “The Bad Physician,” immediately flexes the new form. “Even in error the body / wields cunning,” Gregerson writes:

The child who swallows the amnion now
will swallow milk
by winter. The milk
 
can find a use for me but not
for my belief,
nor yours, and it beggars the best
of our purposes.
The poem describes a sick child whose illness progresses by alternately lurching and idling, a pattern suggested by these spasmodic enjambments. A bit later, in an extraordinary passage, the child’s “muddied / gait” finds its harrowing formal coördinates:

My friend’s young daughter moved
 
with a slightly muddied
gait,
and then her tongue
 
and then her hands
unlearned
their freedom, so newly
 
acquired. Unlearned with great
labor
while the tumor thrived,
 
and all the elixirs in Mexico
could not
revise her sentence by a day.
“All the elixirs in Mexico” recalls Humpty Dumpty, a story that feeds children’s delight in comic accident and mishap; “revise her sentence,” with its pun, admits that Gregerson’s own written “sentence” makes not a bit of difference to the girl or to her parents, who suffer under their own shattering sentence. The next time someone asks me what advantage poetry holds over prose, I will point to these lines, which move beyond the description of pain to its tangible embodiment.

An art that brings this kind of finish to poems about pain risks inoculating itself against its subjects. But Gregerson’s poems, with their frequent pauses and hesitations, imply a present listener whose silent cues the poems incorporate. They invite interruption, or, interrupting themselves, they seem to preëmpt someone else’s barging in. They are occasionally tipped into disequilibrium by rough, intercalated challenges to their authority, especially by children, whose innocence they idolize but also fear. In “Song of Myself,” Whitman’s entire rhetorical system grinds to a halt after a child asks him, simply, “What is the grass?” In “Bunting,” Gregerson’s young daughter sees some horrific news footage of Kurdish children dead after a chemical attack:


“They’re sleeping,” said Emma, “they’re very
tired,”
as the footage came on again,
 
child after child in the chalk
embrace
of chemical death. We saw again
 
the elegant economy with which God
sculpts
the infant face. Not one
 
not cast in heaven’s mold.
Both mother and child here are testing the utility of metaphors. “They’re sleeping” is what children say when they know someone is dead; “chalk / embrace,” “elegant economy,” “God,” “heaven”: these are all, in one way or another, adaptations of brute reality to the consolations of poetic language. It’s all too much to take in, but a poem at least doesn’t supplant “the children on the screen” with a commercial that “lures” Emma “to want / with the whole heart of childhood what / money / will buy.”

The adjacency of fortune and misfortune, the coin toss that decides who lives and who dies, who wins and who loses: these subjects require not just eloquence and feeling but an analysis of the whole social order that in other hands would seem incompatible with lyric compression and intensity. The fact that I am writing these sentences and you are reading them means that for us, compared with a large swath of humanity, the coin toss is rigged to go our way most of the time. The worst response to this realization would be to write a poetry of self-indulgent guilt, itself a product of the luxury it condemns. Weirdly, this is where Gregerson’s scholarship sets her apart. Her poems hack their verbal energy from deep sources in Renaissance poetry and its classical models. Indignation, joy, weariness, sweetness: human beings have felt these emotions before, and their prior expressions well up gorgeously in Gregerson’s most impressive passages:

Jason had the misfortune to suffer misfortune
the third
of July. July’s the month of hospital ro-
 
tations; on holiday weekends the venerable
stay home.
So when Jason lay blue and inert on the table
 
and couldn’t be made to breathe for three and a
quarter hours,
the staff were too green to let him go.
 
The household gods have abandoned us to the gods
of juris-
prudence and suburban sprawl.
The repetitions of “misfortune” and “misfortune,” “July” and “July,” recall the interlocking syntax of Virgil’s Latin and of Milton’s approximation of it. We live in a world still haunted by the old superstitions, and even though clear lines of fault can be tracked, we attribute a tragedy like Jason’s to bad luck. Gregerson’s sly employment of an old-fashioned word like “venerable” to refer to A-list doctors, away for the holiday, suggests the ecclesiastical prestige that physicians—some of whom are said to work miracles—still command. This language preserves intact many vestiges of the cultural past. The new gods of “hospital ro- / tations” and “juris- / prudence” turn out to be just as fickle or vindictive as the old.

In “The Woman Who Died in Her Sleep,” as in the best poems from her subsequent volumes and the fine new poems collected here, Gregerson attains what few contemporary poets even seek: a plausible “we,” a basis for speaking across the lines of individual circumstance and social identity. She knows that she is not credentialled to speak for the entire human race, but her vision of justice is clear, her sadness palpable; she hates what happens when people fall outside our reach. In “Font,” a new poem, the news on her home screen of China’s “Baby 59”—who was rescued after being found in a sewage pipe—reminds Gregerson of the world of vulnerability beyond the circle of anyone’s control. Every poet of moral breadth becomes, in time, a lamenter of her own limitations. We’re all trapped inside the pinpoint circumference of human will and power.

When I was reading Gregerson earlier this summer, word arrived that James Tate had died at the age of seventy-one. Tate was among the strangest and most influential American poets of the past fifty years, his career somewhat warped by significant early fame: he won the Yale Younger Poets prize, in 1967, for “The Lost Pilot,” one of the great début volumes in recent memory, and a Pulitzer in 1992. He regarded this esteem, like everything else, with curiosity and amusement. If Gregerson’s empathy rides the long sight lines of American life, as Whitman’s did, her nomadic imagination resting wherever it pleases, Tate, who lived in Amherst, Massachusetts, for most of his adult life, was—forgive me—quietly gregarious in the spirit of Emily Dickinson, awake to the crabbed beauty of his perceptions. His last book is “Dome of the Hidden Pavilion” (Ecco), a title that my nine-year-old son would give to a sci-fi story that he wrote in camp. That’s the point, with Tate: the obstinate refusal, itself childlike, to relinquish a child’s naïveté.

Like many children and eccentrics, Tate can remind us how hard it is to share in another person’s inner universe; we have one of our own that is already often boring enough. But the poems in “Dome of the Hidden Pavilion” are quite moving, partly because, in their oblique way, they face illness and impending death. In “The Psychiatric Unit,” Tate suggests we’re all in one, all the time; in “Cement,” the human predicament is redrawn slightly, its speaker a man condemned, like a soul in Tartarus, to carry “hundred-pound sacks of cement all day.” Tate is fundamentally a poet of embarrassment, ever showing up in the wrong outfit or on the wrong day. The poems track the absurdist accommodations made by their protagonists to reality and logic, but we never quite get the knack for how life is played, and then the game ends.


The book is filled with echoes of his Amherst neighbor. Dickinson’s famous poem “I heard a Fly buzz—when I died” is the iconic American poem of posthumous recollection. The “I” recollecting dwells in the imagination, which seems infinite; it looks upon its body as a sour piece of flesh with its own pitiable destiny. “Dome of the Hidden Pavilion” ends with “Plastic Story,” a reinvention of Dickinson’s poem:

I had barely said my prayers when I felt a large insect crawl
over my face. I was afraid to move. When I opened my eyes
I saw it was a piece of plastic that had torn loose from
a project I was working on in the next room. But what had
torn it loose?
The piece of plastic is mysteriously self-propelled (“There’s no wind in the house. A piece of plastic can’t / fly on its own”); in the course of the poem, the plastic turns sinister and strangles Tate. Dickinson’s poem is, in part, a boast: the imagination is so powerful that it can imagine its own end. Tate’s ironic version stages his murder at the hands of an unimportant detail—a typical detail from a Tate poem, whose work is filled with such effluvia. The plastic thing kills him, but only within the poem that he created. Which one wins: James Tate, or his homicidal piece of plastic?
<-------->
Sara Solovitch, in “Playing Scared: A History and Memoir of Stage Fright” (Bloomsbury), says that while she was a good pianist as a child, she fell apart—sweating, trembling—when she had to play for an audience. She got through the Eastman School of Music’s preparatory program. Then she quit studying piano, grew up, got married, had children, and became a journalist. In her late forties, though, she drifted back to the piano, taking a course at a community college. By this point, she had no professional ambitions. Surely, she thought, she would now be able to perform calmly. But when her teacher asked her, one night, to play in front of the class, her hands began shaking so hard that she could barely strike the keyboard: “I gazed down at myself from a distance high above the keys, watching a body that was no longer in charge. My fear was at the controls, like an independent organism emerging from inside me, my own Rosemary’s baby.”

Stagefright has not been heavily studied, which is strange because, as Solovitch tells us, it is common not only among those who make their living on the stage but among the rest of us, too. In 2012, two researchers at the University of Nebraska-Omaha, Karen Dwyer and Marlina Davidson, administered a survey to eight hundred and fifteen college students, asking them to select their three greatest fears from a list that included, among other things, heights, flying, financial problems, deep water, death, and “speaking before a group.” Speaking before a group beat out all the others, even death.


Stagefright has been aptly described as “self-poisoning by adrenaline.” In response to stress, the adrenal glands pump the hormone epinephrine (adrenaline) into the bloodstream, causing the body to shift into a state of high arousal. The person’s muscles tense, he sweats and shakes, his heart pounds, his mouth goes dry, he has trouble breathing, he may become nauseated or dizzy, and his throat constricts, making his voice rise in pitch. This is the so-called “fight or flight” response, which our species is thought to have developed because it helped prepare the body for forceful action in response to a threat. But what Cro-Magnon man needed upon finding a bear in his cave is not what a modern person needs in order to play King Lear. Without the release of abrupt action, the hyperactivation becomes, basically, a panic attack.

As for the thoughts accompanying the physical response, the most important seems to be a feeling of exposure. The English theatre scholar Nicholas Ridout, in his excellent book “Stage Fright, Animals, and Other Theatrical Problems” (2006), compares the situation to that of a snail having its shell ripped off. His countryman Stephen Fry, who, one day in 1995, left London—indeed, England—to avoid appearing in the play he was scheduled to perform in, says that, when stagefright hits, the audience sees “the shrivelled penis in your head.” And, in the typical case, the performer can do nothing to change the spectators’ minds, because he feels utterly empty. In 1989, Daniel Day-Lewis, playing the title role in Richard Eyre’s production of “Hamlet” at London’s National Theatre, turned on his heel in the middle of the show and walked off the stage, never to return. (In the twenty-six years since then, he has acted only in movies.) “I had nothing in me, nothing to say, nothing to give,” he said. Others stay, but only by force of sheer, grinding will.

In a number of ways, stagefright doesn’t make sense. Laurence Olivier, when he was in his late fifties, was visited by a spell that lasted, intermittently, for five years, causing him great anguish. At the time, he was the most celebrated stage actor in England. How could he be frightened of failing? Ditto Mikhail Baryshnikov. In the nineteen-seventies and eighties, Baryshnikov was the most famous ballet dancer in the world, and he probably still is, though he ceased classical dancing some twenty-five years ago. Since then, he has built a successful career in modern dance and theatre. But he experiences terrible stagefright, and says that it has only got worse over the years.

This is another mystery of stagefright—that, in so many cases, it doesn’t let up with time. If the artist repeatedly goes onstage fearing failure, and instead has a success, shouldn’t the fear eventually extinguish? “I am onstage more than fifty years,” Baryshnikov says. “Sometimes I do shows every night for weeks. Still, it never doesn’t come. Starts four hours before. I don’t even try to fight it anymore. I know it will always be there.”

A final mystery of stagefright is just how many otherwise capable people suffer from it. A few writers on the subject have suggested that it is a modern phenomenon, born of the nineteenth and twentieth centuries. But Scott Stossel, in his recent book “My Age of Anxiety,” quotes Cicero, ancient Rome’s acclaimed orator, saying, “I turn pale at the outset of a speech and quake in every limb.” After Cicero, examples in the literature thin out until the eighteenth century. Then, however, we get some impressive ones, including Thomas Jefferson, who is said to have been mortally afraid of public speaking. As President, he gave only two speeches, his two inaugural addresses. Gandhi was terrified of having to speak to a group: his vision would fog over; he would fall mute.


As for performers, Barbra Streisand, singing in front of more than a hundred thousand people in Central Park, one night in 1967, repeatedly forgot her lyrics. For twenty-seven years thereafter, she refused to perform live except at charity concerts. Adele told British Vogue, “I puke quite a lot before going on stage, though never actually on the stage.” Jay Z told Terry Gross, on “Fresh Air,” that performance anxiety is the reason rappers often grab their crotches when performing. Many of them, he said, are not accustomed to live performance. “You get up there, you feel naked,” Jay Z said. “So when you feel naked what’s the first thing you do? You cover yourself.”


But singers don’t have to perform live; they can fall back on recording. The two most famous stagefright victims among concert pianists also took that route. Vladimir Horowitz, probably the most acclaimed piano virtuoso of the late twentieth century, retired from public performing four times, for long periods. (One lasted twelve years.) But he didn’t stay home. Some of his finest recordings were made during those sabbaticals. An even more notorious withdrawal, because it was permanent, was that of the Canadian master Glenn Gould. From the beginning of his concertizing career, when he was in his teens, Gould feared and hated the audience. He felt that the spectators wanted him to fail; he was sure that, in any case, he would get germs from them. He retired from the stage at the age of thirty-one and devoted the rest of his short life—he died at fifty—to experimental recordings. In a parallel manner, actors like Daniel Day-Lewis who have found that they can’t bear the stage have switched over to film. There they needn’t fear a muffed scene so much; they can always ask for a retake.

These examples, numerous as they are, are just the ones that appealed to me. There are many, many more: Ella Fitzgerald, Luciano Pavarotti, Mel Gibson. And those are only the people who have been willing to talk about the problem, or whom others have talked about. Performers are often reluctant to discuss stagefright. They think it’s bad luck. (Likewise, most baseball players do not want to discuss the yips.) Also, the stories that one hears are usually about the very bad cases, whereas stagefright is not a single condition but a spectrum, stretching from those who may vomit in their dressing rooms but then go onstage, blazingly, to those who are forced to stop performing. In between is a large intermediate group of people whose careers have been not ended but simply diminished by anxiety. Two years ago, before undertaking a one-woman show on Broadway, Bette Midler told Patrick Healy, of the Times, that she had wanted to be a serious dramatic actress but had faltered for lack of courage. “I have that terror,” she said. “Will people like you? Will they ask you back? Did I make the cut? That’s always on my mind.” To hear the brash, funny, commanding (as far as we knew) Midler tell of worrying whether people would like her is painful. But, in every group of artists, the insiders can tell you who, among them, should have had a bigger career but, for some reason, was held back.

Forces in the culture may help breed stagefright, by making avoidance of the stage seem a reasonable artistic choice. Twentieth-century avant-garde theatre had a strong anti-theatrical bias, the idea being that to care about the people in the audience—to want to entertain them or even to make oneself understood by them—was a forfeiture of artistic status, a lowering of one’s sights. In 1958, the serial composer Milton Babbitt published an essay entitled “Who Cares If You Listen?,” and, though he later said that the title was invented by an editor, its wording sums up the essay pretty accurately. (In music, the twentieth-century artist’s divorce from the general public was probably more bitter than in any other art.)

The popularity, in the nineteen-sixties and seventies, of the writings of the communications theorist Marshall McLuhan no doubt increased the estrangement between the audience and performers who were trained to the live stage. According to McLuhan, what mattered in a work of art—or, at least, what the audience responded to—was not so much the intended content as the medium through which it was conveyed: whether it was live or broadcast, and, if broadcast, on radio or television, and with what amplification, what splices and inserts, and so on. Glenn Gould was a devotee of McLuhan, and to be a McLuhanite meant that you could abandon live performance without any shame, any talk of fright. You were just doing the up-to-date thing. But, in any discussion of the relationship between technology and stagefright, splices and inserts are a small matter. The crux, of course, was the invention of sound recording and then of film, in the late nineteenth century. These things did not create stagefright, but they fostered it, by enabling performers to do their work without having to appear in front of an audience.

Cartoon
“After a hard day at the office, all Barry wants to do is put his feet up and listen to somebody tell him what to think.”
BUY THE PRINT »
Nicholas Ridout says that stagefright may also have social and political underpinnings. Before the twentieth century, and certainly before the nineteenth, many people onstage were there at the behest not of the public but of private patrons—for example, the king. Such performers could, of course, lose their patrons’ favor, but success and failure were not as coldly calculated as they came to be via the box office. Once that switch occurred, paychecks were at risk, not just esteem and self-esteem. Around the same time, the social class of actors began to merge with that of their characters. With the advent of realism, plays were no longer about the rich or the royal; they were about Uncle Vanya or Hedda Gabler. Meanwhile, with the decline in the power of the Church, a stage career became less stigmatized socially. Actors rose into the middle class. In the words of Ridout, “This means that the ‘actual life’ the actor is required to simulate is close enough to her own life for her own to become a private resource for public display.”

The convergence was hastened by the introduction of the Stanislavsky technique in Russia and its spread to the West as “method acting.” Now actors were not just socioeconomically nudged toward identification with their characters; they were forthrightly asked to invest their most personal resources—their emotions, their memories—in their enactments, so that when the audience clapped, or not, the actors could easily feel that what was being approved or disapproved was not so much their skill as them. It is no surprise that the person who came up with the image of the audience as a black hole was Stanislavsky. In his treatise “An Actor Prepares,” the young actor Kostya, describing a rehearsal of “Othello,” says, “I had hardly stepped on to the stage when there loomed up in front of me the immense hole of the proscenium arch, and beyond it an endless expanse of dark mist.” An endless expanse, dark: this is something out of a nightmare, something that could suck you in, swallow you, and your family would never find you again.

Add to these complex factors a simpler one: stagefright may be the product of nothing more than shyness, a disinclination to do one’s work in front of a roomful of people. Carly Simon, who had a long history of stagefright—she once took six years off from live performing—was asked about this by Charlie Rose. She answered that she wouldn’t call it fright. She just didn’t want to be center stage. “I would prefer to be a background singer or a tambourine player, or part of the crew,” she said. It seems cruel that someone who has been given the gift of singing or acting or dancing should find herself unequipped with the wish to exercise it in public. Some performers displace this cruelty onto the audience. The pianist Charles Rosen believed that the spectators were out there waiting for the performer to slip up: “The silence of the audience is not that of a public that listens but of one that watches—like the dead hush that accompanies the unsteady movement of the tightrope walker poised over his perilous space.”

It’s logical that this strong statement should come from a concert pianist. Though many writers on stagefright loyally claim that their own art carries the highest risk, I do not see how anyone could deny that musicians have it hardest. Dancers get relief from anxiety just by moving, and to a rhythm, which restores regular breathing. Actors, when the curtain goes up, usually have some narrative matter that they have to communicate to the audience—the dinner guests are coming, the kingdom has to be divided in three, whatever—and this task will help get their minds off their jitters. Furthermore, dancers and actors are usually onstage with others, who cue them for their lines and their steps, and just keep them company. Solo performers of music are up there alone.

Then, there are the special circumstances of the musicians’ education. Typically, they have not had what anyone would call a normal childhood. At least by adolescence, a person aiming at a soloist career in classical music is practicing about five hours a day. This means that he is alone for at least a third of his waking hours and therefore, unlike his peers, is not engaged in what psychologists call “ego development.” He is not finding out what other people are like; he is not learning how to handle doubt, fear, envy, delay, failure—indeed, success. And, if the young pianist and his family are ambitious, this curtain will come down long before adolescence. Charles Rosen started piano lessons at four and went to Juilliard when he was seven. Could he open the school’s front door? Could he reach the drinking fountain?


There are various ways of coping with stagefright. One is drugs, notably, beta-blockers, which interfere with the binding of stress hormones to their receptors in the sympathetic nervous system and thus weaken the fight-or-flight response. Notably, they quiet pounding hearts. They were first marketed, in 1967, to treat angina, and they are still prescribed for that purpose, as well as for others. But people with heart problems are not the only ones who have palpitations. A 1987 survey conducted by the International Conference of Symphony and Opera Musicians, which represents fifty-two major orchestras in the United States, found that twenty-seven per cent of its members had used beta-blockers. Today, the figure is no doubt considerably higher.

For years, the drugs were controversial. Some people said they resulted in “phoned in” performances. Some raised the ethical question, asking whether the use of beta-blockers by pianists was any different from the use of steroids by athletes. (There is an important distinction, though. Steroids add to the body, increasing muscle mass in order to improve performance. Beta-blockers remove something from the body—the flutist’s lip tremors, the cellist’s hand tremors—in order to permit the person to produce the kind of performance he has already shown himself capable of, outside the auditorium.) But opposition seems to be dwindling. In 2004, the psychiatrist Michael Craig Miller, who was then the editor of The Harvard Mental Health Letter, told the Times, “There’s very little downside except whatever number you do on yourself about taking the drugs.”

Beta-blockers temper only the physical symptoms of anxiety. Instead of sitting there saying to yourself, “Oh, I’m going to do terribly,” and listening to your heart pound, you say those things without listening to your heart pound. If you want, in addition, to eliminate the cognitive components of performance anxiety, you have to look elsewhere. There is a wide range of behavioral and mental exercises that might help, and these are the main subject of Sara Solovitch’s “Playing Scared.” Few of them have firm scientific support, but Solovitch is kind to them.

At the more reasonable end of the spectrum are the Eastern-derived disciplines, such as yoga and meditation. These do not necessarily cure the stagefrightened but simply comfort them, as they do other people, by getting them to breathe properly, taking their minds off their troubles, and, perhaps, for those who are so inclined, putting them in touch with a higher power. Another approach is cognitive-behavioral therapy, or the guided revision of one’s thoughts. Like yoga, this is certainly not something that was invented for stagefright—it’s what people with regular insurance plans are likely to receive today by way of psychotherapy—but it can, apparently, help some victims of performance anxiety, above all by discouraging perfectionism.


Things don’t stay commonsensical for long, however. “Stage fright is passion energy that’s stuck in the body,” one therapist tells Solovitch. A trumpet player advises her—she paraphrases—to “love yourself into excellence by cultivating an internal audience that’s loving.” Soon Solovitch introduces us to E.F.T. (Emotional Freedom Technique), in which you tap various places on your face while uttering restorative phrases, such as “I deeply and completely accept myself,” and E.M.D.R. (Eye Movement Desensitization and Reprocessing), whereby you replace traumatic memories with good memories as the therapist moves his fingers back and forth in front of your face.

With these treatments, though, Solovitch can at least say what is observably taking place. Other teachers and therapists she interviews don’t seem to tell her much more, in essence, than that people with stagefright must be induced to “center” and “focus” and move into the “zone.” She’s not fooled that she’s getting clear answers, but she is sometimes charmed by the eccentricities and the enthusiasms that flourish in this corner of the therapeutic community. She tells how one seventy-five-year-old stagefright specialist, a psychiatrist with a side career in jazz piano, runs up and down Mt. Tamalpais four times a week. She takes detours into the performance anxieties of other cultures: aymat zibur, or fear of saying prayers, among Orthodox Jews in Israel; dhat, or semen-worry, among Hindus. This is fun, but one senses that Solovitch wouldn’t be bothering with it if her declared subject, the treatment of stagefright, were large enough to fill a book. It isn’t. There seems to be no cure for stagefright.

Maybe it’s foolish to expect that there would be. Really, what many of these performers do is almost impossibly difficult. They’re right to be afraid. Solovitch repeats a famous story about Pablo Casals. Once, in 1901, he went hiking and a big rock fell on his bowing hand, crushing several fingers. Casals recalled that his first thought was: “Thank God! I’ll never have to play the cello again!” By that time, though he was only twenty-four, he was regarded by many as the greatest cellist in the world. He had given a command performance before Queen Victoria; he would soon give one for Teddy Roosevelt. So imagine what it was like for him, year after year—he lived to be ninety-six—to walk into concert halls filled with people who had come to see the greatest cellist in the world. Baryshnikov believes that it is the feeling of obligation to the audience that triggers stagefright: “Suddenly the morality kicks in. These people bought a ticket to your show.” He thinks it is useful, if he has to give a speech, to say something completely outrageous to himself beforehand: “Like ‘What the fuck I am doing here?’ You hear your voice. Somehow it helps.” Perhaps, by its craziness, it mitigates the morality.

Sometimes, when performers speak of stagefright, one senses that they do not actually wish it gone—that, for them, it is almost a badge of honor, or, at least, proof that they’re serious about their work. As musicians, especially, will tell you, what they are doing up there is not meeting an agreed-upon goal but, rather, creating something new. Horowitz insisted that the notes in the score did not tell you what the music was. The music was behind the notes, he said, and the performance was your search for it: “I play, so to speak, from the other side of the score, looking back.” This sounds pretty frightening, and, according to some, it was—for the audience. André Watts said that Horowitz, onstage, was “like a demon barely under control.”

There is considerable romanticism in all this. The idea is that the performing artist is a sort of Prometheus: in order to bring us the fire, he has to agree to have his liver eaten. “A divine ailment, a sacred madness”: that’s what Charles Rosen called stagefright. He said that its physical manifestations were the same as those described in medieval medical treatises as the symptoms of the disease of being in love. Many performing artists would be embarrassed to go that far. “People tell you that you have to be nervous to do well,” Emanuel Ax says. “I don’t believe that.” He also finds it self-congratulatory: “Playing the piano, it’s not brain surgery. If I don’t do well, nobody’s going to die.” And he feels that stagefright is a betrayal of what should be the spirit of concertizing. “What you’re trying to do is share music with people who want to hear music.” So why all the fuss? “It’s a terrible waste of time.”

Still, he has stagefright. He doesn’t throw up, he says, but his hands go icy cold. “It happens every time, in varying degrees.” He thinks he’s getting over it, though. Or, “I’m working on it.” He’s sixty-six. 
<-------->
The essayist and caricaturist Max Beerbohm was one of the great figures of the late Victorian and Edwardian era in London—and then had a surprising Indian summer in America in the early nineteen-sixties, when Edmund Wilson wrote at length in his praise, and the playwright S. N. Behrman serialized a book of conversations with the very elderly Max (his admirers always call him by his first name, a not entirely honorable honor) in this magazine. John Updike and W. H. Auden, too, wrote about him, here and elsewhere. Since then, his reputation, like that of most of his contemporaries, has not so much collapsed like a house of cards as shrunk like a boiled head. It remains sharply chiselled, feature by feature, but on a much smaller scale: still intently animate to those who want him, invisible to those who don’t.

Now N.Y.R.B. Classics, which had previously reprinted “Seven Men,” a collection of stories presented as memoirs, with an introduction by Updike, has published a larger anthology of Beerbohm’s work, with the unfortunately patronizing title “The Prince of Minor Writers.” The anthology, on the whole well chosen, begins with a loving if not terribly insightful introduction by Philip Lopate, whose gifts as a New York-style essayist—a troubled intelligence and a blunt talent for heartfelt statement—are so at odds with Beerbohm’s high masquerading style that we are left without much of a feel for the thing inspected. (At one point, he refers to Beerbohm as “a clotheshorse,” although no term could be more ill suited to a writer whose dandyism is so subtly and purposefully differentiated from the dandyism of an Oscar Wilde or a Reggie Turner, not to mention a Beau Brummel.)


I discovered Beerbohm when I was a teen-ager, stumbling on a collection called “The Incomparable Max.” (The nickname came from Bernard Shaw.) Having since read, I think, pretty much every line he ever published—including his theatre criticism, not represented here, his radio broadcasts, and even his verse—I was for a long time a passionate Maximilian, even making a failed college effort to turn his novel, “Zuleika Dobson,” into a musical comedy, an enterprise at which Wolcott Gibbs and George and Ira Gershwin also failed. As the years have gone by, Beerbohm has remained a beacon, but he has also become something of an exasperation. The question is why a writer of almost Proustian gifts has so much less than Proustian achievements; and the answer may rest in a certain catastrophic form of Englishness, in the cult of the little, the diminutive, and the unambitious, a dread of pretension raised to an aesthetic principle. Beerbohm is as English a writer as there can be—fleeing England as soon as he could for Italy, a very English thing to do, while never in forty years learning more than a few words of Italian, also a very English thing to do. Reading Max, you can sense why Paris, in that last great exhalation of writing before the Great War, remade human consciousness, while London, during the same time, remade only its manners. Dandies, it seems, are dandy; but belles-lettres is better.

Beerbohm’s writing tends to be treated by his critics, and even by his admirers, as being all of a piece; minor implies monotone. But it comes in three very distinct colors. There is a period of Pater- and Wilde-style aestheticism, which made him famous on his emergence from Oxford, in the eighteen-nineties, when, at the age of twenty-four, he cheekily published his collected “Works”—highly mannered and unreal and full of Pateresque turns and a purposeful superficiality, counselling cosmetics for women and symmetrical neckcloths for men. Then, there is the journalism, which he began when, in 1898, mostly for money, he succeeded Shaw as the drama critic of Frank Harris’s Saturday Review—a body of writing far more functional, intelligent, impatient, and, often, ill-mannered than his reputation might suggest, the outstanding instance of the form between Shaw and Tynan.

Max was a fine critic of drama. But even better were his forays into dramatized criticism: close reading set in motion as narrative. This includes the stories in the 1919 “Seven Men,” about the dire effects of reading and storytelling on the human soul, along with occasional essays like “A Clergyman” and “Quia Imperfectum,” the first on Dr. Johnson and Boswell, the second on Goethe and German Romanticism. The parodies in “A Christmas Garland” (1912), generally thought to be the best such collection in English, are also criticism of a kind, less genial and more pointed. The tones tend to reappear as needed: “Zuleika Dobson” (1911) is, with its po-faced climax of mass suicide among the Oxford undergraduates in despair at Zuleika’s beauty, very much in the first, aesthetic manner. His BBC broadcasts from the Second World War are written in the style of his brisk, confiding drama criticism, popular journalism of a high order, simple narratives well related.


Beerbohm was a major caricaturist as well—Bernard Berenson called him, hyperbolically but not ridiculously, “the English Goya.” Though his practice was rooted in the French fin-de-siècle practice of caricature, with its emphasis on elegance and animation, more than on Daumier-like grit and grime, Max gave his caricatures a particularly English kind of narrative flair: the series he did of older authorial selves meeting their younger ones, including Henry James and Arnold Bennett (Old Self: “All gone according to plan.” Younger One: “My plan, you know”), is a high-water mark in the history of literary cartooning.

Though geniality is the mood, malice is the savory ingredient—malice passed through a sieve of manners. Beerbohm is in fact quickly disputatious and highly opinionated, on subjects from Strindberg to the music hall. Watching Sarah Bernhardt perform in French to rapturous audiences provoked him to write a sort of angry exposé, “Hamlet, Princess of Denmark.” (Not atypically for the period, there could often be an unhappy vein of misogyny in Max’s brand of malice.) Criticism, in all its guises, is the leitmotif of his art, the place where he breathes most easily. His two best books, “Seven Men” and “A Christmas Garland,” are exclusively about criticism, about reading with a purpose. All seven of his seven men, himself among them, are writers who have an obsessive relationship with texts. Max’s real subject is the one that, in his years in exile, he lived—the pathos of how passionate readers come to be made up of words, which eventually seem far more real than their lives.


The provocateur Malcolm Muggeridge, back in the Beerbohm-infected sixties, once stirred outrage by insisting that Beerbohm was both Jewish and gay, and in denial about both. This has been strenuously refuted by his biographers, who claim, following on Beerbohm’s own account, that his ancestors, merchants who arrived in England from what is now Lithuania in the mid-nineteenth century (sometimes the background is said to be Dutch), were somehow pure Protestant stock—which is exactly what a Jewish family that didn’t want to admit to it would have said in the period. Certainly, the enclosing tone of Max’s relationship with his mother sounds less Dutch or Lithuanian than Ashkenazi. His best friend, Reggie Turner, came from an assimilated Jewish background, while both of Max’s wives were Jewish—first, the American Florence Kahn, and then Elisabeth Jungmann (though by that point he was essentially marrying his nurse). Ezra Pound, a neighbor in Italy, caricatured him as Jewish, and, though hate is hate, hate at times has eyes to see. And the very buttoned-up front that Max showed the world was typical of the closeted Jews of his time. It was a distinguished theatrical family: his half brother, Herbert Beerbohm Tree, was one of the great actor managers of the day.

As for his homosexuality, the very touching letters he wrote to Florence, around their engagement make it plain that he will remain unable to perform sexually with her: “The other sort of caring is beyond me,” he wrote. “It is a defect in my nature.” Biographers declare him a “natural celibate,” but the tone of his early letters to Oscar Wilde and his circle tends to be casually, if cautiously, “Uranian,” as they called themselves. He writes to Wilde’s intimate Bobbie Ross, for instance, urging him cheerily not to introduce their mutual friend Reggie Turner to “the love that dares not tell its name,” adding, “You are a person of stronger character and it doesn’t affect you the way it would affect him.” The tone is not that of an outsider looking in. Homosexual in his inclinations, but seeing what a mess it could make of life then, he may well have chosen celibacy. And it is certainly Wilde’s example and scandal that hang over all his early work and, in many ways, over his life. Max came to fame within Wilde’s orbit, if not directly under his aegis—“The gods bestowed on Max the secret of perpetual old age,” Wilde said of the young Beerbohm. Max writes about Wilde again and again, returns to him obsessively even as an old man, is still scribbling caricatures of him, and hostile ones, to be sure, at the end of his life. Perhaps only Hemingway in the twenties ever had the kind of attraction-repulsion for a generation of writers that Wilde did for his.

Cartoon
“I don’t know—seems like a lot of work.”
BUY THE PRINT »
Max never betrayed Wilde, as so many of his friends did, and had the courage, while the scandal was still fresh, to insist that “The Importance of Being Earnest” was the masterpiece it is. But he always blamed Wilde for his own imprisonment, and saw it as a crime, or a tragedy, of hubris. Max wrote, near the end of his life, “I suppose really it was better that Oscar should die. If he had lived to be an old man he would have become unhappy. Those whom the gods, etc. And the gods did love Oscar, with all his faults.” The generosity is typical; so is the shrug of that “etc.”

The Wilde scandal was no doubt a trauma. It must have encouraged Max to live behind a series of masks, including a mask of Englishness, and one of diminutive infantilism—he draws himself always with outsize head and small body, like a baby. But the period right after the Wilde scandal saw Max’s prose move beyond exquisitism toward a new forcefulness. His style is sometimes called Latinate or overelaborate, but in truth he tried to make it a vocal, speaking, natural style. He loved writing that sounded like talk. A huge fan of the music halls, he actually wrote a song for a music-hall performer titled “But ’E’ll Never Be the Man ’Is Father Woz,” about the son of a pub owner:

I drops in to see young Ben
In ’is tap-room now an’ then,
And I likes to see ’im gettin’ on becoz
’E’s got pluck and ’e’s got brains,
And ’e takes no end o’ pains,
But—’e’ll never be the man ’is Father woz.
The man who could write a lyric that natural is not a sentence-maker remote from the demotic sounds around him. Indeed, no one is more emphatic about the necessity of making a style out of the sound of spoken English. His love of a natural-sounding prose explains why, when he came back to England in the thirties, getting out of Fascist Italy, he was right at home on the BBC, giving a series of broadcasts that demonstrate how to offer an intelligent radio talk that wins without exhausting its audience.

The trick is that, like Kipling, he understood that the sound of spoken English might be anything but blunt—that spoken English tends to be more circuitous, touched by asides, than the self-consciously simplified kind. There are two kinds of extended sentences: one depends on expanding an idea, the other tries to detail a consciousness. The first is argumentative, the second exquisite. The old-fashioned, Johnsonian kind that packed a book into a sentence was going away, Max knew, but the kind that vibrated a small sensation out to its full potential resonance was still alive—indeed, central to all the avant-garde writing of the period. (Beerbohm learned his long sentences from James—who had learned them from the French, and then taught them back to Proust.) Beerbohm had an unexampled gift for gear-shifting between long and short sentences. He knew that it all depended on voice, as he wrote of Whistler’s writing:


The style never falters. The silhouette of no sentence is ever blurred. Every sentence is ringing with a clear vocal cadence. There after all, in that vocal quality, is the chief test of good writing. Writing, as a means of expression, has to compete with talking. The talker need not rely wholly on what he says. He has the help of his mobile face and hands, and of his voice, with its various inflexions and its variable pace, whereby he may insinuate fine shades of meaning . . . but the writer? For his every effect he must rely wholly on the words that he chooses, and on the order in which he ranges them, and on his choice among the few hard and fast symbols of punctuation. He must so use those slender means that they shall express all that he himself can express through his voice and face and hands or all that he would thus express if he were a good talker.
This new talking style—so remote from the languorous undergraduate writing of the collected “Works”—fills “Seven Men,” a true masterpiece of English prose, where the conventional, ever so slightly stuffy sound of the literary memoir slides magically into the elaborately unreal plots of the storytellers. Each story is a study, almost medieval in its neat balance of temptation and punishment, of a literary sinner. Enoch Soames is a sub-Baudelaire diabolist who manages to travel forward in time to see if he is remembered in the future. (He is, but only as a character in a Beerbohm story.) A. V. Laider is a man unable to stop himself from telling horror stories as though they were truthful anecdotes, confessing to his listeners only to begin again, and somehow renewing their credulity by his repentance. “Savonarola” Brown is a clerk who, by night, writes Shakespearean verse about Renaissance Florentine politics.

The stories are inspired by Henry James’s literary tales—“The Figure in the Carpet,” “The Death of the Lion,” and the like—but surpass them in the range and human sympathy of their character-making. All James’s authors are some version of James, while Beerbohm’s, though of the period, are as varied as they are eternal: the Aspirant, the Mediocrity, or the writer, like Enoch Soames, who believes with all his heart in Daring without ever being able to Dare himself. (Later, he would imitate Bukowski, rather than Baudelaire.) There’s the writer who gives it all away in talk and the writer who can do his work only in complicated rivalry with some other writer, as in the case of Maltby and Braxton, the authors of two whimsical classics of the nineties, always on the scene, and always, to their dismay, bracketed together, like Julian Barnes and Martin Amis in another London time. Beerbohm describes them in a paragraph that shows his mastery of sentence oscillation, the long and polite sentences alternating with the short and candid ones, and also the essential satiric disdain that underlies his wide-eyed tone:

No one seeing the two rivals together, no one meeting them at Mr. Hookworth’s famous luncheon-parties in the Author’s Club, or at Mrs. Foster-Dugdale’s not less famous garden parties in Greville Place, would have supposed off-hand that the pair had a single point in common. Dapper little Maltby—blond, bland diminutive Maltby, with his monocle and gardenia; big black Braxton, with his lanky hair and his square blue jaw and his square sallow forehead. Canary and crow. Maltby had a perpetual chirrup of amusing small talk. Braxton was usually silent, but very well worth listening to whenever he did croak. . . . But the casual observer of Braxton and Maltby at Mrs. Foster-Dugdale’s or elsewhere was wrong in supposing that the two were totally unlike. He overlooked one simple and obvious point. This was that he had met them both at Mrs. Foster-Dugdale’s or elsewhere. Wherever they were invited, there certainly, there punctually, they would be. They were both of them gluttons for the fruits and signs of success.

The six stories detail literature as an addiction, rather than as a vocation: all the hero-sufferers would be saner and happier if they had never caught the literary bug. None of them will truly succeed as writers, but none can quite be cured of writing. In this way, they belong to the same world of feeling as “A Christmas Garland,” the Christmas-themed book of parodies Beerbohm published in 1912. There, too, literary style is treated as a kind of seizure, one that takes entire control of a once sane man. Although only a few of those parodied are still much read, the parodies supply their object. Just by reading Beerbohm’s parody of Arnold Bennett’s North of England novels, you understand that Bennett was earnest and awkward, and tried too hard to be cosmic.

Cartoon
BUY THE PRINT »
Though the tone is superficially amiable, Max’s real disgust with literary falseness is felt on every page. The old saw is that parody is essentially appreciative, really a form of flattery. But good parody is an assault, and wounding. A parody is flattering inasmuch as it assumes a density of style that is capable of being imitated. But a parody also posits that a writer can be reduced to a string of tics and mannerisms—that the writer’s style is a code that can be cracked. A writer sees the tics and mannerisms as things under his control, while the parodist suggests that they have taken control of him. And so when Beerbohm has Chesterton confess, “Love is always an extraordinarily fluent talker. Love is a windbag, filled with a gusty wind from Heaven,” he is stepping past Chesterton’s own voice to turn him into a ventriloquist’s puppet, offering a verdict on the ventriloquist. It is not a hanging judgment; but the judgment hangs. Max keeps the charm of his manner in constant tension with the malice of his attitudes. He openly confessed to hating Kipling, while recognizing his genius, and his parody of Kipling is a brutal picture of imperial hysteria at a squirrelly high pitch. (“ ‘Frog’s-march him!’ I shrieked, dancing. ‘For the love of heaven, frog’s-march him!’ ” he has Kipling cry out as Santa is arrested by a policeman and dragged to the station.)

The one exception, the one entirely affectionate parody in the book, may be the best parody in English—the one of Henry James, called “The Mote in the Middle Distance.” It is an absurdly extended description of a small boy looking at his Christmas stocking on Christmas morning while his still smaller sister sleeps. Yet the full, circuitous, late-Jamesian manner, given to a child, works, because children are actually “Jamesian,” helpless creatures of observational nuance: all they have is their scruples and second thoughts to make them human. We get impatient with the grownups in, say, “The Golden Bowl,” because they are grown people who ought to be able to act. Kids can’t act, really. They can only muse. Grownups stuff Christmas stockings. Children observe them:

Thus the exact repetition, at the foot of Eva’s bed, of the shape pendulous at the foot of his was hardly enough to account for the fixity with which he envisaged it and for which he was to find, some years later, a motive in the (as it turned out) hardly generous fear that Eva had already made the great investigation “on her own.” Her very regular breathing presently reassured him that, if she had peeped into “her” stocking, she must have done so in sleep. . . . She really was—he had often told her that she really was—magnificent; and her magnificence was never more obvious than in the pause that elapsed before she all of a sudden remarked, “They so very indubitably are, you know!”
Beerbohm’s best writing is a form of criticism of other people’s; his gift for the observation of manners is small next to his gift for the understanding of how writing engraves itself on our brains. “Note that I am not incomparable,” he said once to Behrman, protesting the “incomparable” label. “Compare me.” If we do, we find that, among the great English essayists, he is the one whose genius depends least on the apprehension of immediate experience and most on what happens when we read. Everything good he writes is about how books, after building us up for life, let us down once we’re in it.


Before the First World War, Beerbohm and Florence had already retreated to Rapallo, on the Italian coast, and the self-exile became part of his legend. He said that he had gone because he knew too many people in London too well—“How many people were there in London? Eight million? Nine million? Well, I knew them all”—or because he “wanted to be alone with Florence,” which can’t have been good for her, as she seems to have been a nervous type who might have benefitted from less isolation. His motives are perhaps less mysterious than they might seem: most English writers dream of escaping to Italy—many of his heroes, from Byron to Browning, had done it. The claustrophobia that afflicts a metropolitan writer when he comes to be known by the metropolis is real. London life was too much with him, then and later. And exile was, in the crude American sense, a good career move, made by a man who was certainly always aware when others made such moves—sensing, without being censorious, how skillfully Goethe fell in love with the right woman at the right time, or how well Byron had done by “shaking the dust of England off his shoes.” Max’s little day was passing, and couldn’t be lengthened by repetition. The smart thing was to subside, and once again become a rumor, an elegant whisper, as he had first been at Oxford.

He had remarkably good relations with the next great group of English aesthete intellectuals, the Bloomsburies. He took up the cause and the case of Lytton Strachey early and passionately; “Eminent Victorians” was a book that Max could have written, and that shows his hidden-dagger hand in every sentence. He became friends with Virginia Woolf. Yet Woolf felt that she belonged to a different literary era, even though Max was hardly older than Keynes. The aftershock of the Wilde trials had long passed, but Wilde himself looked embarrassing, vulgar; and the winds from France, which Max had on the whole resisted, swept away the posh, playful green-carnation aesthetes. Max was a modern writer, but he could never be a modernist, even in spirit. The modernists accepted a higher degree of difficulty. He believed in ease.

Throughout the later years, he seemed bent, sporadically, on trying his hand at a masterpiece, at some larger work. There is talk of a long, Proustian-seeming narrative, to be called “The Mirror of the Past,” and a couple of Rapallo-era pieces are beautiful exercises in memory: “William and Mary,” a story of an Ibsen- and Morris-loving couple, and “The Golden Drugget,” a meditative essay recalling Max’s first years in Italy. But they remain tantalizing fragments, unrealized in any longer form. Beerbohm himself shrugged off the demands that he try something big. “Some people are born to lift heavy weights,” he said. “Some are born to juggle golden balls.” Auden, with more irascibility than one might expect, pointed out that this was a poisonous doctrine—the idea is to juggle golden balls that weigh something. Declining to try, out of a false sense of decorum, was, in Auden’s view, the real English vice.

Proust had to outgrow the habits of diminutiveness, without sacrificing a love of nuance and detail, to become himself. He confronted his own Jewishness, and his own homosexuality, with both decorum and candor. Yes, of course, nobody’s Proust, but those of us who admire Beerbohm see that something Proustian was not entirely out of his reach as a writer, and wish that he had lived within a literary culture more inclined to make him try. One of the funny, but doom-laden, elements in “A Christmas Garland” is the mockery of Galsworthy’s and Bennett’s loquaciousness, which carried within it the seeds of the idea that anything big and long is blowsy.

Auden said that Beerbohm resembled Thurber, but he more resembles another Anglo-Jewish Anglophile, S. J. Perelman. A master parodist, a dandy, Perelman, too, produced a body of writing that is almost entirely a record of things read. Even when he travels, the subject is the sad discrepancy between what the books said it would be like and what it is like. Perelman himself turns sour over time, because he also exhausted his subject, the infatuations of early reading, without being able to replenish it with the stimulations of daily life. (Indeed, Perelman once listed, to a friend, three of his favorite books as Zola’s “Au Bonheur des Dames,” the Goncourt brothers’ journals, and “Seven Men.”) And, like Beerbohm, when the time came for him to write a big book, he couldn’t. It wasn’t so much that he couldn’t raise his game as that he couldn’t change it. Perelman’s tongue had got so used to a rococo elaboration that he couldn’t write straight even when he tried, a knuckleball pitcher—to borrow an image—whose arm is so twisted he can’t throw a fastball when he wants to. Beerbohm had found so many ways to be modest that when he had to try and be major he couldn’t.

Still, there is no such thing as a minor writer, because—there is really no such thing as a major writer. As Max wrote, considering Whistler, even Shakespeare occupies shockingly little of our attention—shocking, that is, for those of us who are trying to occupy it, too. (Boswell, one of Max’s favorites, said the same thing about Voltaire: no one had ever been more talked of, and look how little, really, Voltaire was talked of.) This means that bigness is a mirage, but it also means that smallness is a kind of illusion, too. Anyone who is read at all is more or less the same size. People who love reading will always love reading Max, because he mocked so wisely, and read so well. 
<-------->
The  heavily hyped appearance of Harper Lee’s new or very old, or, anyway, indistinctly dated, novel, “Go Set a Watchman” (HarperCollins), reflects an ambitious publishing venture—complete with slow, striptease-style press leaks and first chapters and excited pre-publication surmise—in which all the other apparatus of literature, reviewers included, is expected to serve, and has. Not since Hemingway’s estate sent down seemingly completed novels from on high, long after the author’s death, has a publisher gone about so coolly exploiting a much loved name with a product of such mysterious provenance. It may well be that what the procurers of the text have said about it—that it is an earlier novel set in the world of “To Kill a Mockingbird,” only recently discovered, and published with the author’s enthusiastic assent—is so. But, if it is, the procurers seem oddly reluctant to be terribly exact about their accomplishment. The finished book that has now emerged, with a charming retro cover, showing a lonely engine on a twilight Alabama evening, has not a single prefatory sentence to explain its pedigree or its history or the strange circumstance that seems to have brought it to print after all this time, as though complete novels with beloved characters suddenly appeared from aging and reclusive and apparently ailing writers every week of the year. (This in a book that includes a fourteen-line note on the type.) And then the story that has been offered about it in the papers—a story that seems to change significantly as time goes by—presents certain difficulties to the reader’s understanding of the book.

The excitement is, in a way, a salute to America’s literary memory: in what is supposed to be an amnesiac society, the memory of a fifty-five-year-old novel burns so bright that an auxiliary volume is still a national event. Of course, the memory is assisted by the universal appearance of “To Kill a Mockingbird” in eighth-grade curricula, but most of what appears in eighth-grade curricula vanishes quickly from memory—has basic biology or beginning algebra ever held our minds as Scout and Atticus have? The reason for that extraordinary hold is made plain, at least, by the incidental beauties of the newly discovered book, which are real. Though “Watchman” is a failure as a novel (if “Mockingbird” did not exist, this book would never have been published, not now, as it was not then), it is still testimony to how appealing a writer Harper Lee can be. That appeal depends, as with certain other books of the time—“The Catcher in the Rye,” “A Tree Grows in Brooklyn,” “A Death in the Family”—on the intensity of the evocation of coming of age, and of the feel of streets and summers at that moment. Harper Lee did for Maycomb (her poeticized version of her home town, Monroeville, Alabama) what J. D. Salinger did for Central Park—made it a permanent amphitheatre of American adolescence. One realizes with a slight, shamed start that we would now condescend to this kind of effort as belonging merely to a Y.A., or young-adult, novel. It’s not that we don’t have books like it anymore; it’s that we segregate their shelving—both John Green and Judy Blume have kept alive the tender evocation of an adolescent world, but they have been relegated to a smaller, specialized niche.


Though the new book is, to be blunt, a string of clichés, some of them are clichés only because, in the half century since Lee’s generation introduced them, they’ve become clichés; taken on their own terms, they remain quite touching and beautiful. The evocation of Maycomb, with which the new book begins, and which recurs throughout its pages, is often magically alive. There is a little set piece about the arrival of a train at a flag stop that makes one feel nostalgic for one’s Southern childhood even if one never had a Southern childhood:

The countryside and the train had subsided to a gentle roll, and she could see nothing but pastureland and black cows from window to horizon. She wondered why she had never thought her country beautiful. . . . The train clacketed through pine forests and honked derisively at a gaily-painted bell funneled museum piece sidetracked in a clearing. It bore the sign of a lumber concern, and the Crescent Limited could have swallowed it whole with room to spare. Greenville, Evergreen, Maycomb Junction.
She had told the conductor not to forget to let her off, and because the conductor was an elderly man, she anticipated his joke. . . . Trains changed; conductors never did. Being funny at flag stops with young ladies was a mark of the profession, and Atticus, who could predict the actions of every conductor from New Orleans to Cincinnati, would be awaiting accordingly not six steps away from her point of debarkation.
The tone is right and lovely, and is just as right and lovely in other pastoral pieces, in the later pages (though almost exclusively flashbacks), about games played with the heroine’s brother, Jem, and the Truman Capote character, Dill. The other, less potent clichés are either the stage-dramatic clichés of the fifties—the kind of dramaturgy you find in an Elia Kazan movie, with neat “reveals” and passionate scenes in which people driven to a climax of anger suddenly tell one another long-buried secrets—or, more drearily, the clichéd rationales that liberal Southerners used for years to justify a social order that they knew to be unjust.


The story related is simple, and suspiciously self-referential—it’s difficult to credit that a first novel would so blithely assume so much familiarity with a cast of characters never before encountered. Scout, the child heroine of “Mockingbird,” now mostly called by her proper name of Jean Louise, comes back to Maycomb from New York, where she is engaged in some undefined career-girl enterprise. For most of the first few chapters, she is fending off her lowborn but well-meaning suitor, Henry Clinton, and fighting with her tight-assed and conventional aunt Alexandra.

Cartoon
“This is the Internet wing.”
BUY THE PRINT »
Jean Louise then discovers that her father, Atticus, her hero and as close to a perfectly honorable man as she can imagine—“Integrity, humor, and patience were the three words for Atticus Finch”—has joined one of the marginally respectable Citizens’ Councils, a kind of less covert version of the Klan. Shocked, she confronts him, and starts on a series of static and prosy debates—first with her uncle Jack (a “character” who combines odd scraps of nineteenth-century English literary and religious knowledge with a bachelor doctor’s existence) and then with Atticus himself—about integration, the N.A.A.C.P., the Tenth Amendment, and other fifties-era subjects, all offered mechanically as set pieces, accented with oaths and “Honey, use your head!”s to make them sound a little more like dialogue. When the action moves to these abstract arguments about civil rights, the book falls apart as art—partly because today it is impossible to find the anti-civil-rights arguments anything but creepy, but more because any novel that depends for its action on prosy debates about contemporary politics will fail. A screenplay-style reversal (Uncle Jack, it turns out, was in love with Jean Louise’s sainted, long-dead mother all along!) jolts the action toward the end, and then, as in another kind of fifties movie, Jean Louise is urged to Come Home to Make Things Better—you can’t create a new South by hanging up there among the Yankees!—and it seems that she does.

That Southernness, however much it is now the material of cliché, is still the most pleasing thing about the book—the kind of easy, Agee and McCullers Southernness (as against Tennessee Williams’s more Gothic version) that was as much a part of the postwar American novel as Jewishness, of which it was the alternative construction. Jews (in Bellow, Malamud, early Roth) were urban, worried, and compellingly neurotic; Southerners (in Capote, McCullers, Harper Lee) were rural, carefree, and absolutely crazy. As always with such things, neither construction makes sense unless you see the missing central panel that both are reacting to: the Wasp ascendancy, only just about to be called so—that average American whiteness from which Southern drinking and Jewish schmalz alike could seem welcome refuges.


The element that intrudes for us now on these Southern vistas—where are all the black people, and why are the few we see treated the way they are?—was certainly a vital part of the story, but it was only a part of the story. Reëxperiencing this kind of Southern eccentricity is to be reminded how persuasive and touching it could be. The Southern Pastoral, in which the children play barefoot in the pastures and the summer is always called summertime, remains one of our strongest forms:

Calpurnia had placed three tumblers and a big pitcher full of lemonade inside the door on the back porch, an arrangement to ensure their staying in the shade for at least five minutes. Lemonade in the middle of the morning was a daily occurrence in the summertime. They downed three glasses apiece and found the remainder of the morning lying emptily before them.
“Want to go out in Dobbs Pasture?” asked Dill.
No.
“How about let’s make a kite?” she said. “We can get some flour from Calpurnia . . .”
“Can’t fly a kite in the summertime,” said Jem. “There’s not a breath of air blowing.”
The thermometer on the back porch stood at ninety-two, the carhouse shimmered faintly in the distance, and the giant chinaberry trees were deadly still.
“I know what,” said Dill. “Let’s have a revival.”
The question is how to preserve this pastoral idyll, and that’s where things get sticky, in several senses. The view that both Uncle Jack and Atticus offer, and which Jean Louise/Lee doesn’t endorse but does take seriously, is that this superior civilization of the agrarian South is being stampeded by Yankee industrialists and the N.A.A.C.P.—all the Outside Agitators—into undue disorder. (One night, Henry Clinton and Jean Louise see a group of blacks driving a fast car, and Henry implies that this is what happens when you let the Supreme Court tell the South what to do.)

So the idea that Atticus, in this book, “becomes” the bigot he was not in “Mockingbird” entirely misses Harper Lee’s point—that this is exactly the kind of bigot that Atticus has been all along. The particular kind of racial rhetoric that Atticus embraces (and that he and Jean Louise are careful to distinguish from low-rent, white-trash bigotry) is a complex and, in its own estimation, “liberal” ideology: there is no contradiction between Atticus defending an innocent black man accused of rape in “Mockingbird” and Atticus mistrusting civil rights twenty years later. Both are part of a paternal effort to help a minority that, in this view, cannot yet entirely help itself.

Atticus is simply being faithful to one set of high ideals in the South of his time. “Jean Louise,” Atticus says in the midst of their argument, “have you ever considered that you can’t have a set of backward people living among people advanced in one kind of civilization and have a social Arcadia?” Not long afterward, he adds, “Jefferson believed full citizenship was a privilege to be earned by each man, that it was not something given lightly nor to be taken lightly. A man couldn’t vote simply because he was a man, in Jefferson’s eyes. He had to be a responsible man.” Blacks are fully human, Atticus allows (nice of him), just not yet ready to vote. Elsewhere, Uncle Jack explains to Jean Louise the metaphysics of the Civil War, which supposedly had nothing to do with slavery or emancipation: didn’t she know “that this territory was a separate nation? No matter what its political bonds, a nation with its own people, existing within a nation? A society highly paradoxical, with alarming inequities, but with the private honor of thousands of persons winking like lightning bugs through the night? . . . They fought to preserve their identity. Their political identity, their personal identity.”

These were the ideas of the Southern Agrarians—that extraordinarily accomplished and influential set of writers and critics who embraced the modernism of Yeats and Pound and Eliot, exactly because it seemed to them a protest against modernization of all types, while they dreamed of a reformed “organic” society in the South, with that “identity,” that cult of “private honor,” still accessible. (It is good to be reminded of a time when “identity politics” belonged to the right.) Writers like Allen Tate, John Crowe Ransom, and Robert Penn Warren—though deluded about the concentration-camp society of the prewar South as it really was—had a worked-out and potent ideology, and one readily assented to by people who should have known better. Had they foundered, as their white-supremacist successors do today, in cheap nostalgia, they would not have had much influence, but they made a brilliant marriage with modernism, and that gave them immense academic authority at a time when little magazines moved big boulders, or seemed to. Theirs was among the most powerful intellectual movements of the thirties and forties, along with, predictably, the companion Jewish one in New York.

And so beneath Atticus’s style of enlightenment is a kind of bigotry that could not recognize itself as such at the time. The historical and human fallacies of the Agrarian ideology hardly need to be rehearsed now, but it should be said that these views were not regarded as ridiculous by intellectuals at the time. Indeed, Jean Louise/Lee herself, though passionately opposed to what her uncle and her father are saying, nevertheless accepts the general terms of the debate as the right ones. Asked her response to “the Supreme Court decision”—one assumes that Brown v. Board is meant—she says that she thought, and still thinks, “Well sir, there they were, tellin’ us what to do again.”

And it should also be said, out of human sympathy, that to demand that people reject their traditions and their understanding, however misconceived, of their own history—to insist that the Atticuses of this world go to reëducation camp—is foolish. The problem is not people who think wrong thoughts, since we all think what will, retrospectively, turn out to be wrong thoughts about something or other. The problem is people who give their implicit endorsement to violence or intolerance in the pursuit of wrong thoughts. And, as far as one can tell, Harper Lee never intends Atticus to be taken for that sort. Atticus’s central commitment is to the law, and that commitment is never questioned. We are meant to see Atticus as someone with skewed convictions about Jefferson, but not as someone who would participate in a cross burning or in fire-hosing protesters.

The Southern Agrarians—it was part of their complexity—didn’t see themselves as racists; quite the opposite. (Robert Penn Warren certainly more than made his peace with the civil-rights movement.) They saw themselves merely as cautious and watchfully conservative about the pace of change. That the pace of change had to be accelerated because it had been held back by terror for so long was not a truth that they wished to be told, or to see. Atticus’s attitude seems entirely authentic, his heroism and his prejudices, as so often with actual human beings, part of the same package. Credibility is the ethic of fiction, and he is a credible character.

In any case, as with most social upheavals that are allowed to work their way through a society, the things that Atticus and the Agrarians feared may have happened, but they didn’t happen because of the Supreme Court. Atticus and his friends vastly overestimated the power of liberal ideology and badly underestimated the power of their other enemy, capitalist commerce. The Monroeville Walmart Supercenter has doubtless altered Monroeville more than all the fiendish Yankee conspiracies to undermine the Tenth Amendment. Certainly the supremacists’ hysterical fears of anarchy, as much as the fondest hopes of civil-rights utopians, have been left unrealized. Hysteria about change is rarely earned by the change when it comes.


Yet here is where the questions of the book’s provenance begin to arise, and they, too, get a little sticky. The emotional force of “Watchman” depends entirely on the reader’s sharing Scout’s shock at the revelation of Atticus’s new friends and new affiliation, and, since Atticus is scarcely dramatized at all before his fall from grace, the reader already suspicious about the pedigree and the background of the book becomes doubly so. If you don’t know Atticus as a hero—and in this book you really don’t, except by assertion—why would you care that he seems to defect to villainy, however well he defends it? Taken as a composite from both books, Atticus may be a credible hero, but you have to read both books to know that. The charm of the flashbacks that ornament “Watchman” is real for those who know Jem and Dill and Cal from “Mockingbird”—but what effect could Lee have expected them to have on readers who don’t? Indeed, the book as a book barely makes sense if you don’t know “Mockingbird.” If “Watchman” is a first novel, even in draft, it is unlike any first novel this reader is aware of: very short on the kind of autobiographical single-mindedness that first novels usually present, and which “Mockingbird” is filled with, and very long on the kind of discursive matter that novelists will take up when their opinions begin to count.

It is, I suppose, possible that Lee wrote it as we have it, and that her ingenious editor, setting an all-time record for editorial ingenuity, saw in a few paragraphs referring to the trial of a young black man the material for a masterpiece. But it would not be surprising if this novel turns out to be a revised version of an early draft, returned to later, with an eye to writing the “race novel” that elsewhere Harper Lee has mentioned as an ambition. (The manuscript might then have been put aside by the author as undramatic and too abstract.) It is sad, though, to think that the preoccupations of this book, however much they may intersect our own preoccupations of the moment, might eclipse her greater poetic talents, evident here, and so beautifully fulfilled in “To Kill a Mockingbird.” There is a genuine dramatic climax, worthy of the writer’s gifts, offered and then evaded in “Watchman.” In the book’s toughest scene, Scout goes to visit Calpurnia, the black woman who brought her and Jem up, with infinite-seeming love, after Atticus agreed to defend Cal’s grandson from a charge of manslaughter. Scout is heartbroken to find that her beloved mother figure is cautiously distanced from her:

“Cal,” she cried, “Cal, Cal, Cal, what are you doing to me? What’s the matter? I’m your baby, have you forgotten me? Why are you shutting me out? What are you doing to me?”
Calpurnia lifted her hands and brought them down softly on the arms of the rocker. Her face was a million tiny wrinkles, and her eyes were dim behind thick lenses.
“What are you all doing to us?” she said.
Then Scout asks, “Did you hate us?,” and Calpurnia shakes her head no. This is credible. But the scene, and the book, would have been stronger if she hadn’t.
<-------->
Much of “The Tale of Genji,” the eleventh-century Japanese masterpiece often called the world’s first novel, is about the art of seduction. Not that any sexual act is ever mentioned; very little in Murasaki Shikibu’s prose is plainly stated. Things are suggested, alluded to, often nebulously. What counts in the seduction scenes is the art, the poetry. Quite literally so: the proper approach to a desired lady was through poems, written on scented paper of the finest quality, delivered by an elegantly dressed go-between of appropriate social rank. More poems would be exchanged as soon as the approach bore fruit. A “morning after” poem was an essential part of etiquette.

One reason that physical contact between men and women is hardly ever described in “Genji” is that courtly lovers almost never saw one another clearly, and certainly not naked; full nudity is rare even in traditional Japanese erotic art. Women of the upper class sat hidden in murky rooms, behind curtains, screens, and sliding doors. For a respectable woman to be seen in daylight, especially standing up, instead of reclining in an interior, under many layers of clothing, would have been provocative beyond belief. Women were shielded by curtains even when they spoke to male members of their own family. A male suitor could be driven wild by the sight of a woman’s sleeve spilling out from underneath a shade, or by the mere sound of silk rustling behind a lacquer screen.


Despite all these obstacles, people must have managed somehow. Indeed, “The Tale of Genji”—now available in a new translation by Dennis Washburn (Norton)—makes clear that the noble gentlemen and ladies in the Heian period (794-1185) were often remarkably promiscuous. Highborn men, like the fictional Prince Genji, the priapic hero of Murasaki’s episodic tale, were expected to have several wives and many concubines. Genji, also known as the Shining Prince, marries his first wife when he is twelve, immediately following his coming-of-age ceremony. Casual affairs with court attendants and ladies-in-waiting were one of the perks of an aristocrat’s life. So were other, more discreet forms of adultery. Genji, when still very young, has a passionate affair with his father’s mistress. Much later, Genji’s son, the high-minded Yugiri, grows infatuated with one of Genji’s wives. And both father and son lust after Tamakazura, a young girl whom Genji has adopted as his daughter.

Little wonder that even emperors were not always sure who their real fathers were. This was a particular sore point in the militantly imperialist nineteen-thirties, when the novelist Junichiro Tanizaki wrote a modern Japanese translation of “Genji.” As a result, he excised references to an emperor who was thought to be in the direct imperial bloodline but was actually the product of Genji’s illicit affair with his father’s mistress.

The main thing required of a noble gentleman was a sense of style. Seducing another man’s wife could be forgiven; a bad poem, clumsy handwriting, or the wrong perfume could not. Ivan Morris, the great scholar of Japanese culture, wrote in his book “The World of the Shining Prince” that, despite the influence of Buddhism, “Heian society was on the whole governed by style rather than by any moral principles, and good looks tended to take the place of virtue.”

I’m not sure this is exactly right. Because of the Buddhist belief in rebirth, beauty, in all its forms, was seen as a sign of virtue in a former existence. To have lovely handwriting, or a talent for poetry, was a mark of good character, in a former life as well as in the present one. A priest in “Genji” describes a young woman as follows: “She really is quite beautiful, isn’t she! No doubt she was born with such features as a reward for good deeds performed in a previous life.” Prince Genji himself is described as cutting “such an attractive figure that the other men felt a desire to see him as a woman. He was so beautiful that pairing him with the very finest of the ladies at the court would fail to do him justice.”

It was, as all this suggests, a rather effete culture. The aristocratic ideal of male beauty—highly perfumed, moon-faced, smooth-skinned, extravagantly dressed—was close to the feminine ideal. A distinct air of decadence during the peak of the Heian period also suggests the approaching end of a regime, a world, in Genji’s words, “where everything seems to be in a state of decline.”

Less than two hundred years later, the self-obsessed nobility of the Heian court, distracted by the rituals and refinements of palace politics, oblivious of the world outside the capital, and mostly bored out of their minds, were overwhelmed by more vigorous provincial clans, notably the samurai, with their warrior codes and martial ideals. But in Genji’s time, the early eleventh century, the imperial capital (today’s Kyoto) still held sway; anyone unlucky enough to live in the provinces was considered too uncouth to be taken seriously.


The sense of style was intimately linked to a sense of hierarchy. Everything, from the right of way on a public road to the permissible colors of one’s garments or the chance to be a nobleman’s wife (instead of a mere concubine), was subject to one’s place in the pecking order. And this, too, was linked to karma: high rank was a virtue earned by good behavior in a previous life. In such ways do ruling classes justify their privilege.

The politics that permeate the novel show that sexual relations were not just part of an elaborate libertine game but also a matter of ruthless practical strategy. Heian politics were really marriage politics. Formally, Japan was ruled by emperors, but real power was exercised, in large part, behind the scenes by the Fujiwara clan. This depended on daughters of the Fujiwara family marrying imperial princes, some of whom would one day be emperors. The Fujiwaras were thus able to exert control over the throne and to rule the country, or at least those parts of the country within reach of the imperial capital.


Lady Murasaki—not her real name; her sobriquet was the name of Genji’s great love—was born into a minor branch of the Fujiwara clan. Her father was a provincial governor, who, unusually for the time, passed on his deep knowledge of Chinese literature to his bookish daughter. Normally, only men wrote in Chinese, as a sign of superior status, while women confined themselves to Japanese. This explains why the first writers of literary prose in Japanese were highborn women, as were their readers. The famous “Pillow Book,” a collection of musings by the court lady Sei Shonagon, was written at more or less the same time as “The Tale of Genji.”

Not much is known about Murasaki’s life. Her father’s position was neither grand nor secure enough to put her into the highest circles. She married late; her husband was a much older man, and Murasaki was probably not his most privileged wife. The story goes that she began writing her novel after he died. Although her middling rank would have excluded her from court circles, her literary reputation gave her an entrée into the empress’s salon, where she often felt out of place. A sense of being on the fringes of society, as has been the case with so many writers since, sharpened her observations. Murasaki watched the sexual maneuverings, the social plots, the marital politics, the swirl of slights and flatteries that went on around her, with the keen, sometimes sardonic, and always worldly eyes of a medieval Jane Austen. Her Buddhist view of life’s fleeting nature and the vanity of human affairs added a dash of melancholy to her ornate aristocratic prose.

“The Tale of Genji” is a very long book, more than thirteen hundred pages in its new English translation, made up of fifty-four loosely connected chapters that span the stories of four generations. Since the literary quality of “Genji” is uneven, its authorship has been contested. Some scholars believe that the book was finished by someone else after Murasaki’s death. At least one recent translator, Royall Tyler, thinks that evidence of sole authorship is shaky. Others, including Dennis Washburn, the latest translator and a professor of Asian literature at Dartmouth, are more persuaded that the book had to have been the work of one person. Despite its great length, “Genji” has a unity of style and sensibility that seems to support this conclusion.

The original manuscript no longer exists. Fragments of text survive in a twelfth-century illustrated scroll, but modern editions of the book are based on a thirteenth-century compilation made by a poet called Fujiwara no Teika. More than ten thousand books are said to have been written about “Genji,” as well as countless scholarly essays, commentaries, and monographs. There have been conflicting schools of thought about “Genji” at least since the twelfth century. Different versions of the book were passed down in certain noble clans like secret family treasures.

Cartoon
BUY THE PRINT »

The chief difficulty in translating “Genji,” into modern Japanese almost as much as into English, is the extreme elusiveness of Heian-period court Japanese—not just the language itself but also the many references and allusions. Every page is sprinkled with poems or phrases pointing to Chinese and Japanese literary sources that an eleventh-century aesthete might have been proud to notice but are lost on most Japanese today, let alone the reader of an English translation. Another problem lies in the character names. Since it was thought to be rude to call people by their birth names, most of the people in “Genji” are identified only by rank. A common solution in translations is to use nicknames derived from poems the characters compose or from their physical surroundings or qualities: Lady Rokujo lived in a mansion on Rokujo, or Sixth Avenue; Lady Fujitsubo lived in the Fujitsubo, or Wisteria Pavilion. Genji’s grandson, Niou, a devastatingly handsome womanizer, is known as the Perfumed Prince, because of his exquisite smell (niou, in Japanese).

A literal translation of “Genji” would be unreadable. And the vagueness, so poetic in Japanese, would simply be unintelligible to the Western reader. The trick is to retain the flavor of Murasaki’s lyrical style while transmitting, with some degree of precision, what she meant to say. Since we often don’t really know what she meant, much has to be left to guesswork and interpretation.

The two most famous English translations of “Genji”—Arthur Waley’s, in the nineteen-twenties and thirties, and Edward Seidensticker’s, in 1976—could hardly be more different. Waley regarded gorgeous prose as more important than accuracy. When he found a passage, or even a whole chapter, too boring or obscure, he just skipped it. He compensated for the vagueness of the original Japanese by making up something equally lyrical in Bloomsbury-period English.

Seidensticker, in “Genji Days,” the diary he kept while translating the book, admits that Waley might have been right to cut certain passages but resolved not to do so himself. And, no doubt as a reaction to Waley’s flowery prose, he stripped away a lot of the ornament to arrive at a more modern text that conveys its meaning with far greater accuracy and concision. But, as a result, the beauty of Murasaki’s long, flowing sentences is lost. Royall Tyler, in 2001, tried to strike a happy medium. Washburn is so eager to throw light on even the murkiest bits that he makes absolutely explicit what is only hinted at in the original.

A few samples reveal the differences. In Chapter 4, titled “Yugao,” Genji comes across a run-down house, the abode of a young woman he is about to seduce. Waley describes the entrance like this: “There was a wattled fence over which some ivy-like creeper spread its cool green leaves, and among the leaves were white flowers with petals half-unfolded like the lips of people smiling at their own thoughts.” Seidensticker: “A pleasantly green vine was climbing a board wall. The white flowers, he thought, had a rather self-satisfied look about them.” Tyler: “A bright green vine, its white flowers smiling to themselves, was clambering merrily over what looked like a board fence.” Washburn: “A pleasant-looking green vine was creeping luxuriantly up a horizontal trellis, which resembled a board fence. White flowers were blooming on the vine, looking extremely self-satisfied and apparently without a care in the world.”

One can see why many admirers of “Genji” prefer Waley. Seidensticker can sound too cut-and-dried, while Washburn errs on the side of wordiness. Indeed, Tanizaki stated that he was inspired by Waley’s translation to write his own modern Japanese version. Quite why Washburn decided to follow up Tyler’s fine translation so soon with a new rendition in English is slightly mysterious. Perhaps it’s some version of the explanation that George Leigh Mallory gave for climbing Everest: just because it’s there. Washburn writes, “I have undertaken this work precisely because there can be no such thing as a definitive translation.” That’s certainly true.

One reason that Murasaki’s text appears to run on so easily is that she seamlessly combines poetry with exposition, interior monologues, and her own authorial asides. This, too, makes translation more difficult. It isn’t always clear who is speaking at any given time. Washburn separates the poems clearly from the prose—something Waley did not do—and puts the interior thoughts in italics, instead of injecting words like “he thought” or “she wondered.” It provides clarity, but at the expense of interrupting the flow.

Washburn’s efforts at clarity can sometimes be jarring, too, especially in passages having to do with sexual attraction and seduction. The great love of Genji’s life is Murasaki, the woman whose name was later conferred on the author. Genji discovers her one night when she is still a girl of about ten, cared for by her grandmother. As is so frequently the case in this shadowy world, Genji peeps at her through a fence (peeping is a common image in Japanese erotic prints, too) and is so taken by the child’s beauty that he imagines what she will be like when she grows up. She reminds him of Fujitsubo, his lover who was also his father’s mistress. (“Genji” is full of such echoes.) He ends up more or less kidnapping Murasaki and sets her up as a wife-to-be in a private residence, where she plays with her dolls, even as the Shining Prince treats her with a rather scandalous degree of intimacy. It sometimes puzzled her female attendants, in Seidensticker’s crisp words, “that she should still be such a child. It did not occur to them that she was in fact not yet a wife.” Washburn renders the same passage as follows: “The people who served at Genji’s mansion had found her childish behavior, which could be quite pronounced at times, awkward and inappropriate, and yet they had no idea that she was in fact a wife in name only, for Genji had not yet had sex with her even though they slept together.”


This certainly spells things out. But it sounds clunky, not to say anachronistic. There is more in this vein. Another woman—a little older—complains of Genji’s “lewd persistence.” This woman—who is commonly assumed to be Genji’s own daughter, but who was in fact born to a mistress whom Genji shared with his best friend and rival—is pursued by a man who, in Washburn’s rendering, “had initiated sexual intimacies with her.” As a result, Genji, to his deep regret, cannot stop her from marrying. Seidensticker’s Genji is just “sorry that she had done as she had.” And in Tyler’s version Genji is “annoyed and disappointed, but it was too late now.”

Perhaps Washburn comes closest to what is really meant. Yet Seidensticker and Tyler seem to be closer in spirit to the original text. Washburn’s scholarship is certainly profound, and he tells us a great deal more in his footnotes than Seidensticker does. But in the actual text he explains a little too much. And despite his claims that he tried to “replicate the general rhythms” of Murasaki’s prose style, too many words and phrases, such as “mind-set,” “scenarios,” and “enough already,” take us too far away from Genji’s time.

To be sure, any translation reflects the time in which it was produced, in Washburn’s case no less than in Waley’s or, indeed, Tanizaki’s. The same is true of textual interpretations. Washburn elucidates this very well. Scholars of the more austere Kamakura period (1185-1333) took a moralistic view of Heian court culture, and the work that most famously represented it. “Genji” was read as a moral tale about the ephemeral nature and vanity of secular life, and this Buddhist theme runs through the book. Wounded by their love affairs, a number of women shave their heads and become nuns. Genji himself often expresses the desire—without, it must be said, much conviction—to retire into a life of religious contemplation. But his worldly entanglements are always much too tempting to give up. That Murasaki made them seem so alluring was regarded as rather scandalous by the time samurai values and Confucian morality had set in. Washburn points out that aristocratic women in the twelfth century would sometimes pray for the author’s soul. This attitude persisted for a long time.

Ivan Morris writes that Japanese scholars of the old school “have tended to disapprove of the Heian period as being one of license and immorality, much as the Victorians looked askance at the bawdiness of the Elizabethan age.” And yet “Genji” also quickly became one of the great icons of Japanese culture, and, in modern times, a source of patriotic pride.

In the Edo period (1603-1868), when the pleasure-seeking merchant class influenced culture as much as the more ascetic samurai, “Genji” was indispensable to a well-bred person’s education, along with the tea ceremony and flower-arranging. Washburn tells us that young ladies would receive illustrated volumes of “Genji” as part of their dowry. The attraction of the Heian masterpiece for Edo-period merchants lay partly in its aristocratic style, much admired by the newly rich.

But it was the same highbrow classical status of the work that tempted Edo fiction writers to mine it for parody. The most famous version, written in the nineteenth century, by Ryutei Tanehiko, was titled “A Country Genji by a Fake Murasaki.” Based only very loosely on the original tale, this parody inspired print artists to make erotic pictures, or shunga. One such series, by Utagawa Kunisada, done in the eighteen-thirties, shows, in great detail, the various “Genji” characters in flagrante delicto; it’s called “Amorous Murasaki Finds Pleasure in Fifty and More Chapters.” More than a century before that, the artist Hishikawa Moronobu produced an equally provocative depiction of the tale, titled “Genji’s Scented Pillow,” with the lovers in Heian robes and a text plucked from the original work.

Cartoon
“What’s that, boy? You’re too tired for a walk and you just want to watch TV?”
BUY THE PRINT »
It was also the custom in the Edo period for prostitutes to name themselves after famous lovers of Genji, a bit as though eighteenth-century English whores had sported Shakespearean names like Juliet or Desdemona. Like their merchant clients, courtesans liked to put on high-class airs while perhaps making fun of them at the same time.

All this is a long way from Lady Murasaki’s courtly world; it is even more remote from the culture of contemporary Japan, to say nothing of the West. And yet “Genji” still speaks to us, even if we have no special interest in Japanese history. This is because beneath the surface of a distant and often distinctly strange culture Murasaki’s characters of a thousand years ago express emotions that remain entirely familiar.

In the polygamous society of the eleventh century, where women were made to accept rivals for their husbands’ affection, feelings were no less vulnerable than they are today. Beautiful young women, invited by gallants like Genji to live as concubines under their luxurious protection, are often fearful of the consequences. What will the other women think of them? Won’t the newcomer suffer from their jealousy, especially when she is younger and prettier than they? Such apprehensions were often fully justified.

Tanizaki wrote that Japanese writers first learned about romantic love from European literature. Before that, all they understood was sexual attraction. The cynical tone of seventeenth- and eighteenth-century Japanese fiction set in the pleasure districts of Kyoto and Edo suggests that Tanizaki may have had a point. And the idea of eleventh-century men expressing their undying love to women they have barely seen might seem strange to us. Nonetheless, “Genji” reveals sentiments that are more complicated, and still easy to recognize.

Genji is a typical Don Juan, especially in his younger years. He can’t resist a challenge. A woman who decides to become a priestess is particularly alluring, as is a girl who is destined to marry the emperor (prompting an adventure that almost leads to his downfall). The great seducer is as vain as a diva. Murasaki pays far more attention to her hero’s physical beauty than to that of his female conquests, who are more often praised for their gentle manners or prowess in calligraphy. Genji can also be perverse. In one famous scene, called “Fireflies,” he arranges, for his own amusement, an amorous encounter between his half-brother and the young girl Genji desires for himself while raising her as his daughter. The petrified girl, not wishing to be seen, quickly retreats behind a curtain, whereupon Genji humiliates her by releasing fireflies from a bag, lighting up her face for all to see.


Murasaki is too accomplished a writer to present her characters as either wholly good or bad; like most human beings, they can be both. Genji loves all his women in his own way. Unlike many Don Juans, he is loyal, too, after a fashion: he takes care of the women even after they no longer hold romantic interest for him. None of this, however, contradicts Tanizaki’s theory about premodern love in Japan. What does is Genji’s relationship with Murasaki, which deepens with time. He is devastated by her early death, described in one of the book’s most moving scenes. Suddenly, life is no longer a game.

In some respects, Genji calls to mind one of those eighteenth-century aristocrats in Choderlos de Laclos’s “Dangerous Liaisons” who are tripped up by their emotions even as they believe they are in total control of their erotic games. Because her mother was of common birth, Murasaki cannot be Genji’s main wife. As a consort, her social position is vulnerable. Despite her jealousy, she puts up with Genji’s dutiful marriage to a young girl of superior rank as well as with all his infidelities.

Her sadness and Genji’s guilt are described beautifully. Ceremonial duty compels Genji to spend the first three nights with his new wife. He tells Murasaki, in Washburn’s translation:

“Tonight is the Third Night, so I must go. . . . It’s the one night you must forgive me for leaving you. I would hate myself if, after tonight, I should ever let anything disrupt our relationship again. . . .” Torn by conflicting emotions, he seemed to be in genuine pain. Murasaki gave a wan smile. “If your heart is indecisive, then how should I be expected to understand how to resolve your dilemma? I wonder which way you will go in the end?” Because it was no use speaking to her, Genji felt ashamed, and he stretched out facedown, his chin cupped in his hands.
When Genji cannot bring himself to leave her, Murasaki gently reminds him of his duties. She says, “Your hesitation will give others the wrong impression, and I’ll be the one who looks pathetic.” Mores have changed; human psychology much less so.

One of the oddities of “The Tale of Genji,” from a modern perspective, is that the two main characters, Genji and Murasaki, die halfway through the book. The main story running through the later chapters concerns two young men, whose pursuit of the same woman drives her almost to suicide. One is Niou, the Perfumed Prince, a rather shallow but seductive ladies’ man, who is Genji’s grandson. His friend, Kaoru, an equally fragrant but much more diffident figure, is ostensibly Genji’s son but is in fact the product of an encounter between another man and Genji’s youngest wife.

Again, Murasaki plays with echoes of previous affairs. Kaoru is the kind of prig who prides himself on his moral rectitude while hating his sexual timidity. He envies Niou his easy way with women, while Niou envies Kaoru’s intensity of feeling. Kaoru is tortured by his unrequited love for the daughter of his mentor, an elderly prince who has retreated to the countryside as a religious hermit. The woman dies young, and Kaoru, frantic with grief, thinks about cutting his ties with human society and following his mentor’s example. Like Genji, however, he is unable to take that step.

Then a beautiful half sister of Kaoru’s former love turns up: Ukifune. Kaoru falls in love with her, seeking a kind of reincarnation of his old passion. She doesn’t really love him but respects him as a noble, upstanding man who could offer her protection. One night, Niou, not knowing who she is but spying her through a half-folded curtain, tries to seduce her. Afraid of losing Ukifune, Kaoru bundles her off into hiding. Niou tracks her down, and succeeds this time by trickery. Ukifune’s passion is aroused, but she is terrified of losing the noble Kaoru. Death seems to be the only way out.

Her apparent death by drowning leaves both suitors in a flood of tears. (Men cry a lot in “Genji.”) But Ukifune doesn’t die. She is found by a group of pilgrims. Pretending to have forgotten her past, she takes her vows and turns her back on the fleeting world. In the last chapter, “Bridge of Dreams,” Kaoru, alerted to her reappearance, sends her beloved younger brother over with a message to persuade her to come back. She refuses. He suspects that another man might be hiding her.

This abrupt conclusion has prompted speculation that the book may not have been finished. Perhaps it wasn’t. Or perhaps the open ending suggests that life simply goes on, in endless cycles, as we keep struggling in vain to be free from the ephemeral pleasures and sorrows of our brief existence. The dreams alluded to in the title of the last chapter might refer to the art of fiction or to the illusory nature of human life. But “The Tale of Genji” lives on because of its author’s genius at making the illusion come fully alive. 
<-------->
It has been almost fourteen years since the September 11th attacks—longer than the Presidency of Franklin Roosevelt, longer than America’s war in Vietnam. The fallout has been an improbable and wrong-footed business from the start, unfolding in a series of improvisations and flukes, with actions or reactions that often seemed not just incommensurate with their consequences but utterly disconnected: nineteen hijackers commandeer four commercial airplanes; the United States drives the Taliban from Afghanistan; Osama bin Laden escapes to Pakistan; the Bush Administration invents a secret legal apparatus; the Taliban return; the U.S. invades Iraq, occupies it for eight years, then leaves; bin Laden is hunted down and killed while under the protection of a putative American ally; Arab states disintegrate; an obscure jihadist from Baghdad declares the restoration of the caliphate; the U.S. returns to Iraq. As narrative, the war on terror has been like the nouveau roman, with no coherent plot, only jarring disjunctions of cause and effect, time and place.

“This conflict was begun on the timing and terms of others,” President George W. Bush said at a prayer service at the National Cathedral, three days after the attacks. “It will end in a way and at an hour of our choosing.” But the conflict didn’t yield to anyone’s mastery. Bush never wanted it to end, and his successor couldn’t find the way or the hour. Twelve years later, speaking at the National Defense University, President Obama proposed a rhetorical exit: “We must define our effort not as a boundless ‘global war on terror’ but, rather, as a series of persistent, targeted efforts to dismantle specific networks of violent extremists.” But narrowing the focus from tactics to groups didn’t have the desired effect. Guantánamo remains open, drone strikes have increased, mass-casualty suicide bombings are routine in half a dozen countries, the fighting in Iraq and Syria has brutally escalated, videotaped beheadings are normal. Much as we want it to be over, the era won’t end.


John Sifton, a former researcher with Human Rights Watch, found the whole thing preposterous from the start. “It was difficult to believe it was all happening,” he writes in “Violence All Around” (Harvard). “This is not a real war, I thought. It is a big misunderstanding. Al-Qaeda—what I knew of it then, and what it emerged to be—a couple of hundred men. Something about the battle, about al-Qaeda’s naïve resentments and America’s naïve responses, seemed unhinged. There was no profundity in it, the affairs of the day seemed puerile, an agenda set by children—very dangerous children for sure, but children all the same.” The usual tone of human-rights advocacy is outrage, with little modulation regardless of the nature of the abuse. Sifton’s version is closer to disdain, but just as often he finds himself in a disturbing state of detachment. He feels detached wandering through the dust and ash of lower Manhattan on the day of the attacks; a few weeks later, in Kandahar, staring at the bombed-out compounds of Mullah Omar and Al Qaeda, he feels the same way.

Sifton is the grandson of the theologian Reinhold Niebuhr, and the son of a federal judge and a writer and editor of books. Inclination leads him to history, philosophy, and literature, not polemic. His job often took him to Afghanistan and had him digging into cases of aerial bombardment, drone strikes, rendition, and torture, but you get the feeling that he would rather have been digging up an Indo-European archeological site. Pondering the events of September 11th makes him think about Frederick Seidel’s “December,” a masterly, sinister poem written from the point of view of a terrorist. After nearly being shot in the head by a Taliban gunman, Sifton indulges in an extended meditation on the role of proximity in violence.

At an extreme, detachment flirts with philosophical pessimism and even cynicism—the sense that everything has always been thus and nothing we do can matter. Sifton is susceptible to it, and the long view can be a drag on the daily motivation of a human-rights worker trying to document abuses: “yet another set of violent episodes on the Asian steppe, villages being attacked in the same manner as they might have been two or three thousand years before.” But detachment is a welcome stance for a book about violence, especially violence in our time. After a decade and a half, we still have no distance from the war on terror—a phrase that made Sifton recoil when President Bush first used it, in the days after September 11th. The era has generated more shallow certitude than lasting insight, with most commentators too intent on justification or condemnation to explore the harder questions that the conflict raises. The instant wisdom that everything changed on 9/11 was later echoed by the assertion that nothing was ever the same after the Central Intelligence Agency waterboarded Abu Zubaydah. Does either claim hold up? How does the violence of recent years fit in the long history of political mayhem?


Sifton’s project at the outset is to see violence objectively, as a human phenomenon. One aspect is its sheer difficulty: killing other people is no easy business, and it’s hardest at close range, when you can look into the other’s face. Even if a killer is untroubled by conscience, the deed itself may put him in a state of physical exhaustion, as if it required a tremendous effort to overcome an instinctive aversion. “People are not wired for unfettered violence,” Sifton writes. For theoretical support he turns to “On Aggression,” by the Austrian ethologist Konrad Lorenz, who proposed that intraspecies violence, while innate in human beings and animals, is held in check by the impulse to submit or retreat.

Why, then, is there so much killing at close quarters, by machete, knife, or handgun? One answer Sifton proposes is a failure of empathy—or a misdirection of empathy, away from the other and toward one’s own kind, whether ethnic, national, religious, or political. If so, the task of the human-rights worker is not to argue from philosophical principles and international covenants. The way to prevent abuses is to create sympathy in the strong for the weak, and the way to do that is by telling stories—“long, sad, sentimental” ones, in the words of the philosopher Richard Rorty—through which the strong will begin to see the weak as fellow-creatures, worthy of protection and care. Sifton, the author of numerous Human Rights Watch reports, replete with tales of human suffering, acknowledges that, by this standard, his work is mostly a failure. (So, for that matter, is mine.)


But violence flows from more than the absence of empathy. In the worst cases, it’s a positive force, full of hatred, grievance, and a righteous sense of justice. Redirecting empathy in the perpetrators might be no easier than reasoning with them through principles and laws. This is why human-rights work can seem so futile, and why its champions have sometimes felt compelled to turn to violence as the only answer to worse violence. For a time, in the years between the end of the Cold War and the invasion of Iraq, humanitarian concern gave rise to the insight that, without the spectre of mutual annihilation by the superpowers, something vaguely known as “the international community” could stop atrocities and bring justice through the collective use of force. The idea was called humanitarian intervention.

As Sifton relates, there’s a long history behind this thinking. In its modern version, the insight that law and morality depend on violence is at least as old as Hobbes, and so is the argument that “massively destructive weapons could be humanitarian, if they were meant to end a war.” The producers of chemical weapons made that claim in the First World War, and President Truman used it to justify the use of nuclear weapons in the Second. Sifton is at his most subtle, and closest to the tragic realism of his grandfather, in showing that violence is inseparable from justice—that even Mahatma Gandhi and Martin Luther King, Jr., understood nonviolence not as pure pacifism but as a strategy that depended on the use of violence by others.

In the nineteen-nineties, war became a favored tool of people who cared passionately about human rights. Humanitarian intervention never happened in Rwanda, and it happened very late in Bosnia, but those cases only strengthened the arguments of its advocates. It then had some success in Kosovo, East Timor, and Sierra Leone. By the turn of the century, humanitarian intervention had become a foreign-policy doctrine. Then came 9/11. Human rights appeared somewhere down the list of the Bush Administration’s reasons for undertaking regime change in Iraq. From a humanitarian perspective, though, the potential overthrow of Saddam Hussein was attractive to many people, including me. At Human Rights Watch, Sifton tells us, there was considerable discussion and disagreement. The group never took a position on the war, and most of its staff members were opposed, but those who worked on Kurdish and Iranian issues pointed out the potential benefits. Sifton, having seen at first hand the instability of post-Taliban Afghanistan, was prescient enough to imagine even worse in Iraq, and he opposed the war. He is also honest enough to acknowledge that the manipulation of human rights as a rationale helped get the U.S. into the mess.


Perhaps as a result, Sifton has a humble idea of his role. He doesn’t chafe at the essential impotence of human-rights work—its inability to do anything to stop the world’s monsters beyond the hope that some quiver of embarrassment might cause one to treat human beings with a little less cruelty. “The alternative seems worrisome: the idea that rights groups might regularly do more,” he admits. “If Human Rights Watch could summon giants easily to do violence against other giants in the name of human rights, we would be soliciting violence every day, from one end of the globe to the other, from Tripoli to Katmandu.”

It’s wisdom, but not the satisfying kind. It offers little comfort to the afflicted. What seemed like clear morality two decades ago has gone completely dark. Intervention in Libya created a failed state, a base for jihadists, and more killing. Non-intervention in Syria allowed for a failed state, a base for jihadists, and massive killing. No one should be sleeping well.

One striking feature of violence in the age of terror is its anonymity. The hijackers couldn’t see the faces of the workers in the Twin Towers. American pilots over Kandahar didn’t know whether children were present in the compound they were about to destroy. The goal of the suicide bomber in the Baghdad market was to kill as many people as possible. The drone operator in Nevada pushed the button based on a video feed of supposedly suspicious activity by passengers in a vehicle. Advances in weapons technology make violence easier by obviating the natural aversion to face-to-face killing, turning war into an automated activity and eliminating the mitigation that comes with our tendency toward submission and retreat. “On the one hand, we have the most intimate form of violence,” Sifton says of drone strikes, “while on the other hand, the least intimate of weapons.” But, judging by the number of drone operators who have been treated for alcoholism, depression, and other outcomes of post-traumatic stress, even this degree of remoteness can’t insulate the perpetrator from the effects of killing. “Modern killers and torturers suffer more than those of the past,” Sifton writes, “because of the larger discordance between our ordinary social lives and our violent activities.”

Sifton devotes a chapter to drones, with a short history of manned and unmanned air strikes. He describes the fallibility of drones (they’re only as good as the intelligence) and their appeal—the secrecy, the lack of physical risk, the low political cost. For American leaders, drones are an irresistible improvement over sending more ground forces to be shot at and blown up in Iraq and Afghanistan. Drones can extend targeted violence into countries with which the U.S. is not at war. If you’re making a case that violence changed after September 11th, drones are among your more persuasive exhibits.

Is this Sifton’s argument? “Violence All Around” doesn’t say so at the outset. The prefatory observations (“Violence is interesting. . . . Violence makes things happen. . . . We rarely look directly at it”) are so broad and speculative that Sifton’s canvas seems to be not just the past fourteen years but the whole human story. The best chapters come early in the book, and describe his experiences in Afghanistan before and after the fall of the Taliban. They move easily between travelogue and historical analogy, personal experience and philosophical meditation, discursion and sharp statement. Sifton is a fine essayist—he wears his reading, his war stories, and his moral indignation lightly. The pessimistic long view keeps threatening to take over, as on a hot afternoon in the much conquered Afghan city of Balkh, now a dusty outpost of neglect: “The sheer ancientness of the place struck me. The teahouse door slammed behind us after we left, causing pigeons in the square to take flight and scatter across the sky. The mountains lay in the distance, indifferent to the passing of time. The echo of the door slamming evoked pointlessness, a line of poetry: ‘all had been done, and long ago, that needed doing.’ ” I like the lack of urgency in passages like this. They defy the mood of most writing about the war on terror, as if to say, “Yes, things changed—they always do, but never as much as we think at first.”

Gradually, Sifton the essayist is eased aside by Sifton the human-rights worker. From drones he turns to torture, rendition, the dubious terminology of the military, the sheer quantity of official bullshit. These chapters cover ground that’s been exhaustively plowed by others. Sifton ascribes much of the era’s confusion and abuse to Bush’s folly of declaring war on a type of violence. The war on terror was both literal and metaphorical, mashing up words, ideas, and deeds, until “nothing was hard and fast, everything was up for grabs.”

This is why Sifton concludes that violence in our era is different: what’s changed is not the scale, not the type, but the idea of it. The war on terror turned a crime into a war. It risked eroding the institutions put in place after the catastrophe of the Second World War, thereby making it easier for those horrors to happen again. “The U.S. government, in responding to the September 11 attacks, was trying to change the very narrative of what violence was,” Sifton writes, using the kind of cliché he ordinarily avoids. “After the September 11 attacks, there was no solemn border anymore between the killings of war and the killings of terrorism and counterterrorism. Correspondingly, a blurring had occurred in the distinction between civilians and combatants. The idea of a war on terrorism had broken down the whole system. Killers, jailers, and victims had all been mixed up together into a new world of brutality, with no end in sight.”

The passage is as blurry with conflations and elisions as its subject. Sifton seems to be saying that the September 11th attackers used terror to try to start a war, the U.S. obliged them, and in these origins the idea of innocents ended, along with all the laws of war that protect them. But civilians have been the primary victims of war for a century. Terrorism and counterterrorism were integral elements of the French-Algerian war and other modern conflicts. The laws of war hardly ever matter to the combatants. What September 11th changed wasn’t the “narrative” of violence but the categories of war: for the first time, a great power went to war with a stateless, borderless group. Global jihadism was something new, and the U.S., faced with a hard strategic question, stepped into Al Qaeda’s trap. But in the course of Sifton’s paragraph the terrorists vanish. It’s the Americans who broke everything apart.

The same thing happens with the book. It begins with a three-hundred-and-sixty-degree view of violence, then slowly zeroes in on the C.I.A.’s torture, rendition, and drone programs. Al Qaeda pretty much disappears. The Taliban blur into the general Afghan population caught in the sights of a Predator drone. The Islamic State barely rates a mention. These are strange omissions, particularly the last, since the extreme violence of ISIS fits perfectly with one of Sifton’s themes—the theatricality of war—while challenging another, the idea that contemporary violence is inflicted at increasingly greater distances in order to make killing easier. Nothing is more intimate than beheading, and the images, spread through social media, give the most remote spectators the sense of being close to the most horrifying violence imaginable. For ISIS, secrecy and automation are not the point of killing. Its violence is personal, overtly cruel, and limitless, in a way that’s both ancient and completely contemporary. These are the very qualities that attract legions of recruits who want to cleanse the world and rebuild it on a mountain of corpses.

Sifton is fascinated by words and ideas, but not by those of Islamists. It’s as if they didn’t belong to the same world as we do. When he writes that “the idea of a war on terrorism had broken down the whole system,” he doesn’t credit the terrorists with figuring out how to do it first. Jean-Pierre Filiu, a leading French scholar of the Arab world, recently told me, “ISIS, and Al Qaeda before, is far more modern and adapted to this world than we are as institutions, and even in our way of reacting. They are totally the product of our modernity, which is something that we are afraid to accept. We are looking for a failure story, while from their point of view it is a success story.”


None of this seems to interest Sifton much. Perhaps the book’s autobiographical through line kept his writing close to his work with Human Rights Watch. But he was the organization’s senior researcher on terrorism as well as on counterterrorism. Judging from the emphasis in “Violence All Around,” he dedicated most of his effort to the American side of the conflict. The pragmatist in him feels compelled to justify this decision. In quantitative terms, the focus of his work was small: he acknowledges that the number of detainees in C.I.A. prisons was around a hundred. From the same realistic viewpoint, he questions the relevance of the legal issues: to a Pakistani family, it didn’t matter whether the Hellfire missile that killed their children was fired by a military aircraft or a C.I.A. drone. “In deciding core issues of human rights abuse,” Sifton writes, “the main focus is on the effects of the violence and the identities of the victims, not the identity of the perpetrators or the type of weapon used.”

By this standard, the war on terror has been far less of an American disaster for human rights than the Vietnam War or the Second World War. No U.S. atrocity in Iraq came close to the wholesale slaughter of My Lai, or of the many My Lais that never became bywords. Aerial bombing in Afghanistan has been minuscule and precise compared with the carpet bombing of North Vietnam, Japan, or Germany. Sifton has more than enough historical consciousness to ask himself why he spent these years “so intently focused on the U.S. government.”

The question comes up near the end of the book, at a moment when Sifton and a colleague are waiting to talk with Polish prosecutors about the no longer secret C.I.A. prison in Poland. Sifton has been at it for almost a decade. He’s jet-lagged and depressed by the Warsaw weather, and he’s begun to wonder what it’s all for. Detachment threatens to set in; he staves it off. He doesn’t expect any American officials to be extradited to Poland and prosecuted. He tells his colleague, “It’s more like writing to the editor of a newspaper.” But human-rights workers like them were sending a message, protecting a principle. They were operating on the level of symbols. Whether anyone was listening, whether anything changed, was secondary.

The work of keeping American leaders honest was a high point in a low era. But it’s not quite the same as looking directly at violence. There’s the well-known American hubris of adventurism, and there’s another kind, which sees our wrongs as dangers to the foundation of civilization. The Bush Administration tortured prisoners and created a legal fiction to justify it; other regimes torture prisoners and call it obtaining a confession from traitors who threaten national security. Is there really such a difference that the second can be dismissed as the way of the world, while the first is “not mere violation but something more profound,” a crime so audacious that “a general deterioration in the valuation of human life, such as was seen in Europe during the first half of the twentieth century, could happen again”? This is exceptionalism in another guise. The belief that when America behaves like other countries the results are worse carries an assumption that America must be different, and in some sense better. The past fourteen years have been hard on almost every dogma, including that one.
<-------->
On September 6, 2006, a score of masked gunmen stormed into a night club in Uruapan, Michoacán, fired at the ceiling, and tossed five severed heads onto the white-tiled dance floor. Being narcotraficantes—members of one of the brutal drug cartels that effectively ruled large swaths of Mexico in the early years of this century—they also left a note. In towns along the border, boastful, taunting, and tendentious banners and placards, or narcomantas, were routinely hung up next to piles of corpses. This one read, “The Family doesn’t kill for money. It doesn’t kill women. It doesn’t kill innocent people, only those who deserve to die. Know that this is divine justice.”

The assassins, or sicarios, as they’re called in Mexico, were members of La Familia Michoacana, a cartel that, despite its penchant for decapitation and torture, had pretensions to piety and a certain rough chivalry. (Years later, remnants of La Familia reorganized as a group calling itself the Knights Templar.) The syndicate’s temporal and spiritual head, Nazario Moreno González, wrote a “bible” of inspirational sayings and admonitions, which members of La Familia were expected to carry with them. Also required reading in the cartel was the book from which Moreno González cribbed much of his pop philosophy, “Wild at Heart: Discovering the Secret of a Man’s Soul,” a paean to muscular Christianity by John Eldredge, an American evangelical who lives in Colorado Springs.


Most crime novelists, especially those reaching for a momentous effect, are obliged to turbocharge their villains. The perpetrator of the locked-room mystery is supernaturally ingenious, the serial killer far more baroquely sadistic than his real-life counterparts, the Mob boss too comprehensively powerful to be believed. Mexico’s criminal cartels have never presented such a problem to Don Winslow, who has written two extensively researched sagas about the war on drugs: “The Power of the Dog,” in 2006, and now “The Cartel” (Knopf). If anything, Winslow has had to tone down the truth and insert some orienting genre formula into the horror and absurdity of actual events. Winslow left the bizarre tidbit about the evangelical self-help book out of “The Cartel,” although parts of the novel are told from the perspective of a member of La Familia, a Chicano runaway trained to kill by sicarios at age eleven and rendered half-feral by a fathomless series of traumas. He’s the one who, in “The Cartel,” removes the five heads from rival cartel members and has them spilled across the dance floor. But the narcos’ reverence for a Holy Roller version of Robert Bly’s “Iron John” must have seemed just too weird to play. (As was, presumably, the 2011 contretemps between another cartel, Los Zetas, and the hacker collective Anonymous—a preposterous movie premise inexplicably graduated to reality.)

“The Cartel,” Winslow’s sixteenth novel, takes place between 2004 and 2012, mostly in Mexico. The point of view skitters among a half-dozen or so characters—all narcos, apart from the novel’s ostensible hero, D.E.A. agent Arturo (Art) Keller—as each pursues his or her own interests through a byzantine web of allegiances, double crosses, devious stratagems, vendettas, and regime changes. The cartels that were mere trafficking gangs in “The Power of the Dog” have become, Keller thinks, “little states and the bosses politicians sending other men to war.” Some of those men are putatively public servants, but graft has so comprehensively penetrated the state that at one point the drug wars take the surreal form of local police fighting their federal counterparts, each side on the payroll of a different cartel. The view that the novel affords is panoramic, and the carnage—drawn from life, or, more precisely, death—is numbing; in 2010, Ciudad Juárez saw an average of 8.5 killings per day, making it the murder capital of the world.

All of Winslow’s novels have been crime fiction, but their stylistic range betrays a restive sensibility. An early series featured the often comic and occasionally globe-trotting adventures of the private detective Neal Carey. Then Winslow, who typically works on two books at once, began bouncing around from the sober epic mode of “The Power of the Dog” to a series of genial mysteries solved by the San Diego surfer-detective Boone Daniels and a pair of sleek thrillers, “Savages” and its prequel, “The Kings of Cool,” which trip giddily from toasted Southern California patois to Baja California nightmare. “Savages” (adapted for the screen by Oliver Stone, in 2012), with a languidly stuttering prose style that practically giggles at itself, cemented Winslow’s reputation. It’s the story of two young Laguna Beach partners in pot cultivation—Ben, a talented botanist who’s into Buddhism and alternative energy, and Chon, a former Navy SEAL who takes a dark view of just about everything—and their shared girlfriend, O, a quipping beach bunny with a taste for acronyms. According to O, her mother, nicknamed Paqu (Passive Aggressive Queen of the Universe), hated having given birth to her:


“She popped me and bought a treadmill on the way home from the hospital.”
Yah, yah, yah, because Paqu is totally SOC R&B.
South Orange County Rich and Beautiful.
Blonde hair, blue eyes, chiseled nose, and BRMCB—Best Rack Money Can Buy (you have real boobs in the 949 you’re, like, Amish)—the extra Lincoln wasn’t going to sit well or long on her hips.
“Savages” and “The Kings of Cool” read like a tale spun out over a long afternoon by someone prone on a couch. “The Power of the Dog” and “The Cartel” seem like the work of another writer entirely—say, a guy with salt-and-pepper temples and an off-the-rack suit, hovering over his bourbon on the next barstool. He’s telling you everything you did and didn’t want to know about what went on and still goes on south of the border in the feeding of North America’s insatiable appetite for pot, heroin, cocaine, and meth. You can’t be sure how much of it is true; Narcolandia is ballad country, a realm of legend and rumor. But none of it is a laughing matter.


Scratch that. Some of “The Power of the Dog” is funny. Winslow can do a comic mid-level Italian gangster as well as most guys. But that novel was written before the slaughter and chaos of the cartel wars reached hallucinatory proportions. Winslow’s subject rose up and challenged him to a rematch. Los Zetas, regarded by many as the most fearsome manifestation of the cartels, isn’t even mentioned in “The Power of the Dog”; in “The Cartel,” the group gets a full history, from its inception, as the enforcement arm of the Gulf cartel, to its eventual takeover of drug-trafficking operations, and on to its more recent expansion into kidnapping, extortion, and the illegal siphoning and sale of oil and natural gas.

Cartoon
BUY THE PRINT »
The narrative spine of “The Cartel” is carried over from “The Power of the Dog”: Art Keller’s long hunt for a Sinaloan drug lord named Adan Barrera. Barrera is locked up in the Metropolitan Correctional Center in San Diego by the end of “The Power of the Dog,” but in “The Cartel” he gets himself transferred to a Mexican prison, where he engineers a life of relative luxury and, eventually, an escape. Keller, having destroyed his family and his personal life in his relentless pursuit of Barrera throughout the first novel, has retired to a monastery in New Mexico, where he keeps bees—the preferred late-life hobby of fictional detectives since Sherlock Holmes. Barrera has put a two-million-dollar bounty on Keller’s head, forcing him on the lam and then, reluctantly, back into the D.E.A.

Thriller heroes tend to fall into two categories, each an idealized projection of the (male) reader’s ego. The first is too good to be true: smarter, braver, and more competent than both the bad guys and the various institutional forces that interfere with his doing what he knows, unerringly, to be best. He’s also potently attractive to women. The second is your basic cable-TV antihero, driven to deeds he deplores by the greater evils of the world, evils that he alone can fully comprehend. This leaves him haunted and alone, although just as potently attractive to women. Winslow’s heroes tend to dwell amid these conventions: Boone Daniels’s sole flaw consists of being so laid-back that he cares more about good friends and doing the right thing than about money, power, or ambition—which is, as shortcomings go, equivalent to the “weaknesses” that candidates offer up in job interviews.


Keller is a brooder. His obsession with Barrera—triggered in “The Power of the Dog” when one of the drug lord’s henchmen tortures his partner to death—has consumed his personality, providing him with the wrecked past so obligatory to his type. He likens himself to “Ahab chasing the great white whale,” but his quest is operational rather than metaphysical; Keller makes the novel go. Winslow gives him a romance with an idealistic doctor, but when Keller tells another character that he reads the novels of Roberto Bolaño and Luis Urrea it’s impossible to picture; surely he winks out of existence when his services are not required by the plot? Keller is not so much a character as a vector, a direction through the unspooling mess of corruption, betrayal, and butchery that harrowed Mexico between 2004 and 2012.

Barrera is also a familiar figure in some respects, a descendant of Mario Puzo’s shrewd and courtly Don Corleone, whose prudence, honor, and decorum evoke admiration in spite of his deeds. (It also helps the image of such men that they rarely do their own wet work.) Much of “The Cartel” hews closely to the reported facts of Mexican cartel history. Sometimes Winslow changes little more than a few proper names. Barrera himself is clearly patterned on Joaquín (El Chapo) Guzmán Loera, the former head of the Sinaloa cartel and a man once deemed by the U.S. Treasury Department to be the most powerful drug trafficker in the world. Like Barrera, Guzmán escaped from a high-security prison; had a long-time mistress who became a cartel operative in her own right until she was murdered by Los Zetas; was involved in a shoot-out that killed a Catholic archbishop (in “The Power of the Dog,” it’s a cardinal); and bribed officials to help him defeat rival cartels and escape captivity. Like Guzmán, Barrera patronizes restaurants by strolling in and having his men confiscate the other diners’ phones, locking the place down until he finishes eating. Afterward, he picks up everyone’s check.

But where Guzmán was something of a hick, barely literate for all his criminal genius, Barrera is suave and tasteful. He disdains the “gaudy, ostentatious displays” favored by the typical “nouveau-riche narcos,” such as diamond-encrusted firearms. In redecorating a family ranch to receive him after his prison break, Barrera opts for “the classic lines of old Sinaloa, while still making sure that the house revealed the proper level of wealth and power.” He would prefer not to live in a mansion, he tells his mistress, “but there are expectations.” Where Guzmán had a love life that was complex enough to fuel several telenovelas—a tangle of mistresses, wives, ex-wives, and short-term paid companions—Barrera spurns the squads of prostitutes deployed at every cartel bash and is a dignified serial monogamist until he agrees to a political marriage with the teen-age daughter of another narco. Above all, Barrera’s violence is always pragmatic. The true villain of “The Cartel,” Heriberto Ochoa, the original Zeta—loosely based on Heriberto Lazcano Lazcano—is a grandiose and bloodthirsty fiend whose followers massacre busloads of migrant workers on the slightest pretext. (Lazcano himself was rumored to feed his enemies to his pet lions and tigers.)

Much is made in “The Cartel” of how Keller’s ruthless fixation on Barrera turns him into a version of the very thing he hates. However true this formulation might be, it’s still a cliché. In truth, Keller isn’t particularly interesting, and Barrera is not much better, but they really don’t need to be. Supporting characters are Winslow’s forte, from Magda, the clever ex-beauty queen who parlays her affair with Barrera into full-fledged narco status, to Eddie Ruiz, a former Texas high-school football star whose placid life as a small-time dealer gets sucked into the nihilistic vortex of the clash between the Gulf cartel and Los Zetas. Best of all, in the middle of the novel Winslow turns his attention to a passel of journalists working in Ciudad Juárez when the cartels were at their peak, and it’s as if he’d opened a window and let in some air. These people—a nebbishy chronicler of Juárez’s street life, a skirt-chasing photographer, a scrappy female reporter who throws fun parties and does “a rather good imitation of the Chihuahua state governor”—feel conscripted from life, not films or books. “The Cartel” opens with a dedication listing the names of a hundred and thirty-one journalists who were “murdered or ‘disappeared’ in Mexico during the period covered in this novel,” so you can tell where this is going.

If the two main characters of “The Cartel” are a little thin, they do their job, delivering the reader into the ongoing disaster that is the war on drugs. The appeal of “The Godfather” was, in part, procedural, as it explained how to conduct a hit or hunker down during a Mob battle, but Winslow’s cartel novels describe how impossible it seems to stop any of it, no matter how much you want to, and no matter how powerful you may be. The characters find themselves forming alliances with their bitterest enemies and betraying their friends in order to fend off consequences that are even worse. Barrera believes that he can’t leave the narco life (otherwise his rivals will assassinate his extended family), and Keller figures that if he doesn’t die in the saddle he’ll just end up hanging out in a Tucson condo until he gets “the bad biopsy,” a prospect he finds even more unbearable. The most fatalistic of the narcos pray to a skeletal saint, Santa Muerte, and boast of drinking human blood in her honor.

The machinery that has delivered all of Winslow’s characters to this place is a vast, interlocking system of competing national interests, ass-covering government agencies, delusional lawmakers, stupid policies, a shortsighted public, corrupt officials, and big business, the whole mass of it driven by the desire for money, power, and chemically induced ecstasy. This machinery has its own perverse majesty, despite Winslow’s well-founded outrage that it has been allowed to grind on and on and on. He has catalogued every part of it: how this piston pushed that crank to rotate this wheel—you don’t write crime fiction, after all, if you’re not fascinated by the operations of crime. Yet the cartel wars escalated from the usual criminal pursuit of self-interest into something extraordinary, something monstrous, a ghost in the machine whose precise origin cannot be traced. Keller calls it “pure evil,” and so does Eddie, who flips on his co-conspirators when things get too freaky. “Someone’s always going to be selling this shit,” he tells Keller. “It might as well be someone who doesn’t kill women and kids. If someone’s going to do it, you guys might as well let someone like me do it.” He has a point. 
<-------->
In August, 1947, when, after three hundred years in India, the British finally left, the subcontinent was partitioned into two independent nation states: Hindu-majority India and Muslim-majority Pakistan. Immediately, there began one of the greatest migrations in human history, as millions of Muslims trekked to West and East Pakistan (the latter now known as Bangladesh) while millions of Hindus and Sikhs headed in the opposite direction. Many hundreds of thousands never made it.

Across the Indian subcontinent, communities that had coexisted for almost a millennium attacked each other in a terrifying outbreak of sectarian violence, with Hindus and Sikhs on one side and Muslims on the other—a mutual genocide as unexpected as it was unprecedented. In Punjab and Bengal—provinces abutting India’s borders with West and East Pakistan, respectively—the carnage was especially intense, with massacres, arson, forced conversions, mass abductions, and savage sexual violence. Some seventy-five thousand women were raped, and many of them were then disfigured or dismembered.


Nisid Hajari, in “Midnight’s Furies” (Houghton Mifflin Harcourt), his fast-paced new narrative history of Partition and its aftermath, writes, “Gangs of killers set whole villages aflame, hacking to death men and children and the aged while carrying off young women to be raped. Some British soldiers and journalists who had witnessed the Nazi death camps claimed Partition’s brutalities were worse: pregnant women had their breasts cut off and babies hacked out of their bellies; infants were found literally roasted on spits.”

By 1948, as the great migration drew to a close, more than fifteen million people had been uprooted, and between one and two million were dead. The comparison with the death camps is not so far-fetched as it may seem. Partition is central to modern identity in the Indian subcontinent, as the Holocaust is to identity among Jews, branded painfully onto the regional consciousness by memories of almost unimaginable violence. The acclaimed Pakistani historian Ayesha Jalal has called Partition “the central historical event in twentieth century South Asia.” She writes, “A defining moment that is neither beginning nor end, partition continues to influence how the peoples and states of postcolonial South Asia envisage their past, present and future.”

After the Second World War, Britain simply no longer had the resources with which to control its greatest imperial asset, and its exit from India was messy, hasty, and clumsily improvised. From the vantage point of the retreating colonizers, however, it was in one way fairly successful. Whereas British rule in India had long been marked by violent revolts and brutal suppressions, the British Army was able to march out of the country with barely a shot fired and only seven casualties. Equally unexpected was the ferocity of the ensuing bloodbath.

The question of how India’s deeply intermixed and profoundly syncretic culture unravelled so quickly has spawned a vast literature. The polarization of Hindus and Muslims occurred during just a couple of decades of the twentieth century, but by the middle of the century it was so complete that many on both sides believed that it was impossible for adherents of the two religions to live together peacefully. Recently, a spate of new work has challenged seventy years of nationalist mythmaking. There has also been a widespread attempt to record oral memories of Partition before the dwindling generation that experienced it takes its memories to the grave.

The first Islamic conquests of India happened in the eleventh century, with the capture of Lahore, in 1021. Persianized Turks from what is now central Afghanistan seized Delhi from its Hindu rulers in 1192. By 1323, they had established a sultanate as far south as Madurai, toward the tip of the peninsula, and there were other sultanates all the way from Gujarat, in the west, to Bengal, in the east.

Today, these conquests are usually perceived as having been made by “Muslims,” but medieval Sanskrit inscriptions don’t identify the Central Asian invaders by that term. Instead, the newcomers are identified by linguistic and ethnic affiliation, most typically as Turushka—Turks—which suggests that they were not seen primarily in terms of their religious identity. Similarly, although the conquests themselves were marked by carnage and by the destruction of Hindu and Buddhist sites, India soon embraced and transformed the new arrivals. Within a few centuries, a hybrid Indo-Islamic civilization emerged, along with hybrid languages—notably Deccani and Urdu—which mixed the Sanskrit-derived vernaculars of India with Turkish, Persian, and Arabic words.

Eventually, around a fifth of South Asia’s population came to identify itself as Muslim. The Sufi mystics associated with the spread of Islam often regarded the Hindu scriptures as divinely inspired. Some even took on the yogic practices of Hindu sadhus, rubbing their bodies with ashes, or hanging upside down while praying. In village folk traditions, the practice of the two faiths came close to blending into one. Hindus would visit the graves of Sufi masters and Muslims would leave offerings at Hindu shrines. Sufis were especially numerous in Punjab and Bengal—the same regions that, centuries later, saw the worst of the violence—and there were mass conversions among the peasants there.


The cultural mixing took place throughout the subcontinent. In medieval Hindu texts from South India, the Sultan of Delhi is sometimes talked about as the incarnation of the god Vishnu. In the seventeenth century, the Mughal crown prince Dara Shikoh had the Bhagavad Gita, perhaps the central text of Hinduism, translated into Persian, and composed a study of Hinduism and Islam, “The Mingling of Two Oceans,” which stressed the affinities of the two faiths. Not all Mughal rulers were so open-minded. The atrocities wrought by Dara’s bigoted and puritanical brother Aurangzeb have not been forgotten by Hindus. But the last Mughal emperor, enthroned in 1837, wrote that Hinduism and Islam “share the same essence,” and his court lived out this ideal at every level.

In the nineteenth century, India was still a place where traditions, languages, and cultures cut across religious groupings, and where people did not define themselves primarily through their religious faith. A Sunni Muslim weaver from Bengal would have had far more in common in his language, his outlook, and his fondness for fish with one of his Hindu colleagues than he would with a Karachi Shia or a Pashtun Sufi from the North-West Frontier.


Many writers persuasively blame the British for the gradual erosion of these shared traditions. As Alex von Tunzelmann observes in her history “Indian Summer,” when “the British started to define ‘communities’ based on religious identity and attach political representation to them, many Indians stopped accepting the diversity of their own thoughts and began to ask themselves in which of the boxes they belonged.” Indeed, the British scholar Yasmin Khan, in her acclaimed history “The Great Partition,” judges that Partition “stands testament to the follies of empire, which ruptures community evolution, distorts historical trajectories and forces violent state formation from societies that would otherwise have taken different—and unknowable—paths.”

Cartoon
“I’m kind of a big deal at my mom’s house.”
BUY THE PRINT »
Other assessments, however, emphasize that Partition, far from emerging inevitably out of a policy of divide-and-rule, was largely a contingent development. As late as 1940, it might still have been avoided. Some earlier work, such as that of the British historian Patrick French, in “Liberty or Death,” shows how much came down to a clash of personalities among the politicians of the period, particularly between Muhammad Ali Jinnah, the leader of the Muslim League, and Mohandas Gandhi and Jawaharlal Nehru, the two most prominent leaders of the Hindu-dominated Congress Party. All three men were Anglicized lawyers who had received at least part of their education in England. Jinnah and Gandhi were both Gujarati. Potentially, they could have been close allies. But by the early nineteen-forties their relationship had grown so poisonous that they could barely be persuaded to sit in the same room.

At the center of the debates lies the personality of Jinnah, the man most responsible for the creation of Pakistan. In Indian-nationalist accounts, he appears as the villain of the story; for Pakistanis, he is the Father of the Nation. As French points out, “Neither side seems especially keen to claim him as a real human being, the Pakistanis restricting him to an appearance on banknotes in demure Islamic costume.” One of the virtues of Hajari’s new history is its more balanced portrait of Jinnah. He was certainly a tough, determined negotiator and a chilly personality; the Congress Party politician Sarojini Naidu joked that she needed to put on a fur coat in his presence. Yet Jinnah was in many ways a surprising architect for the Islamic Republic of Pakistan. A staunch secularist, he drank whiskey, rarely went to a mosque, and was clean-shaven and stylish, favoring beautifully cut Savile Row suits and silk ties. Significantly, he chose to marry a non-Muslim woman, the glamorous daughter of a Parsi businessman. She was famous for her revealing saris and for once bringing her husband ham sandwiches on voting day.

Jinnah, far from wishing to introduce religion into South Asian politics, deeply resented the way Gandhi brought spiritual sensibilities into the political discussion, and once told him, as recorded by one colonial governor, that “it was a crime to mix up politics and religion the way he had done.” He believed that doing so emboldened religious chauvinists on all sides. Indeed, he had spent the early part of his political career, around the time of the First World War, striving to bring together the Muslim League and the Congress Party. “I say to my Musalman friends: Fear not!” he said, and he described the idea of Hindu domination as “a bogey, put before you by your enemies to frighten you, to scare you away from cooperation and unity, which are essential for the establishment of self-government.” In 1916, Jinnah, who, at the time, belonged to both parties, even succeeded in getting them to present the British with a common set of demands, the Lucknow Pact. He was hailed as “the Ambassador of Hindu-Muslim Unity.”

But Jinnah felt eclipsed by the rise of Gandhi and Nehru, after the First World War. In December, 1920, he was booed off a Congress Party stage when he insisted on calling his rival “Mr. Gandhi” rather than referring to him by his spiritual title, Mahatma—Great Soul. Throughout the nineteen-twenties and thirties, the mutual dislike grew, and by 1940 Jinnah had steered the Muslim League toward demanding a separate homeland for the Muslim minority of South Asia. This was a position that he had previously opposed, and, according to Hajari, he privately “reassured skeptical colleagues that Partition was only a bargaining chip.” Even after his demands for the creation of Pakistan were met, he insisted that his new country would guarantee freedom of religious expression. In August, 1947, in his first address to the Constituent Assembly of Pakistan, he said, “You may belong to any religion, or caste, or creed—that has nothing to do with the business of the State.” But it was too late: by the time the speech was delivered, violence between Hindus and Muslims had spiralled beyond anyone’s ability to control it.

Hindus and Muslims had begun to turn on each other during the chaos unleashed by the Second World War. In 1942, as the Japanese seized Singapore and Rangoon and advanced rapidly through Burma toward India, the Congress Party began a campaign of civil disobedience, the Quit India Movement, and its leaders, including Gandhi and Nehru, were arrested. While they were in prison, Jinnah, who had billed himself as a loyal ally of the British, consolidated opinion behind him as the best protection of Muslim interests against Hindu dominance. By the time the war was over and the Congress Party leaders were released, Nehru thought that Jinnah represented “an obvious example of the utter lack of the civilised mind,” and Gandhi was calling him a “maniac” and “an evil genius.”

From that point on, violence on the streets between Hindus and Muslims began to escalate. People moved away from, or were forced out of, mixed neighborhoods and took refuge in increasingly polarized ghettos. Tensions were often heightened by local and regional political leaders. H. S. Suhrawardy, the ruthless Muslim League Chief Minister of Bengal, made incendiary speeches in Calcutta, provoking rioters against his own Hindu populace and writing in a newspaper that “bloodshed and disorder are not necessarily evil in themselves, if resorted to for a noble cause.”

The first series of widespread religious massacres took place in Calcutta, in 1946, partly as a result of Suhrawardy’s incitement. Von Tunzelmann’s history relays atrocities witnessed there by the writer Nirad C. Chaudhuri. Chaudhuri described a man tied to the connector box of the tramlines with a small hole drilled in his skull, so that he would bleed to death as slowly as possible. He also wrote about a Hindu mob stripping a fourteen-year-old boy naked to confirm that he was circumcised, and therefore Muslim. The boy was then thrown into a pond and held down with bamboo poles—“a Bengali engineer educated in England noting the time he took to die on his Rolex wristwatch, and wondering how tough the life of a Muslim bastard was.” Five thousand people were killed. The American photojournalist Margaret Bourke-White, who had witnessed the opening of the gates of a Nazi concentration camp a year earlier, wrote that Calcutta’s streets “looked like Buchenwald.”

As riots spread to other cities and the number of casualties escalated, the leaders of the Congress Party, who had initially opposed Partition, began to see it as the only way to rid themselves of the troublesome Jinnah and his Muslim League. In a speech in April, 1947, Nehru said, “I want that those who stand as an obstacle in our way should go their own way.” Likewise, the British realized that they had lost any remaining vestiges of control and began to speed up their exit strategy. On the afternoon of February 20, 1947, the British Prime Minister, Clement Atlee, announced before Parliament that British rule would end on “a date not later than June, 1948.” If Nehru and Jinnah could be reconciled by then, power would be transferred to “some form of central Government for British India.” If not, they would hand over authority “in such other way as may seem most reasonable and in the best interests of the Indian people.”

In March, 1947, a glamorous minor royal named Lord Louis Mountbatten flew into Delhi as Britain’s final Viceroy, his mission to hand over power and get out of India as quickly as possible. A series of disastrous meetings with an intransigent Jinnah soon convinced him that the Muslim League leader was “a psychopathic case,” impervious to negotiation. Worried that, if he didn’t move rapidly, Britain might, as Hajari writes, end up “refereeing a civil war,” Mountbatten deployed his considerable charm to persuade all the parties to agree to Partition as the only remaining option.

Cartoon
“These pills will cure your O.C.D., but first I wonder if you could organize my shelves.”
BUY THE PRINT »
In early June, Mountbatten stunned everyone by announcing August 15, 1947, as the date for the transfer of power—ten months earlier than expected. The reasons for this haste are still the subject of debate, but it is probable that Mountbatten wanted to shock the quarrelling parties into realizing that they were hurtling toward a sectarian precipice. However, the rush only exacerbated the chaos. Cyril Radcliffe, a British judge assigned to draw the borders of the two new states, was given barely forty days to remake the map of South Asia. The borders were finally announced two days after India’s Independence.


None of the disputants were happy with the compromise that Mountbatten had forced on them. Jinnah, who had succeeded in creating a new country, regarded the truncated state he was given—a slice of India’s eastern and western extremities, separated by a thousand miles of Indian territory—as “a maimed, mutilated and moth-eaten” travesty of the land he had fought for. He warned that the partition of Punjab and Bengal “will be sowing the seeds of future serious trouble.”

On the evening of August 14, 1947, in the Viceroy’s House in New Delhi, Mountbatten and his wife settled down to watch a Bob Hope movie, “My Favorite Brunette.” A short distance away, at the bottom of Raisina Hill, in India’s Constituent Assembly, Nehru rose to his feet to make his most famous speech. “Long years ago, we made a tryst with destiny,” he declaimed. “At the stroke of the midnight hour, when the world sleeps, India will awake to life and freedom.”

But outside the well-guarded enclaves of New Delhi the horror was well under way. That same evening, as the remaining British officials in Lahore set off for the railway station, they had to pick their way through streets littered with dead bodies. On the platforms, they found the railway staff hosing down pools of blood. Hours earlier, a group of Hindus fleeing the city had been massacred by a Muslim mob as they sat waiting for a train. As the Bombay Express pulled out of Lahore and began its journey south, the officials could see that Punjab was ablaze, with flames rising from village after village.

What followed, especially in Punjab, the principal center of the violence, was one of the great human tragedies of the twentieth century. As Nisid Hajari writes, “Foot caravans of destitute refugees fleeing the violence stretched for 50 miles and more. As the peasants trudged along wearily, mounted guerrillas burst out of the tall crops that lined the road and culled them like sheep. Special refugee trains, filled to bursting when they set out, suffered repeated ambushes along the way. All too often they crossed the border in funereal silence, blood seeping from under their carriage doors.”

Within a few months, the landscape of South Asia had changed irrevocably. In 1941, Karachi, designated the first capital of Pakistan, was 47.6 per cent Hindu. Delhi, the capital of independent India, was one-third Muslim. By the end of the decade, almost all the Hindus of Karachi had fled, while two hundred thousand Muslims had been forced out of Delhi. The changes made in a matter of months remain indelible seventy years later.

More than twenty years ago, I visited the novelist Ahmed Ali. Ali was the author of “Twilight in Delhi,” which was published, in 1940, with the support of Virginia Woolf and E. M. Forster, and is probably still the finest novel written about the Indian capital. Ali had grown up in the mixed world of old Delhi, but by the time I visited him he was living in exile in Karachi. “The civilization of Delhi came into being through the mingling of two different cultures, Hindu and Muslim,” he told me. Now “Delhi is dead. . . . All that made Delhi special has been uprooted and dispersed.” He lamented especially the fact that the refinement of Delhi Urdu had been destroyed: “Now the language has shrunk. So many words are lost.”

Like Ali, the Bombay-based writer Saadat Hasan Manto saw the creation of Pakistan as both a personal and a communal disaster. The tragedy of Partition, he wrote, was not that there were now two countries instead of one but the realization that “human beings in both countries were slaves, slaves of bigotry . . . slaves of religious passions, slaves of animal instincts and barbarity.” The madness he witnessed and the trauma he experienced in the process of leaving Bombay and emigrating to Lahore marked him for the rest of his life. Yet it also transformed him into the supreme master of the Urdu short story. Before Partition, Manto was an essayist, screenwriter, and journalist of varying artistic attainment. Afterward, during several years of frenzied creativity, he became an author worthy of comparison with Chekhov, Zola, and Maupassant—all of whom he translated and adopted as models. Although his work is still little known outside South Asia, a number of fine new translations—by Aatish Taseer, Matt Reeck, and Aftab Ahmad—promise to bring him a wider audience.

As recently illuminated in Ayesha Jalal’s “The Pity of Partition”—Jalal is Manto’s great-niece—he was baffled by the logic of Partition. “Despite trying,” he wrote, “I could not separate India from Pakistan, and Pakistan from India.” Who, he asked, owned the literature that had been written in undivided India? Although he faced criticism and censorship, he wrote obsessively about the sexual violence that accompanied Partition. “When I think of the recovered women, I think only of their bloated bellies—what will happen to those bellies?” he asked. Would the children so conceived “belong to Pakistan or Hindustan?”

The most extraordinary feature of Manto’s writing is that, for all his feeling, he never judges. Instead, he urges us to try to understand what is going on in the minds of all his characters, the murderers as well as the murdered, the rapists as well as the raped. In the short story “Colder Than Ice,” we enter the bedroom of Ishwar Singh, a Sikh murderer and rapist, who has suffered from impotence ever since his abduction of a beautiful Muslim girl. As he tries to explain his affliction to Kalwant Kaur, his current lover, he tells the story of discovering the girl after breaking into a house and killing her family:


“I could have slashed her throat, but I didn’t. . . . I thought she had gone into a faint, so I carried her over my shoulder all the way to the canal which runs outside the city. . . . Then I laid her down on the grass, behind some bushes and . . . first I thought I would shuffle her a bit . . . but then I decided to trump her right away. . . . ”
“What happened?” she asked.
“I threw the trump . . . but, but . . . ”
His voice sank.
Kalwant Kaur shook him violently. “What happened?”
Ishwar Singh opened his eyes. “She was dead. . . . I had carried a dead body . . . a heap of cold flesh . . . jani, [my beloved] give me your hand.”
Kalwant Kaur placed her hand on his. It was colder than ice.
Manto’s most celebrated Partition story, “Toba Tek Singh,” proceeds from a simple premise, laid out in the opening lines:

Cartoon
“Wait, what if we convinced the jury that, while they’re wasting their time with me, the real Socrates is still at large?”
BUY THE PRINT »
Two or three years after the 1947 Partition, it occurred to the governments of India and Pakistan to exchange their lunatics in the same manner as they had exchanged their criminals. The Muslim lunatics in India were to be sent over to Pakistan and the Hindu and Sikh lunatics in Pakistani asylums were to be handed over to India.
It was difficult to say whether the proposal made any sense or not. However, the decision had been taken at the topmost level on both sides.
In a few thousand darkly satirical words, Manto manages to convey that the lunatics are much saner than those making the decision for their removal, and that, as Jalal puts it, “the madness of Partition was far greater than the insanity of all the inmates put together.” The tale ends with the eponymous hero stranded between the two borders: “On one side, behind barbed wire, stood together the lunatics of India and on the other side, behind more barbed wire, stood the lunatics of Pakistan. In between, on a bit of earth which had no name, lay Toba Tek Singh.”

Manto’s life after Partition forms a tragic parallel with the institutional insanity depicted in “Toba Tek Singh.” Far from being welcomed in Pakistan, he was disowned as reactionary by its Marxist-leaning literary set. After the publication of “Colder Than Ice,” he was charged with obscenity and sentenced to prison with hard labor, although he was acquitted on appeal. The need to earn a living forced Manto into a state of hyper-productivity; for a period in 1951, he was writing a book a month, at the rate of one story a day. Under this stress, he fell into a depression and became an alcoholic. His family had him committed to a mental asylum in an attempt to curb his drinking, but he died of its effects in 1955, at the age of forty-two.

For all the elements of tragic farce in Manto’s stories, and the tormented state of mind of Manto himself, the reality of Partition was no less filled with absurdity. Vazira Zamindar’s excellent recent study, “The Long Partition and the Making of Modern South Asia,” opens with an account of Ghulam Ali, a Muslim from Lucknow, a city in central North India, who specialized in making artificial limbs. He opted to live in India, but at the moment when Partition was announced he happened to be at a military workshop on the Pakistan side of the border. Within months, the two new countries were at war over Kashmir, and Ali was pressed into service by the Pakistani Army and prevented from returning to his home, in India. In 1950, the Army discharged him on the ground that he had become a citizen of India. Yet when he got to the frontier he was not recognized as Indian, and was arrested for entering without a travel permit. In 1951, after serving a prison sentence in India, he was deported back to Pakistan. Six years later, he was still being deported back and forth, shuttling between the prisons and refugee camps of the two new states. His official file closes with the Muslim soldier under arrest in a camp for Hindu prisoners on the Pakistani side of the border.

Ever since 1947, India and Pakistan have nourished a deep-rooted mutual antipathy. They have fought two inconclusive wars over the disputed region of Kashmir—the only Muslim-majority area to remain within India. In 1971, they fought over the secession of East Pakistan, which became Bangladesh. In 1999, after Pakistani troops crossed into an area of Kashmir called Kargil, the two countries came alarmingly close to a nuclear exchange. Despite periodic gestures toward peace negotiations and moments of rapprochement, the Indo-Pak conflict remains the dominant geopolitical reality of the region. In Kashmir, a prolonged insurgency against Indian rule has left thousands dead and still gives rise to intermittent violence. Meanwhile, in Pakistan, where half the female population remains illiterate, defense eats up a fifth of the budget, dwarfing the money available for health, education, infrastructure, and development.

It is easy to understand why Pakistan might feel insecure: India’s population, its defense budget, and its economy are seven times as large as Pakistan’s. But the route that Pakistan has taken to defend itself against Indian demographic and military superiority has been disastrous for both countries. For more than thirty years, Pakistan’s Army and its secret service, the I.S.I., have relied on jihadi proxies to carry out their aims. These groups have been creating as much—if not more—trouble for Pakistan as they have for the neighbors the I.S.I. hopes to undermine: Afghanistan and India.

Today, both India and Pakistan remain crippled by the narratives built around memories of the crimes of Partition, as politicians (particularly in India) and the military (particularly in Pakistan) continue to stoke the hatreds of 1947 for their own ends. Nisid Hajari ends his book by pointing out that the rivalry between India and Pakistan “is getting more, rather than less, dangerous: the two countries’ nuclear arsenals are growing, militant groups are becoming more capable, and rabid media outlets on both sides are shrinking the scope for moderate voices.” Moreover, Pakistan, nuclear-armed and deeply unstable, is not a threat only to India; it is now the world’s problem, the epicenter of many of today’s most alarming security risks. It was out of madrassas in Pakistan that the Taliban emerged. That regime, which was then the most retrograde in modern Islamic history, provided sanctuary to Al Qaeda’s leadership even after 9/11.

It is difficult to disagree with Hajari’s conclusion: “It is well past time that the heirs to Nehru and Jinnah finally put 1947’s furies to rest.” But the current picture is not encouraging. In Delhi, a hard-line right-wing government rejects dialogue with Islamabad. Both countries find themselves more vulnerable than ever to religious extremism. In a sense, 1947 has yet to come to an end.
<-------->
People kept mentioning his name, but I was slow to encounter the Chilean writer Alejandro Zambra. I hadn’t read anything by him before opening his new story collection, “My Documents” (McSweeney’s). The title story is immediately captivating; it bolts straight out of the book, running at the reader in gusts of life and joy. Though the narrator and the author may not be identical, the wonderful details have the liberated onrush of memory: they tumble like things randomly released, not lengthily chosen. The narrator reaches back to memories of his Santiago childhood and brings us vivid scenes: life at secondary school, where his friend Dante, a very tall, autistic boy, wanders about, telling everyone his exact weight (“Hi, today I’m weighing 227 pounds”); the narrator’s mother, who becomes obsessed with Simon and Garfunkel and plasters the marital bedroom with posters of her idols, despite her husband’s irritation; and going to Mass, where the priest (who can be seen zooming around the neighborhood on a scooter) hurries through the homily, “delivering it with a pleasant disdain, and even making, quite often, a hand gesture that meant ‘et cetera.’ ”

The story begins like this: “The first time I saw a computer was in 1980, when I was four or five years old.” The little boy sees the strange object in his father’s office. But his mother, though trained on a computer, prefers a typewriter, on which she types up songs, poems, and stories written by the narrator’s grandmother, who “was always entering some contest.” (Hence the narrator’s formulation “My father was a computer and my mother was a typewriter.”) This grandmother is remembered as a woman of ready phrases. If someone suggested that it was cold outside, she would return with: “Well, it certainly isn’t hot.” And instead of just saying the word “no”


she was quick to reply “Not at all, as the fish said,” or just “As the fish said,” or simply “Fish,” to summarize this saying: “Not at all, as the fish said when asked how he’d like to be cooked, in the oven or the fryer.”
The grandmother, though a believer, has little time for organized religion. “I don’t need to say prayers,” she tells her grandson. “It’s enough to have a conversation with Jesus, freely, before I go to sleep,” a statement that the boy finds curious and a bit intimidating. At school, in 1985, there is a new teacher, Juan Luis Morales Rojas, who instructs the class to repeat his name, which the children do with burgeoning confidence and volume: “And after a while we were shouting and jumping while he moved his hands like an orchestra director, or like a musician who was enjoying listening to the audience sing along to the chorus of one of his songs.” When the kids get tired of shouting and laughing, the teacher tells them that now they will never forget his name: “In all my years at that school, I don’t remember a happier moment than that one.”

The title story (also the first) is worth lingering over, because it’s so appealing and funny, and because it displays in miniature Zambra’s delicate talents. On the one hand, the writer opens his senses wide, to the jubilant secularism of remembered detail, to a cataloguing of life that seems free, unjudged, open-ended—those schoolchildren, for instance, shouting the teacher’s name again and again, the scene apparently placed in the story for no better reason than that it still delights. On the other hand, these are Chilean schoolchildren, and Alejandro Zambra was born in 1975, two years after the coup that brought down President Salvador Allende and installed the murderous Augusto Pinochet, so history will fatally interrupt—interrupt, then warp and dominate. The narrator tells us that after the attempt on General Pinochet’s life, in 1986, Dante went around asking everyone in the neighborhood “if they belonged to the right or the left.” Now the narrator begins to hear about those who have been arrested, tortured, disappeared. He is filled with feelings of “impropriety, of ignorance, smallness, estrangement.” With his friends, he is left-wing, but at home he is more right-wing. Mainly, he keeps quiet and tries to fit in.

Two years later, in 1988, he enters the National Institute, Chile’s oldest secondary school (which Zambra himself attended). “And that’s when, at the same time, democracy and adolescence arrived. The adolescence was real. The democracy wasn’t.” Many Chilean Presidents were educated at the National Institute, including Salvador Allende. To be at such a school is to be political, whether one wants to be or not. The unstoppable jubilance of the kids who endlessly shouted their teacher’s name has become something else, something warier, more knowing, disillusioned. And by 1994, when the narrator enters the University of Chile, even the sweetness of his musings about childhood computers and typewriters has been subtly stained. As a student, he uses a computer, but he always erases his files: “I didn’t want to leave any records.” The narrator’s “documents” are at once innocent and corrupted. They are nothing more than a joyous calendar of reminiscence, and at the same time a bitter reckoning with history, and the reader understands that there can be no purely innocent fictional record, however much the author may long for it. “I was a blank page, and now I am a book” is the last line of this story, one that stands as a kind of admonition for the rest of Zambra’s collection: blank pages get written on, scored, scrawled over, filled up, and used up. And, in ways both good and bad, books can’t be erased as easily as computer files.



“My Documents” is the fourth book by Alejandro Zambra to be translated into English (this one very ably by Megan McDowell). All of them are very short and strikingly original, and display a wry self-consciousness about the obligations, difficulties, and pleasures of writing fiction. Zambra often features protagonists who are writers, and often these writers are seen to be writing the stories we are reading. In his earlier work, this metafictional element, though likable, occasionally seemed a bit modish and weightless, as if the young author were dutifully channelling his fellow-Chilean Roberto Bolaño (the obvious influence, gratefully studied) and Paul Auster (the more complicated influence, ultimately resisted). There’s a little too much of this kind of thing: “Anita’s husband was called Andrés, or Leonardo. Let’s agree that his name was Andrés and not Leonardo. Let’s agree that Anita was awake and Andrés half-asleep.”

Zambra’s first novel, “Bonsai,” translated into English by Carolina De Robertis and published here in 2008, holds stories-within-stories; it hides nestled simulacra, like those wallets in stores which contain fake credit cards. It tells about two young lovers, Julio and Emilia, who are briefly and passionately together, and about how, after their relationship ends, Emilia commits suicide. Julio and Emilia are brought together, in part, by their love of literature. They happen to read a story by Macedonio Fernández, about a couple who buy a small plant as a symbol of their love and, realizing that if the plant dies their relationship is symbolically doomed, decide to lose the little plant amid a lot of other identical plants. Julio and Emilia dislike the story—a sign, perhaps, that their own love is waning. Later, when, indeed, Julio is no longer living with Emilia, we see that he is working on a novel called “Bonsai,” which appears to be about a man who is mourning the death of the woman he loved in his youth. When this couple were together, they took care of a little plant, a bonsai. Julio’s novel is his homage to the memory of Emilia.

“Bonsai,” though it attracted plenty of attention in Chile when it was published there, in 2006, seems fairly slight. The self-reflexive fictionality, in its multiple iterations, appears obsessive, and strikes one as an elaborate way to make a point already familiar in much postmodern work of the past forty years: that life resembles a fiction, and that fiction resembles another fiction, too. But Zambra’s novel is always lively, often funny and aphoristic, and it introduces the kind of intriguing young man who will appear often in Zambra’s later work, a Chilean, updated version of Russian literature’s Superfluous Man—spectatorial, somewhat literary (i.e., always “writing” something), hovering on the edge of things, passionate in love but destined to lose what he loves, and thus fatalistic and defensively unserious. Julio, we are told, avoids serious relationships, “hiding not from women so much as from seriousness.” This posture could describe several of the men who appear in the stories in “My Documents”: Rodrigo, in “The Most Chilean Man in the World,” who wanders around a city in Belgium, having feebly tried to win back his girlfriend; or the feckless (and finally sinister) Max, in “Memories of a Personal Computer,” who “smoked a lot while he wrote, or, rather, he wrote a little while he smoked a lot,” and is apparently unbothered by the disintegration of his relationship with his lover; or Martín, in “Family Life,” who is cat-sitting for a family and spends the four months of his tenure doing little more than watching TV, smoking, and fantasizing about inhabiting the lives of the house’s owners—that is to say, impersonating them.

These men seem in some mysterious way corrupted by writing (and by reading). There is a quality of masquerade to their lives, as if all that time spent in parallel fictional worlds had infected the stability of nonfictional reality. Zambra’s second novel, “The Private Lives of Trees” (published here in 2010, in a translation also by Megan McDowell), carries an epigraph from John Ashbery: “Life as a book that has been put down.” In Zambra’s world, it seems all too easy to put that book down. Indeed, Zambra’s work displays a deep ambivalence, amounting to a kind of shame, toward fiction-making. The bonsai’s uselessly autotelic function can seem uncomfortably close to that of the novel. The bonsai is “an artistic replica of a tree, in miniature. It consists of two elements: the living tree and the container.” Julio feels that writing is like tending a bonsai, and that his own novel (the one titled “Bonsai”) has become “unnecessary.”

Cartoon
“I don’t know if I want to marry, but I would like a combined household income.”
BUY THE PRINT »
Zambra’s work abounds in qualifications and complaints of this kind. Julian, the protagonist of “The Private Lives of Trees,” described as “a professor, and a writer on Sundays,” waits up one evening for his wife, Veronica, to return home. To pass the time, and to keep his young stepdaughter distracted, he tells her a story that he has been improvising at bedtime, which he calls “The Private Lives of Trees.” But Julian is also writing a real book, which sounds a lot like Zambra’s own first novel—it’s “about a young man tending a bonsai.” This level of self-reflexivity can sometimes seem about as resonant as the prospect of repeatedly having to smell one’s own breath, and perhaps Zambra is knowingly protecting himself from such criticism when he has one of Julian’s friends complain to him that he’s been reading “too much Paul Auster.”

But Zambra’s second novel is not content to doodle metafictionally; it gains surprising power from its reflections on storytelling. Like Julio, Julian seems disappointed with the book he is writing. When he remembers 1984, and the Los Angeles Olympics, he thinks that the only truly “necessary” book would be not about tending a bonsai but “a long story about those days of 1984.” Elsewhere, Julian remembers a period when he lived alone, above a bar. He would write at night, sometimes with feverish productivity, at other times haltingly. He liked hearing the music and the voices: “the sour voice of an older woman who used to tell anyone who would listen about her father’s death, and the panic of an adolescent who, one winter dawn, swore that he would never screw without a condom again.” Zambra’s writing flares up here, in a premonition of the life-filled energy of “My Documents.” And Zambra seems to sense it, too, because he has Julian reflect that it would have been a good idea just to write down everything he heard: “There would surely be more life in those accidental pages than in the book he was writing. But instead of being content with the stories that destiny put at his disposal, Julian remained fixated on his bonsai.”


What appears to torment Zambra is, in fact, the old realist dream of an infinite novel, a fiction that haplessly captures all of life, a novel that has escaped the artificiality of form, that has vanquished the aestheticism of authorial selection: a long book about a whole year, say; a book made up of nothing but inventoried reminiscences; the blank page before it has become a book, as open to life as a camera or a microphone, waiting to be filled up with existence—the “accidental” book that would perforce become a “necessary” one. Recall Zambra’s description of the bonsai: “the living tree and the container.” The container—form, machinery, convention—is what avant-garde fiction has been trying to explode since at least the nineteen-fifties, the better to isolate and nurture the living tree. (Most avant-gardisms, even the antirealist ones, march under the banner of better, or different, or new realisms; writers are sure they know how to make their particular tree grow best.)

Zambra’s breakthrough occurred with his third novel, “Ways of Going Home” (2011; published here in 2013, in Megan McDowell’s sparkling translation), which seems a different order of achievement from his earlier work. Here, at last, Zambra’s authorial self-consciousness, his reflections on the perils and pleasures of fiction-making, finds a theme that gives it moral gravity and not just formal ingenuity. The novel begins with some of the charm and joyousness of “My Documents.” A nine-year-old boy in a suburb of Santiago witnesses the earthquake of March 3, 1985. He is afraid, but he also enjoys the new excitements on the street—the grownups gathered around a fire, the kids put in tents for the night. The boy notices Claudia, a twelve-year-old girl; a couple of years later, they become friends. Claudia is interesting for several reasons, one of them being that she has an uncle, Raúl, who lives alone: “Raúl was the only person in the neighborhood who lived alone. It was hard for me to understand how someone could live alone. I thought that being alone was a kind of punishment or disease.” Mysteriously, Claudia asks the boy to spy on her uncle, who is rumored to be a Communist: “To me, a Communist was someone who read the newspaper and silently bore the mockery of others—I thought of my grandfather, my father’s father, who was always reading the newspaper. Once I asked him if he read the whole thing, and the old man answered that yes, when it came to the newspaper you had to read it all.” The boy later discovers that Raúl is in fact not Claudia’s uncle but her father, Roberto, a left-wing activist who has been living under a new identity in order to escape the dictatorship’s scrutiny.

In the novel’s second section, the narrator is the writer of the story we have just read in the novel’s first section; and Zambra’s book proceeds like this, the fiction about Claudia and Raúl/Roberto alternating with sections narrated by a man who is writing those very fictions. But what might have been dryly self-involved steadily opens out into Chilean history and political reality. This man, this writer, is trying to come to a reckoning with recent political events, and with the knowledge that his parents were politically quietist (and possibly right-leaning) during the Pinochet years. He has searing memories from his childhood. When he was thirteen, he became aware for the first time that his schoolmates included the children of murdered and tortured parents, and of murderers, too. One day, when he was sixteen, the police chased some thieves into the school’s parking lot and fired shots. The class’s history teacher started crying and hid under the table. “He slowly managed to calm down as we explained to him that no, the military had not taken over again. . . . Of course I knew, we all knew; he had been tortured and his cousin was taken prisoner and disappeared.” The teacher asks the boy about his parents, and the boy replies that during the Pinochet years they “kept to the sidelines.” The teacher seems to look at the boy with curiosity and disdain.

The man who is writing the story about Claudia and Raúl and the earthquake wrestles with the function and utility of writing fiction. He comes to the conclusion that “the novel” (by which I think he means the stable, solid, old-fashioned realist novel) belongs to his parents’ generation. As they suffered, their lucky children played and drew pictures: “While the country was falling to pieces, we were learning to talk, to walk, to fold napkins in the shape of boats, of airplanes. While the novel was happening, we played hide-and-seek, we played at disappearing.” If “the novel” belongs to the parents, to the generation that witnessed and suffered and did things (or, in the case of the narrator’s parents, did nothing very much), then what is left for the next generation? To begin with: something that will not look quite like a “novel.” The containers will have to be broken up. But perhaps the odor of triviality will cling to the fictions of the younger generation? Perhaps the young writer is just playing in the shadows, as he did when he was a child?


Once again, as in Zambra’s earlier books, the writer-narrator tells us that he is disappointed by his work. He deletes a lot of what he has written. He switches to writing verse, and this suddenly feels like a reprieve from fiction-making—“no compositions of place, no unnecessary scenes.” Sometimes, he thinks,

when we write, we wash everything clean, as if by doing so we could advance toward something. We ought to simply describe those sounds, those stains on memory. That arbitrary selection, nothing more. That’s why we lie so much, in the end. That’s why a book is always the opposite of another immense and strange book. An illegible and genuine book that we translate treacherously, that we betray with our habit of passable prose.
But this isn’t merely passable prose; it’s rigorous and essential prose, and if Zambra is disappointed by literature he is also saved by it. “Ways of Going Home” becomes a “genuine” book, a necessary one. It is structurally exquisite. The alternating fictions are beautifully mixed, hardly separable: Zambra seems only to be pouring slightly different-colored liquids from one urn to another. And the metafictional meditation takes on a justified ethical anguish: in a political culture of actual disappearance, how can the writer not be acutely sensitive to questions of fictional ethics—to the whole complicated business of fictional lying, of inventing parallel worlds, of game-playing, of narrative presence and absence? How could the responsible writer not bind these scruples into the very form of his work?

In his new book, Zambra returns to the twin sources of his talent—to his storytelling vitality, that living tree which blossoms often in these pages, and to his unsparing examination of recent Chilean history. These come together magnificently in a story titled “National Institute,” which reads like a snatched memory of the years that Zambra spent at the school. A few pages into it, the author starts listing apparently random memories, each of which begins with “I remember.” He remembers friends at school, severe male teachers, a female teacher he fell in love with: “I remember the list of Chilean presidents who had studied at my school. I remember that when teachers reeled off that list, they omitted the name of Salvador Allende.” He remembers one friend in particular, a brilliant, difficult student named Pato Parra, who committed suicide. One morning, near the end of his time at the National Institute, the boys get into a fight and are hauled up before the school’s inspector general, Mr. Musa, who happens to be visiting. He magnanimously informs the boys that he is not going to expel them but is instead going to tell them something they will never in their lives forget. The story ends as, of course, it must: “I forgot it immediately. I sincerely don’t know what Musa told me then.” What begins as a slight exercise in reminiscence becomes a deeper tale about presence and absence, appearance and disappearance, in which unofficial memory (the author’s casual “I remember”) triumphs over official memory. It’s a vindication of what the committed and talented fiction writer can do best, and the victory is all the sweeter because Alejandro Zambra peels off this utterly charming fragment as if it were nothing very much, as if he were just offering us one of his candies, on the way home from school. 
<-------->
John Ashbery’s latest book of poems—his twenty-sixth, not counting various compilations and re-issues—is “Breezeway” (Ecco). As with most of Ashbery’s work, its medium is composed partly of language foraged from everyday American speech. The effect is sometimes unnerving, as though somebody had given you your own garbage back as a gift, cheerfully wrapped. Ashbery is nearly eighty-eight; more than ever, his style is a net for the weirdest linguistic flotsam. Few others of his generation would think to put “lemon telenovela” or “texasburger” in a poem, or write these lines: “Thanks / to a snakeskin toupee, my grayish push boots / exhale new patina / prestige. Exeunt the Kardashians.” He has gone farther from literature within literature than any poet alive. His game is to make an intentionally frivolous style express the full range of human feeling, and he remains funnier and better at it, a game he invented, than his many imitators.

It’s common for people to prefer a prior Ashbery, though few can agree on which one. There is the noncompliant poet of “The Tennis Court Oath,” his 1962 book, giddy in his defiance of meaning; the poet of childhood and its longueurs whom we encounter in his seven-hundred-and-thirty-nine-line poem, “The Skaters” (1966); the sublime meditative poet of “Self-Portrait in a Convex Mirror” (1975); the elegist of “Your Name Here” (2000).


But for years now Ashbery has been writing poems like those in “Breezeway,” short lyrics that begin anywhere and end with a shrug, formed from a bricolage of pop-cultural trivia and cliché. They aren’t “closed works,” as he has put it; they are lengths of consciousness that he will “snip off” at random intervals, like licorice cut from a spool:

Someone said we needed a breezeway
to bark down remnants of super storm Elias jugularly.
Alas it wasn’t my call.
I didn’t have a call or anything resembling one.
You see I have always been a rather
dull-spirited winch.
The style works partly by taking phrases whose contours already exist in the mind—“remnants of super storm Elias,” for example—and substituting near-misses: the verb “to bark down” is almost “to back down” or “to break down,” which I suppose you might do when hit with a storm’s debris; the meaningless adverb “jugularly” might be “jocularly” or “muscularly,” misheard through the storm’s strong winds. You’d rather have a “winch” than a “wench” in a storm: the context implies the former, the tone the latter. These poems conjure a massive mental errata slip made up of what they almost say and nearly mean.

Ashbery’s style prizes such mistakes and misapprehensions, as though looking for the word on the tip of the tongue. William James described consciousness as the “alternation of flights and perchings,” suggesting that we tend to overvalue the “perchings,” the nouns or the primary verbs in a sentence that steal the spotlight from the little words, like “in,” “and,” “but,” “or,” and “of.” It was James, a profound influence on Ashbery, who coined the term “stream of consciousness,” and who insisted on what he called a “reinstatement of the vague and inarticulate to its proper place in our mental life.” James’s “flights” and in-between zones find, in “Breezeway,” a breezeway: a structure between structures, a place to rest that is not a resting place, a “long Q & A period” before the big event is adjourned—a period marked, as in the title of one poem, by deliberate “Andante and Filibuster.”

These are late poems, working alertly within the uncommon genre of poems written in extreme old age, a genre they in turn significantly expand. The poems anticipate death but hold it off—they filibuster—by transfiguring it into comic forms. Before I looked it up, I figured that “Auroch” was a parody of the fashionable names hipsters give to their children, but “Seven-Year-Old Auroch Likes This”—while it mentions “a Brooklyn family”—in fact refers to aurochs, an extinct variety of cattle. The bad news is that you’re extinct; the good news is that you’re only seven. Switch this around, and you get Ashbery’s plight: the species carries on while you approach mortality. The feeling of renewal within doom, of gearing up for the last time, colors the poem:

Antique mud wrestlers shape up
for the last time, no scuttling of vain things
left undone. When you get back I’ll just
hit another menu, safe as a can of soup in a mini-mart.
Saw you first on Masterpiece Theater.
I used to climb right in. That was funny yet unbidden.
When you were alive they called him a stooge.
My voice to young adolescents is like, whom d’ya know,
hiding their accomplishments in bread?
I suppose we will all be faced with the choice of whether to become an “antique mud wrestler,” rotting in the grave, or “a can of soup,” shelved in an urn beside the others in some mini-mart mortuary. Am I reading too much into these lines? Of course; but part of the fun in Ashbery is finding how much narrative sense can be pieced together from these remarkable associative feats, in order to appreciate the surplus above and beyond the story they nearly tell. In the afterlife, maybe we’ll run into all the people who were old when we were young, like the stars of “Masterpiece Theatre”; in the meantime, just trying to speak the language of adolescents is enough to kill a person.


“Breezeway” is partly about the contents of individual memory, so distinct from the official cultural record. It’s “Exeunt the Kardashians” because, like all the effluvia of the current moment, the Kardashians replaced an earlier canon of throwaway cultural artifacts, and will themselves be replaced by newer ones soon.

But this relentless tilling of culture by culture does not erase our individual memories. These poems are little lockboxes for all the forgotten material of an idiosyncratic mind, from Mr. Coffee Nerves to the Ritz Brothers to Klondike Scotty. The irony is that culture now has a means of recalling these forlorn details, in the form of Google and YouTube. For a poet of Ashbery’s predisposition, this nearly miraculous reappearance of things long thought lost and now instantly available to anyone who looks creates a new kind of old age, where, instead of watching the bird feeder, a person can watch the culture, his own past flashing eerily before his eyes.

Now that we can view “The Black Cat” or “I Am a Fugitive from a Chain Gang” on our phones, we forget that, for a person growing up in the nineteen-thirties, seeing a particular film might have seemed like a once-in-a-lifetime event. The movies were especially important if you grew up in a rural outpost like Sodus, New York, where Ashbery spent his childhood; they were more important still if you were gay, since they brought to you, sitting in the dark, an entire palette of suppressed desire masquerading as straight romance. The title of the poem “Queer Subtext” suggests the way that you cannot help but look for one when you’re watching that kind of film, even when confronted with “young, freelancing, orange-juice-in-the-desert, / mythical ladies of China.” But subtexts generally aren’t acknowledged in the titles of poems, or at all; that’s what makes them subtexts. “The Ritz Brothers on Moonlight Bay” plays this kind of game, hiding its secrets in plain sight:

A gay avalanche destroyed much of the town.
Please, I thought we were winning.
Set the wolves, I mean the dogs
on her, that is, him.
The stalled investigation proved otherwise. . . .
Al and Harry had their moment in the sun.
Oblivion swiftly followed, the universe
playing catch-up, as
it is wont to do. Oh, bugger
the attendance record! I see a long line
of attendees waiting, cock in hand.
Avalanches are “gay” and oblivion is “playing catch-up” because these features are filtered through the prerogatives of a horny kid, suddenly more eager to “bugger” than to set the “attendance record,” though sex is another kind of attendance, and offers another opportunity to set a “record.” The “long line of attendees” suggests a wake and an orgy, the mourners gathered not “hat in hand” but “cock in hand”—though the phrase could also modify the “I,” whose fantasies govern the passage.

The finest lyrics in this book rank with Ashbery’s best short poems: “Farm Hubbub,” “Supercollider,” “A Breakfast Radish,” the prose poem “Dream of a Rarebit Fiend” (its title taken from a 1906 silent film, long impossible to find, now easily available on YouTube), “Hand with a Picture,” as well as “Listening Tour”:

We were arguing about whether NBC
was better than CBS. I said CBS
because it’s smaller and had to work
harder to please viewers. You didn’t
like either that much but preferred
smaller independent companies.
Just then an avalanche flew
overhead, light blue against the
sky’s determined violet. We
started to grab our stuff but
it was too late. We segued . . .
The speaker, probably dead from the avalanche he described, remembers the minute distinctions and gradations of judgment that the living use to pass the time. It struck me that all of Ashbery’s recent work could be imagined as posthumous, fixated as it is on the revealed beauty of allegedly trivial experiences. Which network you prefer doesn’t matter, until it matters, later, that it once mattered. From his current vantage point, monitoring the past with a gift as big as any American poet has ever controlled, keeping an ear alert for the invigorating ironies of the present, Ashbery must know he is one for the ages.

The final poem in this book, its title quoting Robert Herrick, is “A Sweet Disorder.” It ends with the great question that Keats asked at the end of “Ode to a Nightingale.” Most poets who live into their eighties must occasionally think of Keats, who died at twenty-five, and wonder what that beautiful young man’s old age might have looked like:

My gosh, it’s already 7:30.
Are these our containers?
Pardon my past, because, you know,
it was like all one piece.
It can’t have escaped your attention
that I would argue.
How was it supposed to look?
Do I wake or sleep? 
<-------->
Forty  years ago, for a brief stretch of my long, non-affluent slog through graduate school, I lived at 30 Francis Avenue, in Cambridge, Massachusetts, at the house of Professor and Mrs. John Kenneth Galbraith. In exchange for a small room (no board), I walked the family dog three times a day and did household chores that included vacuuming the basement, which was decorated with some whimsical art work by Jacqueline Kennedy.

My general misery was alleviated by what felt like a measure of Victorian beneficence: I had the run of the house’s library. Still, I was so preoccupied with studying for my oral exams (English, not economics) that I rarely made it past the inscribed title pages of those volumes by the living and the famous which I plucked from the Galbraiths’ shelves. I recall one greeting that appeared at the front of a collection of essays by William F. Buckley, Jr.: “To Ken and Kitty, Once a day, for dizziness. Love, Dr. Bill.” The civilized improbability of the Buckley-Galbraith comradeship, sustained across an intellectual divide and upon the ski slopes of Gstaad, made each into the other’s best-known unlikely friend.


A half-dozen years after departing Professor Galbraith’s house, I began writing frequently for National Review, the journal of his conservative foil, even though most of my own literary lodestars were on the left. (I once managed to praise, within the magazine’s pages, the wit of Gore Vidal, an object of fury and litigation for its editor.) Few writers glowed more brightly for me than Norman Mailer, whose reportorial astonishments and overreachings had lit with meaning anything of the nineteen-sixties that I had managed to glimpse through my freshman dorm window or on my black-and-white portable TV. There was, and remains, a daring and a bigness to Mailer, derived from his preference for being knocked off balance instead of dug in. Among American writers of his day, he was alone in thinking that a trip to the moon, even one funded by the military-industrial complex of the country that he sometimes called Cancer Gulch, might be worth a book.

Buckley held Mailer in high, wary regard. Fairly early in his busy career, Buckley had given up on writing his own “big” book; later, he conceded of Mailer, “He’s a genius and I’m not.” Upon Mailer’s death, in 2007, only months before his own, Buckley repeated his belief that the novelist had “created the most beautiful metaphors in the language.” By that point, it scarcely mattered that Mailer, when operating on more literal levels, had advanced a view of the world that Buckley found in large part preposterous.

In “Buckley and Mailer” (Norton), whose overstated subtitle is “The Difficult Friendship That Shaped the Sixties,” Kevin M. Schultz, a historian at the University of Illinois-Chicago, sets out to reconstruct an association that in fact had less warp and woof to it than Buckley’s friendship with Galbraith. John B. Judis’s biography of Buckley says that he was “friendly with” but never “very close” to Mailer. Still, Buckley’s durable cordiality toward Mailer is more remarkable than his being amigos with Galbraith or belligerents with Vidal, and it seems pardonable for Schultz to extend what ought to have been a magazine article into a book-length safari in search of something significant. Here and there he even finds it.

Show biz, for which both men had plenty of aptitude, first connected the two: John Golden, a young Chicago promoter, arranged to put Buckley and Mailer together in a public debate, on September 22, 1962, which quickly sold out.

Mailer was several months away from turning forty. Having largely flamed out in fiction after the success of “The Naked and the Dead,” and lucky to have escaped a prison sentence for stabbing his second wife, he was still in the early, shaky stages of a comeback second only in that era to Frank Sinatra’s. It had begun with the self-referential miscellany “Advertisements for Myself” (1959) and was continuing with his Esquire pieces on the Kennedy candidacy (“Superman Comes to the Supermarket”) and Presidency. Another half decade would bring him to greatness with “The Armies of the Night,” which won a Pulitzer, and “Miami and the Siege of Chicago.”

Buckley, only thirty-six, was the standout son of a large, rich, conservative Catholic family. (Vidal called them “the sick Kennedys.”) He had attracted notice for his own books (“God and Man at Yale,” “Up from Liberalism”) and for National Review, whose rearguard crusades appeared to have even fewer prospects of success than those being waged on the far-left side of the political greensward.

Posters for Buckley and Mailer’s Chicago bout promised “the Debate of the Year” between the “forceful philosopher of THE NEW CONSERVATISM” and “ ‘America’s angry young man’ and Leading Radical.” Abbie Hoffman was in the audience; folksingers provided entertainment during intermission; and Playboy held the rights to publish a transcript. As Schultz notes, Buckley had less to lose than Mailer, whom he had called, two years earlier, in National Review, a “moral pervert.” In the event, he came away from the auditorium impressed, telling readers of his next column that Mailer “doesn’t know what it is he wants to say, but his desperate anxiety to say it, fired by his incandescent moral energy, makes him very much worth watching.” There may have been, along with the grudging admiration, a tinge of envy. As Schultz notes, Mailer was a bold, quicksilver “philosopher,” whereas the far less introspective Buckley always tended to see himself as a mere “salesman” for certainties he was duty-bound to make obvious to others.


Mailer protested the subtitle that Playboy printed with the text of his speech: “A Liberal’s View.” Schultz’s account of the evening supports the objection, as did Buckley’s new awareness of Mailer’s bold, unfixed positions. Mailer characterized himself as a “libertarian socialist”—a label that Mary McCarthy, another left-leaning writer Buckley admired, had also applied to herself, demonstrating what Buckley called “her gift for oxymoron.” But there was something more elemental than synthetic in Mailer’s attempt to have things both ways. In speaking to the Chicago audience, he put as much emphasis on self-realization as upon society, acknowledged his belief in both God and the Devil, and at one point, hamstrung by the tit-for-tat quality of debate, cried, “I’m trying to talk about the nature of man!” Buckley discerned that, show biz aside, this wasn’t an act. He also must have realized the truth of the thesis that Schultz works hard but usefully: the common ground on which he and Mailer stood was not inconsiderable. Both were disgusted with the insipid aspects of American liberalism—a tepid consensus, corporate and bipartisan, that left each fearing not that the center couldn’t hold but that it would. “Buckley and I had been attacking this Center from our opposite flanks,” Mailer insisted. Even so, both were repelled by the violence with which it unexpectedly collapsed, and they were left cold by what Schultz calls the “rights-based model” of society, the beginnings of the identity politics that in the nineteen-seventies started replacing the liberal establishment.


In the meantime, each man gaudily challenged that establishment by running for mayor of New York City—Buckley in 1965, Mailer four years later. Both got beaten by John V. Lindsay, but only after stealing the show with campaigns of quixotic provocation. Mailer told one audience that he was running because “I want to see where my own ideas lead.” He proposed that New York become the fifty-first state and give its neighborhoods a degree of local autonomy undreamed of by even such a small-government advocate as Buckley. Under his plan, Mailer explained, a geriatric patch of the city “might wish to purchase massive police protection,” while a younger, hipper one would be free to legalize LSD.

Cartoon
“Wasabi?”
BUY THE PRINT »
Buckley’s campaign is remembered chiefly for what he said his first action would be if he won (“Demand a recount”) and for his proposal to ease traffic and trim flab with an elevated bike path, a then quirky idea that now sounds almost blandly Bloombergian. Schultz confuses what he calls Buckley’s “hatred” of John Lindsay with what was actually contempt; while discussing Buckley’s “special animus” toward his liberal Republican opponent, he quotes his remark that Lindsay “belongs in the Democratic Party.” Six years later, Lindsay joined it, and, a few years after that, Buckley’s old campaign argument that New York’s politicians were “approaching Washington as supplicants, begging it to return to the City some of the income it has taken from it,” seemed worth a second listen.

Throughout Schultz’s book, Buckley tends to be held to stricter standards of morality and logic than Mailer, whose moments of inspiration are more often indulged as yearning or lyrical. Schultz properly condemns Buckley for National Review’s opposition to the civil-rights movement, which Buckley himself eventually recanted, only to note his having “harped on” the Cold War, as if the consideration of political enslavement and possible nuclear apocalypse might have been keeping his audience from something serious. Mailer’s more grotesque moments—haranguing students with his own psychoanalysis of Lyndon Johnson; his uncertainty whether the children of those legal-LSD neighborhoods would end up “creating castles” or being “two-thirds dead of liver disease”—are pretty much allowed to stand as instances of Norman being Norman. Evenhandedness is not necessarily to be prized, but Schultz is operating within what is so much a rote political discourse that he probably doesn’t even know when he’s being less than fair. During Buckley’s run for mayor, we’re told, “the white working class did not sound like reactionaries when Buckley was their mouthpiece.” It’s a coarse, nasty characterization. Try reversing Schultz’s polarity so that a Yale-educated tribune of the black poor—say, Lindsay—is called their “mouthpiece.” That’s not a sentence that’s going to get written.

The Buckley-Mailer correspondence, not especially deep or voluminous, contains sprinklings of genial insult and even the record of a contribution Mailer made to the chronically empty coffers of National Review. Buckley’s fashionable wife, Pat, called Mailer Chooky Bah Lamb, an endearment she’d got from her Scottish nanny and which Mailer threw into his novel “An American Dream”; he called her Slugger. The Buckleys were occasionally, though not often, in the company of Mailer and whomever he was then married to, but the venues seem mostly to have been crowded ones, such as Truman Capote’s 1966 Black-and-White Ball, where Mailer got very drunk and had to be kept from assaulting President Johnson’s national-security adviser, McGeorge Bundy, over Vietnam.

The war was, for a time, a formidable obstacle to any deepening of fellow feeling between Buckley and Mailer. In 1965, Mailer demurred at the possibility of a personal get-together: “I think this is the wrong time for us to have dinner, because instead of having a nice calm quiet and lively conversation about the future of conservatism, my left conservatism and your right conservatism, there’d be too much pressure to have a screaming match.” In 1968, when both went to Chicago for the Democratic National Convention, Buckley sided with the police as reluctantly as Mailer sided with the protesters. During that violent year, Buckley declined the chance to play a small role in Mailer’s film “Maidstone,” about political assassination; he was thus absent on the day that one of the actors, Rip Torn, struck the auteur with a hammer.

Mostly, though, it was the demands of work and the logistics of celebrity that kept Buckley and Mailer from getting closer. The two were peaking at just the same moment: Buckley appeared on the cover of Time two weeks after the massive antiwar demonstration at the Pentagon that Mailer went on to chronicle in “The Armies of the Night.” Early in 1969, Mailer had to turn down an invitation to help crew Buckley’s schooner on a weeklong sail from Miami to New York: the moon shot and then the mayoral race got in the way. Though Mailer appeared three times on “Firing Line,” Buckley’s interview-and-debate program, the pair’s encounters were few and far enough between that Schultz struggles to thicken the broth of the friendship, which sometimes seems more like a motif than a subject. He even resorts to comparing Buckley’s “playful and provocative” eyes with Mailer’s “piercing and oceanic blue” ones.


There’s something else Schultz is up against: this is all getting to be a while ago. If members of Generation Z don’t stumble over a YouTube link to one of these writers, they may never encounter them at all. (During a thesis-advising conversation a few weeks ago, a fine creative-writing student of mine, a college senior, politely asked me to slow down and back up. “Who is Norman Mailer?” he asked.) So it may seem reasonable for Schultz to provide extensive primers on Vietnam, Selma, and “In Cold Blood” (a run-up to the Black-and-White Ball). On the other hand, does he imagine that anyone already drawn to his highly particularized book is going to need the 101 on any of those sixties subjects?

In matters of style, Mailer seems to have had more influence than Buckley upon their joint chronicler. Not all of it is for the best. “For Mailer,” Schultz writes, straining for the spirit of things, “the war symbolized everything that was wrong with postwar America, the conjoining of all the corrosive elements snuffing out the honest American heart—and the price was being paid in human flesh.” Apart from its virtuosic vocabulary, Schultz doesn’t have much sense of Buckley’s prose. He describes “How to deal with Norman Mailer?” as the “first ponderous sentence” of Buckley’s obituary for the novelist. But Schultz’s flaws are mostly ones of exuberance, the jittery overcompensations of an academic in search of an audience. (His previous book was “Tri-Faith America: How Catholics and Jews Held Postwar America to Its Protestant Promise.”) Pumping up the jam is understandable enough, but I couldn’t help thinking that both subjects deserve better sentences than “Deep-seated mistrust about large bureaucracies crept into the tenor of the nation.”

Schultz, who is forty, could also do better with the over-all feel, and some of the facts, of his chosen era. He is a year off on the Montgomery bus boycott; he refers to Ho Chi Minh as “Minh,” and seems to believe that “Oswald’s Tale,” Mailer’s footnoted account of Kennedy’s assassin, was a novel. A few descriptions are plain odd (Pat Nixon’s “harsh” reputation? Roy Cohn a “role model” for even “a certain portion” of young people in 1962?), and one or two are genuinely misleading: a reader will think from Schultz’s account of the Goldwater campaign that the Senator’s biggest opponent for the Republican Presidential nomination was the late-entering William Scranton instead of his dogged foe in the primaries, Nelson Rockefeller. Mailer was the president of PEN not in the nineteen-nineties but during the mid-eighties, when he annoyed many of its members by inviting George Shultz, Ronald Reagan’s Secretary of State, to address them.

This last solecism occurs during Schultz’s epilogic rush through the final decades of his subjects’ lives. To focus on the sixties is fine, but, in arguing for that decade’s centrality, the author ends up condemning both Mailer and Buckley to a kind of premature inconsequentiality, claiming that by the early seventies they were “calcified”; staying busy as “personalities” and writers, they “removed themselves from the pitch of battle, patron saints already.” One can understand a reluctance to dwell on Mailer’s persistence through the nineties and beyond—his endless C.I.A. novel, “Harlot’s Ghost”; the swollen Picasso biography—but he was hardly hors de combat.

The banishment of Buckley is even more disputable. “What had become of the great conservative?” Schultz asks, suggesting that Buckley’s support for the domestically liberal Nixon was a political surrender rather than a tactical feint in the long game he was playing. Schultz sees the Buckley of the seventies as “less involved in the political fray,” but the record will show that the salesman was steadily doing what he could to promote a candidate who finally delivered that “great November day in the future”—what Buckley prophesied in the face of Goldwater’s impending loss, in 1964.

Rather than giving his subjects the hook, Schultz would have been better off with a coda that looks beyond the Reagan years—less transformative than Buckley wished and Mailer no doubt feared—to the current moment. A half century beyond the sixties, when the self-actualizing plea to be “a name and not a number” first attained urgency, Everyman, with each click of the keyboard, now embraces his digitization, sells his privacy for a mess of algorithms used to orchestrate a world neither libertarian nor socialist, an app-happy Cloud of anesthetized convenience. If one is going to evaluate Mailer’s and Buckley’s complementary opposition to the liberal ethos of their time, one ought to carry the examination toward a conclusion as grim as it is inescapable: both men lost. 
<-------->
Family life can seem metaphysically enormous, comprehensively intense, and everything at once: a little society and a conspiracy against society; an inherently conservative unit bristling with radical splinters; the most efficient imaginable conduit for the transfer of misery and the source of all joy. It engrosses, it stuns, it distracts, and it overwhelms. It drags one in the wake of its moral inertia. “Family happiness completely absorbs me,” Tolstoy wrote in his diary, in 1863, “and it’s impossible to do anything.” But family unhappiness would doubtless have been as absorbing, unhappy families being unhappy in their own way.

More than most contemporary writers, the Irish novelist Anne Enright finds it hard to escape the tidal pull of the family. In a series of funny, bleak, radically unsentimental novels, she has examined the engrossments of such life and has pored over the social genetics of family inheritance—the unhappiness we bequeath, the pleasure we inherit, the tyranny of biological contingency. Like “The Gathering” (2007), in which the narrator tells the story of her brother’s suicide, in Brighton, and the consequent wake, her latest novel, “The Green Road” (Norton), is about a clan’s dispersal and reunion. And it, too, has a complicated matriarch at its center. We first encounter Rosaleen Madigan in 1980, in County Clare. Rosaleen’s eldest son, Dan, back from his first year of college, has declared that he wants to be a priest. In response, his mother takes to her bed. It’s not the first time she has employed what Dan calls “the horizontal solution,” but it is the longest application of these particular histrionics. A life of celibacy will mean no grandchildren, no little Dans, from the child who appears to be Rosaleen’s favorite. Dan’s siblings—an elder sister named Constance and a younger brother and sister, Emmet and Hanna—are studious readers of their mother’s selfish passions, lurking around a house become silent and large without their mother’s downstairs presence. Their father is the mild Moses who occasionally ascends to the parental bedroom to bring back the opaque judgments and laws. But there is something more in this pain, something unexplained. “I made him,” Rosaleen says to Hanna of Dan when she finally emerges from her theatrical stupor. “I made him the way he is. He is my son and I don’t like him, and he doesn’t like me either.” When Hanna soothingly replies, “But you like me, Mammy,” her mother offers only the limited lease of the eternal tyrant: “I like you now.”


Enright possesses an unusual combination of talents. She is a rich, lyrical prose writer, who cascades among novelties—again and again, she finds the unexpected adjective, the just noun. A glass of soda, the “surface of it a hush of bubbles.” A stray dog backing away from its owner “in a palsy of hind limbs.” But she is at the same time a brisk and satirical aphorist, who often conceals more than she displays. In an early novel, “What Are You Like?” (2000), a woman is described as having “a big anxious head and smug little feet.” Enright’s sentences often waver, enigmatically: “After the pub they ran down a lane and were suddenly in a place where everyone smelt of the rain.” Or: “She cried the way she always cried in the evening: vague tears.” Late in “The Green Road,” anxious that his mother has died, Dan feels a child’s need for his parent, and feels it “like a whiteness inside his chest.”

Hanna Madigan thinks of her grandmother as “a woman who looked like she had a lot to say, and wasn’t saying any of it.” In the same way, Enright can look as if she were saying something that she is actually concealing. There is a familiar Irish talkiness in her work—eloquent, heated, intimate—that is combined with a bitter reticence akin to that of Harold Pinter’s dramas. (The book might have been named “The Homecoming.”) We are not told exactly why Dan’s announcement brought on his mother’s collapse. What did Rosaleen mean when she told Hanna that she didn’t like Dan? We gather the answer only from juxtaposition and implication. In the book’s second chapter, we leap forward eleven years, to 1991, and to New York City, where Dan is now living. He did not become a priest, and although he has a girlfriend whom he assures everyone he is going to marry, he is having an intermittent relationship with a young man named Billy. Perhaps Dan is bisexual, though none of his gay friends are in any real doubt about the matter, and they appraise the wife-to-be without mercy: “Skinny, as they often are. . . . a classic beard,” with “the unreliable little ribcage, with a pair of those flat little triangular breasts like flesh origami: also lumpy bits from waist to hip where her underwear was a bit too pragmatic.” The answer to the question is obliquely provided; Rosaleen probably intuited Dan’s orientation, and took the declaration of priestly celibacy to be a transferred confession.


The novel moves forward in bursts of acceleration, each new section set in a different period, and each devoted to one of the Madigan children, now grown up. These chapters, generally narrated in a free indirect style fairly close to interior monologue, tend to linger on a state of arrest or frozen crisis. Life happens more speedily offstage, in the gaps between the sections.

There is Constance, in 1997, now thirty-seven, who is waiting at a hospital in County Limerick to learn whether or not she has breast cancer. She has three children, is married to a contractor, and has a widowed mother. (So Rosaleen must now be alone.) Her life seems circumscribed, satisfying, banal, disappointing. Like Dan, she went to New York: “This was the place you went to get a whole new life, and all she got was a couple of Eileen Fisher cardigans in lilac and grey.” She has “two sons who told her nothing and a husband who told her nothing and a father who told her nothing and then died.” There is Emmet, in 2002, who is thirty-eight and working for an N.G.O. in Mali, dealing daily with death and disease. He is austerely charitable, committed to the rigorous politics of international aid and development: “He remembered Geneva airport, a place where he had, after a tough sixteen months in the Sudan, experienced an overwhelming urge to lie down on the clean, perfumed floor.” There is Hanna, in Dublin in 2005, struggling to maintain a career as an actress: “Hanna had the wrong face for a grown-up woman, even if there were parts for grown-up women. The detective inspector. The mistress. No, Hanna had a girlfriend face, pretty, winsome and sad. And she was thirty-seven. She had run out of time.”

Cartoon
“You should text some people.”
BUY THE PRINT »

“The Green Road” is a more conventional novel than anything Enright has written, and these episodes at times have the air of burnished performance. A difficulty with novels that stop and start, that spread their form among various characters and locales, is that much of the narrative energy gets diverted away from a continuous project and into the repeated establishment of new fictional constructions: each chapter becomes an unwanted test case, as the reader waits to see if Enright can “do” early-nineties New York, or Mali. Enright can indeed raise these varied novelistic pop-ups. But the book’s first thirty pages, set in Ireland, a world she knows down to its roots, have a kind of vitality and particularity absent from the chapters set elsewhere. Here is Ireland:

Emmet said their Grandfather Madigan was shot during the Civil War and their Grandfather Considine refused to help. The men ran to the Medical Hall looking for ointment and bandages and he just pulled down the blind, he said. But nobody believed Emmet. Their Grandfather Madigan died of diabetes years ago, they had to take off his foot.
This is storytelling, with the blood-pulse of lived gossip, that little run-on final sentence bearing witness to its coursing unstoppability. Inevitably, Enright’s Manhattan shows little of that insider’s possession, and her portrait of a city stalked by AIDS, sensitive as it is, occasionally resembles only learned gossip:

Various things happened. Massimo went off with Mandy to her family bolt-hole in the Caribbean, Billy held a dinner party which was a qualified success. Arthur published his book on Bonnard and wept for Max (who had detested Bonnard: who spat at the mention of Bonnard) at the launch. Then Emily von Raabs came to town and she hosted a large and informal supper in her wonderfully ramshackle house on East 10th.
The second half of “The Green Road” returns to Rosaleen’s house in County Clare, where the novel lives and breathes, and where the Madigan children must return also, hauling their caravan of complications. It is Christmastime, 2005, and a gathering is taking place at the house in which the Madigans were raised. The last hundred pages are beautifully searching and sad, shot through with difficult wisdom and with much tart comedy. Rosaleen, always demanding, has become monstrously manipulative and self-pitying in old age, “a woman who did nothing and expected everything. She sat in this house, year after year, and she expected.” She gave her children everything, and of course they have disappointed her, and she is content to tell them so. Emmet jokes to Hanna: “At least you didn’t go bald. . . . She took that very personally.” Constance gives her mother an expensive silk scarf, but Rosaleen is not above the timeworn power play: “This is far too good for me.” Enright writes, wickedly, that Rosaleen “hated being upstaged by her own clothes.” In the end, Dan thinks that the family house holds more meaning than one’s heart. The house, after all, has the reliability of inanimate objects, “the reassuring madness of patterned wallpaper under the daily shift of light.” Rosaleen, at seventy-six, wants to sell it, provoking the crisis that consumes the final pages of the novel.

“The Green Road” is true and rueful, as terribly adult in its clarity as its battered Madigans. Enright understands adulthood as a kind of aberration that befalls families: siblings must grow up, but their maturity is oddly irrelevant to the atavism of the family unit. Beneath the social achievements of adult life beat the wings of childhood. Constance, Dan, Emmet, and Hanna hardly know one another as adults. Dan liked Emmet as a boy, “but, grown up, the man bored and frightened him.” (An extraordinarily sad sentence!) To be middle-aged is sometimes to feel that an imposter has grown up around oneself, has choked off one’s own youth. Constance feels her face to be “a shadow passing over the front of her head.” She collects Dan at Shannon Airport. Dan is now living happily in Toronto, and is on the verge of marrying a “big-featured” man named Ludo, a wealthy lawyer. Constance, who knows nothing about Ludo, nothing about the texture of her brother’s adult life, is living much as she was in 1997. And Dan knows little about his sister’s Irish existence. They are people linked only by memory, and the writing opens out magnificently to incorporate and memorialize that memory:


Dan was a year younger than Constance, fifteen months. His growing up struck her as daft, in a way. So she was not bothered by her brother’s gayness—except, perhaps, in a social sense—because she had not believed in his straightness, either. In the place where Constance loved Dan, he was eight years old.
He stood beside her as she sorted out the ticket, then they walked across the car park together, almost amused.
This was the boy who ran alongside her in her dreams. Constance, asleep, never saw his face exactly, but it was Dan, of course it was, and they were on the beach in Lahinch coming round a headland to find something unexpected. And the thing they found was the River Inagh as it ran across the sands into the sea. Sweet water into salt. Constance had been there many times as an adult, and the mystery of it remained for her. Rainwater into seawater, you could taste where they met and mingled, and no way to tell if all this was good or bad, this turbulence, if it was corruption or return. 
<-------->
When  Edward Follis was nineteen, he heard the Glenn Frey song “Smuggler’s Blues” on the radio. Three lines stuck in his head:

It’s propping up the governments in Colombia and Peru
You ask any DEA man,
He’ll say, “There’s nothin’ we can do . . .”

For weeks, he thought of little else. He talked about the song obsessively with his friends. Finally, he had a moment of clarity. As he recalls in his memoir, “The Dark Art: My Undercover Life in Narco-Terrorism” (Penguin/Gotham), written with Douglas Century, “I said to myself, Fuck it. I’m gonna become that DEA man. Let ’em try to tell me there’s nothing we can do.”

Follis joined the Drug Enforcement Administration after a stint in the Marine Corps, and from the moment of his first bust—when he posed as a buyer for a group of Mexican heroin wholesalers—he was “hopelessly addicted to undercover.” During the next three decades, he fought the drug war in Thailand, Mexico, and Korea, and rose to become the agency’s chief representative in Afghanistan. The cast of characters he met along the way could populate a movie set. There was Dragan, “a young Rutger Hauer, six-one, close-cropped blond hair and cobalt-blue eyes,” for whom the D.E.A. put together an entire warehouse of advanced weaponry in a drugs-for-arms deal. “His demeanor remained ice-cold,” Follis writes:

He didn’t say shit. Didn’t so much as nod. And he damn sure didn’t smile. I don’t think he was a white supremacist, but to me, he had an almost neo-Nazi appearance; he held your gaze for too long, and those blue eyes were chilling. I’ve learned with guys who look like that, guys who think they’re bad-asses, you don’t keep your distance from them. You move in closer.
Then, there was Kayed Berro, scion of the infamous Berro clan, from the Bekaa Valley, in Lebanon, and an alleged associate of the Pakistani heroin kingpin Muhammad Khan, who, Follis tells us, was widely feared and never seen, in the manner of “the Keyser Soze character in The Usual Suspects.” In Thailand, Follis went in search of the elusive Khun Sa, the opium warlord known as the Prince of Death. Follis became so fluent in Thai that his Thai girlfriend once exclaimed, after listening to him set up a meeting with a trafficker, “When I listen to you speak, I wouldn’t even know you were white.” He hauled duffelbags stuffed with five hundred thousand dollars in small bills through a secret passageway under Hong Kong International Airport. He stared down an ex-con named Mike, who pointed an Uzi between his eyes, wondering if Follis was who he said he was. (“What are you talkin’ about, Mike?” Follis fired back. “Think I’m a fuckin’ cop or something? . . . How could I be a cop? Listen, man, I’d be in fuckin’ jail for what I’ve done with you so far.”) When one of his informers was grabbed in Kabul, he picked up an M4 carbine, a Glock, and a bowie knife, and took off through the city’s streets in a scene worthy of the “Fast and Furious” franchise:

“Haji up,” I said. We threw on our UC garb: the white cotton tops of the shalwar kameez, black scarves around our faces, and two Massoud caps—tan-colored beret-like hats that were the favored headgear of the Lion of Panjshir himself. I was gunning the gas, on the edge, swerving the heavy armored Toyota as if I’d taken a straight shot of adrenaline. The streets of Kabul swarmed around us like a medieval bazaar. I had tunnel vision, oblivious to the thumping as the side mirrors of the Land Cruiser clipped pedestrians, knocking more than a few to the pavement. . . . Behind us, we heard angry shouting.

Follis wants us to know that the D.E.A. is as wily and tough as the drug traffickers it is sworn to catch:

“Great shop you’ve got—I hear you do nice work,” I said, turning to admire some of the luxury cars and SUVs they were kitting out.
“Yeah,” Ivan Espinoza said, stroking his sparse goatee.
“I also do nice work,” I said, offering a half smile.
“That’s what we hear.”
This is Follis describing the time he posed as an L.A. private eye turned hit man, hoping to be hired by a pair of Mexican drug traffickers looking to assassinate a D.E.A. special agent. Follis was in full private-eye-turned-hit-man costume: long ponytail, black loafers, button-down shirt, dress slacks. He continues:


Wasn’t the first time I’d played the role of a hit man. When you’re selling yourself as a killer for hire, you never start off saying anything too direct—“I can body that guy” or even “I can do him.” That’ll raise the bad guys up instantly.
You speak in an understood criminal code: innocuous-sounding phrases, half-finished statements, and knowing glances.
“I hear you have some issues here,” I said. “Heard you have an infestation.”
They nodded, warily.
I kept glancing around the tinting shop. “I’m the kind of guy—Well, I know how to eradicate disease.”
“Yeah?”
In the 1964 essay “The Paranoid Style in American Politics,” the historian Richard Hofstadter described the psychological characteristics of what he called “movements of suspicious discontent.” Such groups, he said, share an interpretation of history centered on personality. They focus on people, not systems, and the object of their suspicion is “clearly delineated: he is a perfect model of malice, a kind of amoral superman—sinister, ubiquitous, powerful, cruel, sensual, luxury-loving.”

Hofstadter observes, “It is hard to resist the conclusion that this enemy is on many counts a projection of the self; both the ideal and the unacceptable aspects of the self are attributed to him,” and he goes on:

The Ku Klux Klan imitated Catholicism to the point of donning priestly vestments, developing an elaborate ritual and an equally elaborate hierarchy. The John Birch Society emulates Communist cells and quasi-secret operation through “front” groups, and preaches a ruthless prosecution of the ideological war along lines very similar to those it finds in the Communist enemy. Spokesmen of the various fundamentalist anti-Communist “crusades” openly express their admiration for the dedication and discipline the Communist cause calls forth.

The paranoid crusader is not disdainful of his enemy. He is in awe of him. Hofstadter quotes that staunchest of cold warriors, Barry Goldwater: “I would suggest that we analyze and copy the strategy of the enemy; theirs has worked and ours has not.”

Follis began his career in the Marines, and the Marines operate by very different principles from those of the D.E.A. In a fascinating history, “Underdogs: The Making of the Modern Marine Corps” (2012), the historian Aaron O’Connell points out that, before the United States entered the Second World War, the Marine Corps was the least popular branch of the military services: only five per cent of young men considering a military career listed the Marines as their first choice. In response, members of the Corps’s leadership embarked on a course of identity creation. Working with Hollywood and the media, they recast the Corps’s reputation for boorishness and violence as an ethic of courage and loyalty. The Navy and the Army talked about equipment and technology; the Marines talked about character. The Army, by the end of the war, was known for its impersonal bureaucracy. The Marine Corps made itself the most family-friendly of the services, reaching out to the wives and the parents of enlisted men. Toys for Tots—which became one of the biggest children’s charities in the country—was a Marine Corps operation. O’Connell writes, “They privileged the collective over the individual, venerated sacrifice and suffering, and spoke often of their service’s unique sense of community.” The Marine Corps advanced its goals by differentiating itself from its competitors and adversaries.

But undercover work—which Follis believes to be at the core of the D.E.A.’s mission—is based on assimilation, not differentiation. The father of undercover police work, the early-nineteenth-century French detective Eugène-François Vidocq, was a former criminal. He began by selling his talents as an informer to the Paris police on the ground that it took a thief to catch a thief. When he formed the Sûreté Nationale—the plainclothes state-security police—he staffed it heavily with other ex-cons. Covert police action is based on the notion that sophisticated criminals operate according to a code and a logic that are inaccessible to an outsider. The criminal is brilliant and devious. We need to look and think like him in order to catch him. Theirs has worked and ours has not. Undercover policing, in its idealization and emulation of the objects of its suspicion, is paranoid.

In “The Dark Art,” all of Follis’s targets are amoral supermen. “He carried himself like a true Ibo prince: dignified, impeccably dressed in a tailored tan suit and gleamingly shined oxblood shoes,” Follis writes of a Nigerian drug dealer. We are told that Dragan, the young Rutger Hauer, had a doctorate. He came to one meeting wearing a “hand-tailored suit, with Italian loafers, striped silk tie.” Follis befriends a heroin trafficker turned D.E.A. informant known as Philip the Armenian: “Nobody I’d then met—and nobody I’ve met since—had the connections, the savvy, and the swagger of this guy. The Armenian was highly educated, knew seven languages—all of them like a native-born speaker.” It was through Philip the Armenian that Follis met Kayed Berro, who, he says, was then living in Southern California, finishing up a master’s degree in engineering at U.S.C. while running a vast heroin operation.

Follis would tail Berro from his house to the engineering library on the U.S.C. campus and sit patiently outside, awed by the long hours that his quarry spent there. Berro’s wife was an opera buff. He dressed beautifully. “He was a Lebanese Renaissance man,” Follis writes. “He spoke flawless English, and his Arabic was about as beautiful as any Arabic I’ve ever heard.”

To catch a man like Berro, you have to enter Berro’s world. “I knew I could match Kayed Berro’s smarts and sophistication, with a dose of my own street swagger,” Follis says. So he pulled his hair back in a ponytail again, and, from the D.E.A.’s fleet, picked out a candy-red Corvette that had once belonged to a major heroin dealer. “Driving it at high speeds on the freeway made me feel like a major trafficker,” he writes. Berro and Follis became friends. They shared confidences. “I’ve thought about it a lot in the intervening years,” Follis says. “What I admired about him, I suppose, is that I saw a lot of me in him.” It is hard to resist the conclusion that this enemy is on many counts a projection of the self.

Later in “The Dark Art,” Follis mentions something that happened when he was making a case against the Juárez cartel, in Mexico. Through one of his informers, he procured nine “crystal clear” audiotapes of two of the cartel’s leaders. Then the D.E.A. brass in Virginia accused him of staging the tapes, “complete with fabricated voices.” Follis shrugs off the episode. But it shows the extent of the paranoid culture at the D.E.A. Apparently, it was not difficult for Follis’s bosses to imagine that one of their most trusted agents might be tempted to slip into the role of Mexican drug lord. Nor was it difficult for them to imagine that someone versed in “the dark art” of undercover work could pull it off—could write, act in, and produce a nine-part set of audiotapes in which he staged the private conversations of two of Mexico’s biggest narcotics kingpins. You almost wonder whether Follis took the accusation as a compliment.

In 2006, the job of D.E.A. country attaché in Afghanistan opened up. Everyone in Follis’s family urged him not to apply. But Osama bin Laden was thought to be in Afghanistan, and Follis saw the country as ground zero in the war on drugs and terrorism. He woke one morning with a clear resolve:


You know what you need to do. If you have any chance to do this guy, Ed, . . . you’ve got to do this. Everyone else wants you to put your career first, but you’ve got to go to Kabul.
Afghanistan occupies a peculiar place in the international drug trade. As recently as 1980, it accounted for just two hundred metric tons of opium—a tiny fraction of the world’s production. That number rose to fifteen hundred and seventy tons in 1990, after the chaos of the Soviet occupation. But it was in the wake of the U.S.-led invasion of Afghanistan that the country’s position as the world’s center of opium production was solidified. By the time Follis got to Kabul, the country’s opium production was more than eight thousand tons a year—more than ninety per cent of the global output.

In a recent paper, three Norwegian economists—Jo Thori Lind, Karl Ove Moene, and Fredrik Willumsen—argue that there is a direct connection between war and the surge in Afghanistan’s drug economy. “Opium is more drought resistant than wheat, the main alternative crop, and opium does not require road transportation,” they write. “Military activities that destroy infrastructure such as irrigation and roads therefore make opium relatively more profitable.” The three prove their point by showing that, the more fighting there was in any particular region, the likelier its farmers were to switch from wheat to opium.

Because a large share of the opium profits were flowing to the Taliban, the United States instituted efforts to reduce opium production. But, as the economist Jeffrey Clemens has shown, those efforts were most effective in government-controlled areas and least effective in Taliban-controlled areas. So, as the U.S. spent more to eradicate poppies and to encourage farmers to plant other crops, the share of the opium trade that went to the Taliban increased. In 2004, Taliban regions accounted for forty per cent of Afghanistan’s poppy production; by 2010, that share had risen to ninety per cent. In other words, the fighting in Afghanistan accelerated the country’s drug trade, which enriched the Taliban, which caused the U.S. to launch an effort to eradicate poppy cultivation, which enriched the Taliban still further, which caused the U.S. to step up its assault on the Taliban’s territory, which caused more farmers in Taliban territories to switch from wheat to opium, which accelerated the drug trade.

This was the mess that Follis inherited when he arrived in Afghanistan. But in “The Dark Art” there is little consideration of the broader context of the war on drugs. When Follis describes his work in Mexico, he speaks of the fact that many tens of thousands of Mexicans have been killed in the drug war there. It never occurs to him that the war on drugs, which had consumed his whole career, might have contributed to that violence. In Afghanistan, his job was to control a drug trade fuelled in part by his own country’s attempts to control the drug trade. But that paradox does not seem to interest him. As Hofstadter writes, “The paranoid’s interpretation of history is distinctly personal: decisive events are not taken as part of the stream of history, but as the consequences of someone’s will.” Follis’s world is not shaped by markets and incentives and institutional choices. It is the product of bad guys in fast cars and sharp outfits.


So what does Follis do in Kabul? He ends up befriending one of the baddest of all Afghanistan bad guys, a “mountain of a man” named Haji Juma Khan Mohammadhasni. HJK, as Follis calls him, was a close associate of Osama bin Laden and Mullah Omar. The D.E.A. suspected him of giving hundreds of millions of dollars to the Taliban. Follis would meet him for long dinners at HJK’s favorite Persian restaurant in Kabul, where HJK would eat one kebab after another and speak learnedly of Afghan history and culture. When HJK had a cancer scare, Follis whisked him to Washington for treatment. They watched “The Passion of the Christ” together on Follis’s laptop. They talked about God and their faith and puzzled over Jesus’ saying, “My father, if it is possible, let this cup pass from me.”

“Strange as it sounds, the hours I spent undercover with HJK were becoming a source of solace: an escape from the stress of embassy politics, the constant infighting and war of wills,” Follis writes. As embassy life grew more strained, he began to have fantasies of a simpler existence: “Half waking, half dreaming, in my single bed in the embassy, I could see myself running away to live the rest of my life with HJK.”

When one of the top C.I.A. agents in the region accused Follis of withholding crucial intelligence that he had gleaned from his undercover sources, Follis came close to the brink. HJK would never treat him that way:

Little wonder then that I’d lie there in my Spartan bedroom almost every night, dreaming of running away with HJK, lost in clouds of desert dust, rolling through those badlands in caravans of SUVs. Sounds strange but—compared to the amorality and treachery of these spooks—I felt more at ease in the world of guys like HJK. . . .
There was a warmth to his manner, openness in his laughter, a sophisticated charm. A skeptic could say that it was all charm—a master manipulator at work. But I’d like to believe there was something deeper at play.
Follis watched HJK fingering his iridescent prayer beads and recalled the hours he’d spent with his rosary beads, growing up in St. Louis: “I’d inwardly smile as he did it, too, thinking, This guy is just a version of me.”

The final section of “The Dark Art” is entirely about the unravelling of Follis’s relationship with HJK. Follis learns that the Pentagon has put HJK on its hit list. Frantic, Follis asks for ninety days. He gets sixty. He telephones HJK and tells him to come to Jakarta. The D.E.A. charters a Gulfstream V. The two friends meet at the airport and share a bittersweet reunion, and then Follis hustles HJK onto the Gulfstream for the long ride back to the United States, where he arrests him. “I have to take his life away from him to save his life,” Follis explains. (HJK has contested the charge against him and is awaiting trial.)

Culture, O’Connell writes, serves to differentiate and discipline: “it operates by policing the boundaries within and between groups. It provides signs and rules for what is ‘inside’ or ‘outside,’ normal or abnormal.” But undercover cultures have no signs and rules, no bright line between normal and abnormal. The hunter dresses like his prey. They fall in love. And, when the relationship comes to an end, hearts are broken.

“In my right ear I felt the kiss-like warmth, his breath faintly scented with roasted cauliflower and jasmine tea,” Follis writes of a cherished moment that he and HJK spent together. They were in a mosque in Kabul. They had prayed, side by side, on their knees, for more than an hour. Then HJK—alleged opium king and friend of the Taliban—embraced him:

For the first time he used the Dari word to address me.
Baradar.
Even today, I can feel the wiry barbs of his black beard pressing into my cheeks.
“You’re more than a friend, Ed,” he said. “I love you today as my brother.” 
<-------->
For  the schoolteacher, the changes had come slowly. First, his walking had grown unsteady; then his hearing had worsened. He had become stooped, and had begun walking with a cane, even though he was only in his late fifties. Now he sat with his wife and son in the consulting room of Henry Marsh, a London neurosurgeon, looking at a scan of his brain, which showed a tumor growing near the base of his skull. The question was whether it could, or should, be removed. Marsh, who had been practicing neurosurgery for only a few years, was unsure. The tumor was massive—he was startled by its size—and it was situated in the brain stem, a vital area. Left to itself, it would destroy the schoolteacher’s hearing, rob him of his ability to walk, and, eventually, kill him. But, Marsh explained, surgery could leave him paralyzed, or worse. The family faced a difficult choice, between the certainty of a slow, predictable decline and the possibility of an immediate cure—or catastrophe.

They decided to seek a second opinion from an older, eminent neurosurgeon. A few days later, the surgeon phoned Marsh. “It’s a young man’s operation,” he said. “I’ve told them you should do it.” Flattered, Marsh agreed to go ahead. The surgery began at nine in the morning and continued late into the night. Brain surgery is slow and dangerous, and removing a tumor can be like defusing a bomb. Often, surgeons look through a microscope and use long-handled, fine-tipped instruments to pull the tumor away from the brain before removing it with a sucker. A quarter of the body’s blood courses through the veins and arteries of the brain; if one of them is torn, bleeding and stroke can result. It’s also possible to remove important parts of the brain by accident, because brain tissue and tumor tissue look pretty much the same. Unlike the rest of the body, the brain and the spinal cord rarely heal. If a neurosurgeon makes a mistake, the damage is often permanent.


By midnight, Marsh and his team had removed almost all of the tumor. The atmosphere in the operating theatre was relaxed and celebratory; the surgical team paused for cigarette breaks and listened to Abba and Bach. “I should have stopped at that point, and left the last piece of tumor behind,” Marsh writes in his memoir, “Do No Harm” (Thomas Dunne). Instead, he ventured further—he wanted to be able to say that he had taken it all out. “As I started to remove the last part of the tumor,” Marsh writes, “I tore a small perforating branch off the basilar artery, a vessel the width of a thick pin. A narrow jet of bright red arterial blood started to pump upwards.” The basilar artery carries blood to the brain stem, which regulates the rest of the brain. Marsh quickly stopped the bleeding, but the oxygen deprivation was enough to irreparably damage the man’s brain stem, and he never regained consciousness.

Marsh, who is now sixty-five, is one of Britain’s foremost neurosurgeons. He is a senior consultant at St. George’s Hospital, in London, and he helped to pioneer a kind of surgery in which patients are kept awake, under local anesthesia, so that they can converse with their surgeons while they operate, allowing them to avoid damaging what neurosurgeons call “eloquent,” or useful, parts of the brain. Marsh has been the subject of two documentary films. Still, he writes, “As I approach the end of my career I feel an increasing obligation to bear witness to past mistakes I have made.” A few years ago, he prepared a lecture called “All My Worst Mistakes.” For months, he lay awake in the mornings, remembering the patients he had failed. “The more I thought about the past,” he recalls in his book, “the more mistakes rose to the surface, like poisonous methane stirred up from a stagnant pond.”

There’s a tradition of physicians writing about their errors. “When the Air Hits Your Brain,” a neurosurgical memoir by Frank Vertosick, Jr., begins with a scene in which a resident, while drilling a hole in a man’s skull, accidentally goes too far, plunging the drill bit into the brain. “Oh, shit!” he exclaims. (An older doctor reassures him: “It’s just the lateral hemisphere.”) Physician writers usually view such errors with a generous spirit. They point out that medicine is built on mistakes, because doctors, like the rest of us, learn by screwing up.

Marsh isn’t interested in the usefulness of error. He is the Knausgaard of neurosurgery: he writes about his errors because he wants to confess them, and because he’s interested in his inner life and how it’s been changed, over time, by the making of mistakes. As an epigraph to “Do No Harm,” he quotes the French doctor René Leriche: “Every surgeon carries within himself a small cemetery, where from time to time he goes to pray.” Marsh knows there’s something unprofessional about this inwardness—a surgeon’s emotions are supposed to be beside the point compared with his patients’ suffering—but he is drawn to “reckless honesty.” (When he delivered “All My Worst Mistakes” to an audience of neurosurgical colleagues, he writes, “it was met by a stunned silence and no questions were asked.”) “Do No Harm” is an act of atonement, an anatomy of error, and an attempt to answer, from the inside, a startling question: How can someone spend decades cutting into people’s brains and emerge whole?


Marsh became a neurosurgeon almost by accident. Midway through his undergraduate years, at Oxford, he fell in unrequited love and, inspired by the Jack Nicholson movie “Five Easy Pieces,” fled to Newcastle, in the rugged northeast of England, to nurse his broken heart. There, he wrote bad poetry, worked as a hospital porter, and saw his first surgery. “I found its controlled and altruistic violence deeply appealing,” he writes. After he finished his degree, in 1973, he entered the Royal Free Hospital School of Medicine. Students weren’t allowed into the neurosurgical theatres, but one day Marsh caught a glimpse through a round porthole in a closed door—“a naked woman, anaesthetized, her head completely shaven, sitting bolt upright on a special operating table.” The image stayed in his mind, and struck him as “a scene from a horror film.”


Marsh married, and qualified as a doctor. Not long afterward, his three-month-old son, William, developed a tumor in the center of his brain and successfully underwent surgery to remove it. Marsh feels now that he didn’t fully appreciate the risks: he writes that, much later, “I watched a child bleed to death in the very same operating theatre where my son had been treated, as my boss—the very surgeon who had saved my son’s life—now failed with a similar tumor.” Soon after his son’s surgery, while working in intensive care, Marsh observed an aneurysm operation. The surgeon had to make his way deep into the brain, exposing the small, deadly balloon of arterial blood so that, without rupturing it, he could seal it off using a miniature metal clip. It “was more like a blood sport than a calm and dispassionate technical exercise,” Marsh writes. It also “involved the brain, the mysterious substrate of all thought and feeling. . . . The operation was elegant, delicate, dangerous, and full of profound meaning. What could be finer, I thought, than to be a neurosurgeon?” Neurosurgery—strange, brutal, and miraculous—had seduced him, and he started the training as soon as he could.

Marsh is fascinated by the brain. He loves looking at it through his counterbalanced surgical microscope, which “leans out over the patient’s head like an inquisitive, thoughtful crane.” To Marsh, the view is beautiful. At the center of the brain, he writes, the internal cerebral veins are like “the great arches of a cathedral roof”; the Great Vein of Galen can be seen “dark blue and glittering in the light of the microscope.” It is “a very private view,” “clearer, sharper and more brilliant than the world outside,” and “made all the more intense and mysterious by my anxiety.”

That anxiety begins long before surgery, with the decision to operate in the first place, which could easily be wrong. (A brain scan is mute on the all-important question of how tightly a tumor will cling to the brain.) It continues through a series of meetings in which Marsh must try to explain that uncertainty without alarming his patients. (It’s tempting to be reassuring, he writes, but after failed operations he has “bitterly regretted having been too optimistic.”) Bicycling to the hospital, Marsh is oppressed by dread—“almost a feeling of doom”—and, before surgery, he is often seized by panic, which is swept away, at the last moment, by “fierce and happy concentration.”

Brain surgery itself, Marsh writes, is “something I hate doing.” Beforehand, patients are depersonalized—their heads are shaved, and they are covered in sterile drapes—although you can’t entirely depersonalize the brain. Often, there’s a question about how far to go: if an aneurysm clip is not quite perfectly positioned, should Marsh take the risk of repositioning it? To do so, he must struggle against the “urge to finish the operation and escape the fear of causing a catastrophic haemorrhage.” Eventually, he writes, “I decide at some unconscious place within myself, where all the ghosts have assembled to watch me.”

Cartoon
“I’ve made my fortune, and now it’s time for me to run for office and consolidate it.”
BUY THE PRINT »
Neurosurgical disasters can be cruel. A patient can wake up and appear healthy only to die, a few days later, of a stroke or a hemorrhage that’s related, in “some unknowable way,” to the operation. And patients can live on despite severe brain damage—an outcome that’s a particular source of fear for Marsh. He tells a colleague, “Nobody, nobody other than a neurosurgeon, understands what it is like to have to drag yourself up to the ward and see, every day—sometimes for months on end—somebody one has destroyed and face the anxious and angry family at the bedside.” The schoolteacher lived on in just this way. Seven years after that failed surgery, Marsh was visiting a home for vegetative patients when he looked into a room and “saw his grey curled-up body in its bed.” Of the feelings such experiences produce in him, Marsh writes, “I will not describe the pain.”

In his decades of medical practice, Marsh has been a witness or a party to almost every kind of mistake. There are errors of commission (the hubristic removal of too much tumor) and of omission (the missed diagnosis). There are errors that go unreported (after a successful surgery, Marsh might decide not to tell a patient about a close call) and errors for which Marsh is held accountable. (He writes that, after one operation, “I told them to sue me. I told them I had made a terrible mistake.”) There are errors of delegation—as when Marsh allows a resident to perform a simple spinal surgery, and the patient is left with a paralyzed foot—and historical errors: at a mental hospital, Marsh encounters victims of lobotomy. One morning, Marsh operates after having a petty argument with another surgeon, and the operation paralyzes half the patient’s face. He writes, “Perhaps this was going to happen anyway—it is called a ‘recognized complication’ of that particular operation—but I know that I was not in the right state of mind to carry out such dangerous and delicate surgery, and when I saw the patient on the ward round in the days afterwards, and saw his paralyzed face, paralyzed and disfigured, I felt a deep sense of shame.”

In a 1976 essay, the philosopher Bernard Williams explored a concept that he called “moral luck.” Often, he observed, we are morally responsible for actions that contain an element of chance. Imagine two people who drink too much at the same party, and who both drive home drunk; suppose that one of them hits a pedestrian. The driver in the accident is morally responsible for this outcome, and yet only chance distinguishes him from the other driver. Much of moral life, Williams thought, contains a similar element of luck. We happen to find ourselves in situations that bring judgment upon us. Yet this doesn’t absolve us of responsibility for what we do. It underscores an unsettling fact about moral life—that the distribution of moral fault in the world depends, in many ways, on good and bad luck.

A soldier’s life is deeply shaped by such moral luck. So, it turns out, is a neurosurgeon’s. “As I become more and more experienced it seems that luck becomes ever more important,” Marsh writes. Even so, he will be blamed for what goes wrong and praised for what goes right—treated as a murderer in the morning, by one family, and as a savior in the afternoon, by another. People who are regularly exposed to moral luck often find it helpful to have some standard other than morality by which to judge themselves—a code, more or less. Marsh’s code has to do with his own emotions. If he can’t control how a surgery turns out, he will control how he feels. He tries not to let his feelings add to his patients’ fear and unhappiness; at the same time, he tries never to lie. He yearns, therefore, for feelings that are strong but realistic, fully voiced yet even-keeled. In one of the book’s most moving passages, he is called to the bedside of a favorite patient, David, a warm, accomplished, and intelligent man, whom he has known for twelve years. Marsh has fought David’s tumor in three surgeries, but now it has reached a deeper, fatal stratum of the brain. Marsh explains, with great sadness, that a fourth operation won’t do any good; David says that he’s suspected as much. Marsh holds David’s hand, is embraced by his wife, and says, “It’s been an honor to look after you.”

Given the circumstances, it’s an ideal meeting. And yet, afterward, Marsh’s emotions rebel. Leaving the hospital, he writes,

I quickly became stuck in the rush-hour traffic, and furiously cursed the cars and their drivers as though it was their fault that this good and noble man should die and leave his wife a widow and his young children fatherless. I shouted and cried and stupidly hit the steering wheel with my fists. And I felt shame, not at my failure to save his life—his treatment had been as good as it could be—but at my loss of professional detachment and what felt like the vulgarity of my distress compared to his composure and his family’s suffering, to which I could only bear impotent witness.

In writing “Do No Harm,” Marsh has seemingly violated his code: he expresses many of the feelings that he’s worked very hard to keep hidden. But codes, by their nature, exclude the complexities of inner and moral life, and Marsh wants to understand himself—and wants us to know him—in the light of those complexities.

Marsh writes like a novelist—he thinks in terms of scenes, patterns, and contrasts—and, reading “Do No Harm,” I thought of another Henry: Henry Perowne, the neurosurgeon protagonist of Ian McEwan’s novel “Saturday.” (In writing his book, McEwan shadowed a younger English neurosurgeon, Neil Kitchen.) The two Henrys could not be more different. Perowne, who is in his late forties, is confident and optimistic. In his surgeries, he says, he can “control outcomes”; he experiences “the pleasure of knowing precisely what he’s doing.” He admires the impersonality of scientific knowledge. He enjoys “the relief of the relatives when he comes down from the operating room like a god, an angel with the glad tidings—life, not death.” Most of his patients survive, and even, McEwan writes, “thrive.” Presumably, the same is true of Marsh’s patients. The difference is one of temperament. “It’s not the successes I remember,” Marsh writes, “but the failures.”

Years ago, when I read “Saturday,” I was in awe of Perowne. Now that I’ve read Marsh’s memoir, the character comes across as curiously unburdened by his work. (The novel imagines Perowne humbled, but by forces—time, evil, history—that lie outside the surgical theatre.) Perowne has, apparently, never done what Marsh did to the schoolteacher, whose story Marsh tells in a chapter called “Hubris.” That experience changed Marsh, professionally and spiritually. He no longer operates for so long at a stretch. He has become wary of his own optimism and talent, and suspicious of the exhilarations of surgery. (“I can no longer bear to listen to music while operating,” Marsh writes; Perowne listens to the Goldberg Variations.)

The Henry Marsh of “Do No Harm” is a character, too. In 2007, the documentarian Geoffrey Smith made a film about Marsh, titled “The English Surgeon.” It seems to star a slightly different man. In the film, Marsh is goofy; he’s very tall, and wears bold, perfectly round glasses. When he talks about medical equipment, he becomes boyish. At a frozen lake—the documentary takes place in Ukraine, where Marsh has been doing pro-bono brain surgery for decades—he slides across the ice with ease. If he’s nervous before an operation, his voice rises and he grabs his head. He smiles regularly. When he delivers bad news, his eyes fill with tears: “Life can be very cruel,” he says, “I’m sorry.” It’s obvious that he’s an emotional man—the sort who might leave school to nurse a broken heart. At one point, Marsh visits Katya, the mother of a young girl whose life he tried to save. Marsh describes the scene in “Do No Harm”: sitting at her dinner table, surrounded by her family, “I was so intensely moved to see Katya again that I could scarcely talk,” he writes. It’s remarkable that such a sensitive man has become a brain surgeon. There, too, age may play a role: “I became hardened in the way that doctors have to become hardened,” Marsh writes, but “now that I am reaching the end of my career this detachment has started to fade.”


In Kiev, Marsh works with a neurosurgeon named Igor Kurilets to perform state-of-the-art procedures with second-hand surgical equipment. In “Do No Harm,” Marsh writes about the terror of operating in a strange place, with substandard equipment, but he can’t quite bring himself to describe his work there accurately. (It’s heroic.) His self-portrait, in short, leaves something out. Marsh writes that, when speaking with patients, he struggles to find the balance between “hope and reality,” “optimism and realism,” “detachment and compassion.” He also struggles to find that balance in writing about himself.

Why should that be? The darkness of Marsh’s book isn’t a kind of false modesty; his self-abnegation isn’t disguised self-regard. Instead, his desire for atonement seems to darken his recollections—faced with the irrevocability of his patients’ suffering, he is unable to escape from its shadow. And the memoir’s final chapter suggests a further possibility. Marsh writes about a woman who comes to see him in his clinic. Twenty years earlier, she had a benign brain tumor removed; even as the operation saved her life, it severed one of her facial nerves. Surgeons call this kind of trade a “sacrifice.” In most people, the result of this sacrifice would be a numbness of the face, with which they come to terms. Only a few, Marsh writes, are, like the woman, “driven mad by the numbness.” The Latin name for this, he says, is “anaesthesia dolorosa—painful loss of feeling”; the final chapter is named for that condition. Marsh, I think, is afraid of anaesthesia dolorosa. He can’t bear the thought of going numb. He is determined to feel as much as he can. 
<-------->
A self-portrait by the American poet Terrance Hayes graces the cover of “How to Be Drawn” (Penguin), Hayes’s fifth book of poems. If you want to be drawn, one straightforward plan would be to draw yourself, as Hayes has done; change the word to “represented,” and the political meanings of his title become clear. Hayes is black. In American poetry, if a black person wants to exist at all, he can either submit to representation by white artists or choose to portray himself. But words are trickier than charcoal and pencil: Hayes can’t make a poem that “looks” like Terrance Hayes, by the standards of visual art, since “Terrance Hayes,” by the standards of poetry, doesn’t exist until his words invent him. Authors, after all, aren’t causes; they’re effects produced by their own language.

Hayes is forty-three and lives in Pittsburgh, where he is a professor of English at the University of Pittsburgh*. In 2010, his volume “Lighthead” won a National Book Award, and last year he received a MacArthur “genius” grant. He played basketball for Coker College, in South Carolina, where he was an Academic All-American, but he has the bounding imagination of someone fortified and defended, for years, by shyness. If you judge a poem by how big a chunk of reality it smuggles into language before returning it, transformed, you will have a hard time beating this catalogue from “Wigphrastic”:


Nonslip polyurethane patches, superfine lace,
Isis wigs, Cleopatra wigs, Big Booty Judy wigs
under the soft radar-streaked music of Klymaxx
singing, “The men all pause when I walked into the room.”
An ekphrastic poem is one that describes a work of art; “Wigphrastic” describes Ellen Gallagher’s “DeLuxe,” a portfolio of sixty works on paper that depict, among other things, vintage ads for hair straighteners and skin whiteners. You can see the piece, and explore all of Hayes’s references, on his Web site. If the Internet had been around when T. S. Eliot wrote “The Waste Land,” the idea of literary difficulty might have been moot.

Hayes is a poet of swallowed garrulity, imagined riposte, mock correction, and interior litigation. We all have, in our heads, a marionette theatre where we stage what we might have done and should have said. There we are always the conquering puppet. Hayes’s poems are like a Pixar version of the mental marionette show, a dazzling space crammed with comic jabs. “Black Confederate Ghost Story” recalls a hick “handyman’s / insistence that there were brigades of black / Confederates.” Hayes replaces his actual, too polite response with a B-movie horror sequence:

Attention, African-American apparitions hung,
burned, or drowned before anyone alive was born:
please make a mortifying midnight appearance
before the handyman standing on my porch
this morning with a beard as wild as Walt Whitman’s.
Except he is the anti-Whitman, this white man
with Confederate pins littering his denim cap and jacket.
(And by mortify, dear ghosts, I mean scare the snot out of him.)
The tone is taken partly from those “Attention, shoppers” announcements heard in supermarkets; instead of flocking to the produce aisle, these “ghosts,” literally “mortified” by having been murdered, are summoned to metaphorically “mortify” this cartoonish dolt. I never noticed before that Whitman’s name is very nearly “white man”: the poem, like all of Hayes’s poems, operates by swift cuts and screens until it finds an opening. Hayes imagines a “tolerant” Whitman “waltzing across the battlefield like a song / covering a cry of distress”; Hayes himself wants to be “a storm / covering a Confederate parade.”

Racial trauma is everywhere in Hayes’s work, instantiated by his personal ghosts—an absent father, a mother who worked as a prison guard, an array of family troubles and damage. But he is brilliantly boxed in by his style, which elates in the language it finds to express tragedy. Hayes has called himself “a gray-area, between-area person”; his poems refuse black-and-white emotions. I have no idea how he works, but the poems give the impression of spontaneity; even if he labors over them, the result is a wild ride without an off switch, an unbroken verbal arc propelled by his accelerating actions of mind. The poem “How to Be Drawn to Trouble” starts out as a tribute to James Brown, “stoned on horns and money,” who was briefly an inmate in the prison where Hayes’s mother worked. By its close, Brown’s song “Please, Please, Please” has gone from soundtrack to sing-along, as Hayes recalls a searing night from his past. His mother has “gone out Saturday night, / and come home an hour or so before church”:


She punched clean through the porch window
When we wouldn’t let her in. I can still hear all the love buried
Under all the noise she made. But sometimes I hear it wrong.
It’s not James Brown making trouble, it’s trouble he’s drawn to:
Baby, you done me wrong. Took my love, and now you’re gone.
Those lyrics are at once sung by Brown, cried by Hayes’s father, and written by Hayes. So much of life is an uncanny acting out of emotions that we first encounter in art, a notion that Hayes’s verse, in which the poet quotes his father’s quotation of James Brown, explores with extraordinary power.

Hayes’s titles often set up arbitrary collisions, self-imposed restraints, hodgepodge high-wire ideas: “Portrait of Etheridge Knight in the Style of a Crime Report,” “Instructions for a Séance with Vladimirs,” “Some Maps to Indicate Pittsburgh.” All of these poems, foregrounding their own eccentricity, choose rather rigid homemade forms and then stick to them. If the past is prologue, we may now see a rash of poems in the style of a crime report: Hayes is one of a small number of contemporary poets who have invented forms that actually caught on. My favorite is a form that predominates in “Lighthead,” an adaptation of the Japanese slide-show format used for business presentations, called pecha kucha: twenty slides shown for twenty seconds each. In Hayes’s hands, short poems take the place of slides (each can be read in about twenty seconds); the result is a total overhaul of linear narrative, a story with twenty beginnings and twenty endings.

In poetry, form and feeling relate in countless unpredictable ways. The risk with Hayes’s work, which fits strong emotions into virtuoso forms, is that the emotions may also come to seem virtuosic. The poems handle form so deftly that they sometimes seem backfilled with feeling, as though Hayes is afraid of his own aplomb. But the greatest poets can use their style as a way to see past it. Hayes is good enough that we want from him even more, which may mean, in his case, even less: fewer turns of mind, fewer formal tricks and contrivances. I realize that I am in the unenviable position of telling him that he ought to have less fun on the page. Hardly anyone who reads him will agree with me.

Deborah Landau’s new book, her third, is “The Uses of the Body” (Copper Canyon). Many of her previous poems dealt with the accommodations made, in daily life, for fantasy, especially for sexual fantasy. They had a wonderful close-up strut and naughtiness, but you couldn’t really tell what was in the writer’s heart; the poems starred their speakers, whose performances were no less showy for being so personal.

Landau, who directs the Creative Writing Program at N.Y.U., has found an insidiously catchy music in “The Uses of the Body.” It’s like weaponized vers de société. Here is a section from “The Wedding Party”:

Oh, skin! What a cloth to live in.
We are not at the end of things.
He’s tuxedoed and I’m in a cocktail dress.
How gussied up we get.
Drink this, roll that.
Another sender different gender.
We’re going to hit a winner.
We’re going to swallow vodka
and slap down money
and stand around frocked and gossiping
and bleed a little in the bathroom
from earlier today when we were a little minx.
(He really is of the masses, mama said.)
The phrases—“gussied up,” “slap down money,” “hit a winner”—are outtakes from the fifties flicks that many weddings still absurdly resemble. The brutality sneaks in sideways, especially in the shrewd deployment of that creepy “we” into which the “I” seems to have been forcefully conscripted. This was supposed to be a wedding night—what are we doing downing vodkas in a casino?

There are several bodies in “The Uses of the Body”: a woman’s, torn between sexual “urge” and the “mandate” that keeps it in check, resisting the “somber hungry forcefield” of men’s gazes; the body of a sick friend, “frayed” and “decayed” before he dies and is “removed // from it promptly and with force”; the body of a fetus, “pale and puny,” who “welled inside me // without visa without a pretty box / dollface-down” or “bald and silverfisted” on an ultrasound. Poetry, too, is a body, built to last—“butchered,” as Allen Ginsberg wrote, out of poets’ ”own bodies” and “good to eat a thousand years.” It makes a deathless sustenance out of waste and loss. The uses of its body are never clearer than when it lists the uses of ours:

The uses of the body are heavy and light.
Bellinis, cradles, carousels.
Biopsies, sobriety, sensible shoes.
I am cozy, I am full of want until chest pain,
until a heavy cramp. The pain of form.
See how caught up we are
in our habitual flying patterns
until we have to look the unfair doctor in the eye.
The genitals are irrelevant then.
Dr. Rutkowski, what was it you said?
“The pain of form” is an odd outburst here, as though the poem has borrowed its conflicted relationship with its body from the poet’s ambivalence toward her own. Landau gives us the sublime feeling that formal accomplishment comes with a steep cost. Art uses us; it may even use us up. 
<-------->
Writers aiming to tell us about human life have often done so under cover of telling us about animals. Animals are fun—they have feathers and fangs, they live in trees and holes—and they seem to us simpler than we are, so that, by using them, we can make our points cleaner and faster. With Madame Bovary, you pretty much have to say who her parents were. With SpongeBob, you don’t, and this keeps the story moving. Most important, the use of animals to stand in for human beings creates a fertile ambiguity. We know that the author is not proposing a one-for-one equivalence between human and nonhuman life, but some kinship is certainly being suggested. Think of Swift’s Houyhnhnms, trotting down the road, their withers shining in the sun, saying sober, passionless things to Gulliver. How beautiful they are, and how creepy. Animal narratives have allowed writers with lessons on their mind to make art rather than just lessons.

Such tales are no doubt as old as animal paintings on cave walls. The earliest evidence we have of them is the beast fable, a form that is said to have come down to us by way of Aesop, a Greek storyteller who was born a slave in the sixth century B.C. Actually, no solid evidence exists that there ever was an Aesop, any more than there was a Homer. As with the Iliad and the Odyssey, we are talking about manuscripts that date from a period much later than the supposed author’s, and were probably assembled from a number of different fragments. In any case, a beast fable is a very short story (the Penguin Classics edition of Aesop renders “The Tortoise and the Hare,” perhaps the most famous of the fables, in five sentences) in which, typically, a couple of animals with the gift of speech learn a lesson from their dealings with one another. This moral is then stated at the end of the fable, and it is usually of a cautionary variety: don’t eat too much, don’t brag, watch out for this or that. As early as the third century B.C., these stories were being gathered together in various editions, usually for children, to teach them Latin (most were in Latin until the late Middle Ages) and some basic rules about life.


Eventually, in continental Europe, a more complicated kind of animal story, the “beast epic,” grew up alongside the beast fable. Beast epics used some of the Aesopian material, but they were much longer and more novelistic. They dispensed with the great Noah’s ark of generic animals that we see in a collection of beast fables: a duck, a goat, a frog, an ass, etc. Even a good-sized beast epic features no more than perhaps a dozen types of animal, each represented by only one or a few individuals, with names and rudimentary personalities. In the typical epic, the star is a fox—Reynard, Renard, or whatever, depending on the language—with his unwavering wiliness. Dominated by that slippery character, the beast epic no longer makes it altogether clear what lesson we are learning, or whether we should be learning it.

The fox epic was imported into England by William Caxton, the man who set up the first English printing press. In 1481, Caxton brought out “The History of Reynard the Fox,” a translation—by him, into his late Middle English—of what was basically a thirteenth-century Dutch version. By 1700, this had been followed by twenty-two further editions. Given the prevailing literacy rates, such a sales record qualifies the book, in the words of the Harvard medievalist James Simpson, as a “runaway best-seller.” This, Simpson says, is because its cold satire “answered to the intensely competitive, materialist conditions” of the time. Perhaps in the belief that such conditions still hold, Simpson has produced his own translation of Caxton’s “Reynard the Fox,” and Liveright has just published it.

Like most beast epics, the story begins at court, with the animals more or less standing in line to report to their monarch—King Noble, a lion—the crimes of Reynard the Fox. He stole a sausage from me, Courtoys the Dog says. He ate eleven of my chicks, Chaunticleer the Cock says. And so on. The King’s council decides that Reynard must answer these charges, and Bruin the Bear is sent to fetch him. When Bruin arrives at Reynard’s hideout, Reynard says all right, he’ll go, but first, would Bruin like a honeycomb? If he would, he should stick his snout into that cleft log over there. Bruin does so, and the log snaps shut on his head. By the time he extricates himself, the skin on his face has been torn off, together with his ears. Blood is gushing into his eyes. He can barely make his way home. The King, indignant, sends a second emissary, Tybert the Cat. Tybert returns minus one eye. Finally, on the third summons, Reynard decides that it would be wise to present himself to the King.


At court, he is condemned to death. He asks if he may confess his sins, and in the course of his recital he mentions that he has “so much treasure, both silver and gold, that seven carts wouldn’t be able to carry it.” Wait a minute, the King says. What’s this about treasure? Well, Reynard answers, it was part of a scheme that Bruin and Isengrim the Wolf and others were cooking up, to unseat the King. The King has Bruin and Isengrim arrested. Then he orders Reynard to take him to where the treasure is. Reynard says he’s sorry, but he can’t. He did something to annoy the Pope, and he has to go to Rome to secure forgiveness. But you’ll find the treasure easily, he tells the King, and he gives him some completely bewildering directions. (“To the west of Flanders there’s a forest called Hulsterloe, and a lake called Krekenpit nearby. . . . When you come to Krekenpit, you’ll find two birch trees.”) Reynard then leaves, going not to Rome but to his house. He invites Cuwaert the Hare to join him on his travels, adding that, since he is a carnivore and Cuwaert is a herbivore, there will be no competition between them for food. It does not occur to Cuwaert that he might be a carnivore’s food. When they arrive, Reynard sinks his teeth into Cuwaert’s throat. Reynard’s wife rushes up to drink the hare’s blood.

Let this synopsis stand for the rest: Reynard preys on the other animals; the King has him hauled in; Reynard saves himself with an outrageous lie; the King discovers it and hauls him in, again and again. Finally, Isengrim the Wolf challenges Reynard to hand-to-hand combat, before the whole court. The fight is ferocious. Reynard tears out one of Isengrim’s eyes. Then Isengrim pins him to the ground and seems ready to go in for the kill when Reynard reaches up and grabs his testicles, “twisting them so violently that the wolf howled. . . . He spat blood and shat himself” before fainting dead away. Reynard is declared the victor. Indeed, the King, forgetting all Reynard’s crimes, appoints him his chief counsellor: “In all the land you’ll be sovereign above all the others.” Reynard returns home in triumph.

“Reynard the Fox” is loose structurally. It is serial, episodic, ABCD, with events lined up like bottles on a shelf, rather than interlocking, leading to and away from one another by causal relations or any relations. Early in the book, Reynard confesses his sins to Grimbart the Badger as the two are travelling to court. Fifteen chapters later, they are travelling to court again, and Reynard says that he had forgot something when he made the first confession. He then takes more than two pages to tell of another sin—a story that, while perfectly O.K., differs in no important way from the ones he told before. (It’s about how he arranged for Isengrim to get kicked in the head by a horse.) There are riddles and digressions and repetitions. There are countless unnecessary explanations. When the fox is telling the King and the Queen what we know to be a lie, the author interrupts to say, Look how the fox is telling the King and Queen a lie!

This sort of where-was-I narration is common in very old literature, especially in works that were cobbled together from pieces, and most especially when oral tales were among those pieces. (The canonical Gospels are perhaps the best example.) Our notion of a clean-lined narrative did not develop until well after the Renaissance. Most of the stories in the world have nothing to do with it, and that is not to speak of stories that, like “Reynard,” are satires. “Reynard,” whatever its narrative logic or its over-all literary worth, is one of the defining documents of a vast tradition in Western art, indeed, in Western consciousness: the trickster tale.


In literature, you can trace the wily fox not only through the beast fables and epics but also in plays, the classic being Ben Jonson’s “Volpone” (the name means “big fox”), about a pack of legacy-hunters. There are trickster foxes in musical drama—for example, Stravinsky’s chamber cantata “Renard” (1916), the basis for ballets by Bronislava Nijinska, Serge Lifar, Balanchine, and others. Wes Anderson’s movie “The Fantastic Mr. Fox” (based on the Roald Dahl book), about a chicken-stealing fox with the voice of George Clooney, was nominated for two Academy Awards, in 2010. A year later, the choreographer Mark Morris made a new “Renard,” to Stravinsky’s score.

But the trickster needn’t be a fox. In Sanskrit folklore, one of the leading tricksters is a jackal. In Native American folklore, the trickster is often a coyote; in West African folklore, a hare. African-American slave stories were the basis for Joel Chandler Harris’s “Uncle Remus” tales, starring Br’er Rabbit, who, in his incarnation in the 1946 Disney movie “Song of the South”—rarely seen today, because of the racism of its live-action sections—was one of the most popular tricksters of mid-twentieth-century popular culture. In January, “Brer Rabbit the Opera: A Funky Meditation on Gentrification,” by the performance artist Aisha Cousins, the musician Greg Tate, and the band Burnt Sugar the Arkestra Chamber, was shown as a work-in-progress at Brooklyn’s BRIC Arts. In it, Brooklyn is the “briar patch,” the place that inspires thoughts of home in insiders like Br’er Rabbit, and thoughts of slum clearance in others.

One doesn’t have to be an animal, however, to join the ranks of the tricksters. Gods can be tricksters—Hermes, in one of his aspects; Loki, from Norse mythology—and so can mortal heroes, such as Odysseus. A fictional trickster can also be a more ordinary man, like Patricia Highsmith’s Mr. Ripley. Lewis Hyde, in his widely cherished book “Trickster Makes This World: Mischief, Myth, and Art” (1998), says that artists can be tricksters. He nominates Marcel Duchamp and John Cage. It seems that just about the only kind of creature that can’t be a trickster, at least of the classic type, is a female, a fact that somebody should write a dissertation on.

Why do we like tricksters? A comforting answer is that we enjoy watching the play of intelligence. Even nicer is the idea that we like to see intelligence triumph over power. Actually, brains and brawn are not mutually exclusive—Goliath may have been a smart man—but in trickster tales they are usually opposed. Three pages into “Reynard,” Tybert the Cat, hearing Courtoys the Dog accuse Reynard of stealing his sausage, says, Not so! That was my sausage—I stole it from the miller. From then on, almost no episode passes without someone practically walking into a wall.

James Simpson, in the introduction to his translation, says that this is a great part of the pleasure of the book: its revelation of the stupidity of our fellow-creatures. He adds that Reynard’s preying on them might nevertheless trouble us if it weren’t for the fact that they are also brutal and greedy. Reynard gives them their “comeuppance,” and thereby “becomes a hero, or antihero of sorts.” I think that, for the most part, this is not true. On the contrary, the most interesting thing about “Reynard” is its moral ugliness, or, at least, lack of hope, like something out of a Russian novel. Reynard indulges in ecstasies of cruelty. When he tells the King the big lie about the treasure and gets permission to go to Rome, he decides that he’s not finished. One last thing, he says: I can’t make the trip without shoes. How about Isengrim’s? And so the wolf’s shoes are “pulled off from the claws to the sinews. . . . He didn’t move a muscle, even though his feet bled.”

There is worse: Reynard’s rape of Isengrim’s wife, Arswind. Isengrim describes it to the King. On a winter’s day, when Arswind was going past a frozen lake with Reynard, he told her that he could show her how to catch fish with her tail. All she had to do was find a hole in the ice and lower her tail into it: “So many fish would then cleave to it that four wolves wouldn’t be able to eat them all.” Arswind does as she’s instructed, and soon her tail is locked in the ice, whereupon Reynard jumps on her. As it happens, Isengrim is walking on the shore at that moment:

I could see him on my wife, shoving and sticking as men do when they’re at such work and play. What grief I suffered in my heart! I almost fainted, and cried, insofar as I was able, “Reynard, what are you doing there?”
When he spotted me so close, he jumped off and went on his way. I approached my wife in great distress. I went deep into the mud and water before I could break the ice. She endured intense pain before she could extract her tail, and even then she left a bit of the tail behind. We were likely to have lost both our lives there, because she was sobbing and crying so loudly from the pain that men of the village came out with staves and swords, with flails and pitchforks. And the women came with their spinning poles and shouted pitilessly: “Kill them! kill them!” and they struck us viciously. Never in my life was I so afraid.

Poor, stupid Arswind with her tail clamped in the ice; the fox mounting her, “shoving and sticking” as Isengrim watches; Arswind sobbing, humiliated in front of her husband, trying to yank her bleeding behind out of the ice; the two of them running through the night as the peasants chase them with pitchforks—it is almost unbearable.

Not that we expect Reynard to be good. He is a fox. But Caxton, or the narrator, isn’t, and his attitude toward Reynard’s sins is strangely inconsistent. As the book draws to an end, he tells us, righteously, that immorality such as Reynard’s is now widespread. A few pages later, he says that if we detect in ourselves any of the fox’s faults we should correct them, fast. But between these two sections we find something entirely different, an account of how, when the fox returned to his castle, after being proclaimed first counsellor, he “rejoiced that he’d prospered so well.” Now “he could promote the interests of his friends and obstruct his enemies. He also thought that he could do whatever he wanted without being blamed, as long as he was clever.” His wife and children welcome him home, and they all live happily ever after. This disguises nothing of the situation, but it also seems to show an almost psychotic dissociation from the statements of disapproval that frame it. What does the author think of his story? That question comes up repeatedly in the book, in ways both large and small. Here’s a small one, but is it so small? Reynard’s most pitiable victim, Arswind—why is she named Arswind, or Ass-wind; that is, “fart”?

Possibly what we are looking at is some sort of proto-Nietzscheanism, a joy in flouting morals. But while that might explain Reynard’s world view, it doesn’t explain the author’s. The performance is too nervous, too vacillating. Let’s imagine something else. It seems to me that if you lived in a world where you and almost everyone else had a firm belief in God but could never see God’s hand consistently at work, for good, around you, you could learn to laugh at injustice, because there would be no hope of justice in this life—only in the next.


What was Caxton’s world? One where there had been armed conflict—the Hundred Years’ War, the Wars of the Roses—for a century and a half, where religious persecution was the rule (the first auto-da-fé of the Spanish Inquisition, in Seville, was carried out the year that “Reynard” was published), where the spikes of city gates were topped with rotting heads. Some commentators, such as Hieronymus Bosch, saw this world as a place of madness. To others, like Machiavelli, it was merely a scene of mortal danger, in which—he says it straight out—one must imitate the fox. Machiavelli’s tone is steady and pitiless. But other works of the period are as baffling as “Reynard.” Consider the Unicorn Tapestries (1495-1505), in which, in a forest carpeted with daisies and marigolds, with little rabbits and birds running around, men plunge their swords into the beautiful white animal, and blood runs down its side. As in “Reynard,” you can’t figure out what you’re being told.

Why is “Reynard” being republished now? Simpson says that his version is the first readily accessible English translation to appear in almost a hundred years. I am glad that he rescued it, but I’d also like to know why no one else bothered to. There are a lot of medievalists in our universities, and Caxton’s English, which is only about a hundred years older than Shakespeare’s, isn’t difficult. (“The wulf sayd I may wel forbere your mockes and your scornes and also your felle venymous words strong theef that ye ar.”) Maybe, because of the book’s puzzling nature, people didn’t like it much, and so they left it alone. And now, perhaps, it has ridden in on the coattails of the iconoclastic trend in the modern study of fairy tales, with Disney’s prettified versions being execrated by feminists and queer-studies writers. (By Marxists, too, notably Jack Zipes, a pioneer in this campaign. His new and proudly horrifying version of the Grimms’ tales was published last year by Princeton.)

Amid the newly exposed atrocities in our folk literature, Caxton’s back-and-forthing on the subject of Reynard’s morals does not appear so shocking. One should consider, too, whether we might not be living in a time that’s comparable, or at least relatable, to Caxton’s, in the sense of strong religious feeling being juxtaposed with terrible events. The Ebola virus, Crimea, Ukraine, Syria; the Pakistani Taliban invading a school, setting a teacher on fire in front of her students, and then gunning down the children, a hundred and thirty-two of them. And that’s just last year. Worse things happened in preceding centuries, but we didn’t know about them. A beheading could be shared with only a certain number of spectators. Today, it seems, many people believe that the world is coming to an end. One of the most widely used settings for novels, movies, and television programs is a post-apocalyptic world. Measured against that, “Reynard,” laughing at cruelty, doesn’t seem so strange. 
<-------->
What do I think of life? A little and a lot. Sometimes a great deal, sometimes a very little.” So says mild, sweet-natured Pastor Niemeyer, near the end of Theodor Fontane’s novel “Effi Briest,” in response to Effi’s anguished interrogation. An artist needs to have attained great maturity, and great simplicity, in order to dare such open, sunlit evenness of tone. I can think of other writers who sound like this, but the people who immediately come to mind are filmmakers—like Ozu, or Bergman, or Satyajit Ray—whose cameras linger gently on life itself, on mundanities and fugitive details and the slow transit of things. “Isn’t life disappointing?” Kyoko asks in Ozu’s “Tokyo Story.” “Yes, it is,” comes Noriko’s seasoned reply. But life does not seem disappointing to the calmly hospitable filmmaker, whose acute selection and framing of it brings us steady joy. One of those joys is that such artists make the selection of detail seem easy—like nothing more than lingering. “The raw material of the cinema is life itself,” Ray wrote. “It is incredible that a country which has inspired so much painting and music and poetry should fail to move the film maker. He has only to keep his eyes open, and his ears.”

In an essay, Amit Chaudhuri quotes these sentences, and comments, “When Ray speaks of ‘life’ and the ‘raw material’ of life, he is speaking of a refutation of the spectacular that comprises the exotic, in favour of the mundane, the everyday, and the transfiguration of the mundane.” Chaudhuri—who was born in Calcutta in 1962, grew up in Bombay, and went to university in England, where he now lives—is really speaking of himself: he has beautifully practiced that “refutation of the spectacular” throughout his career, both as a novelist and as a critic. He has struggled, as an Indian novelist writing in English, with the long shadow of Salman Rushdie’s Booker-winning novel, “Midnight’s Children” (1981), and with the notion, established in part by the success of that book, that fictional writing about Indian life should be noisy, magical, hybrid, multivocally “exotic”—as busy as India itself. Too often, he charges, the Indian English novel since Rushdie has pursued “a mimesis of form, where the largeness of the book allegorizes the largeness of the country it represents.” (Much the same might be said of the ceaseless quest for the mimetically overfed Great American Novel.) He points out that in the Bengali tradition “the short story and novella have predominated at least as much as the novel,” and that there are plenty of Indian writers who have “hoped to suggest India by ellipsis rather than by all-inclusiveness.”


Chaudhuri has made the best case for his aesthetic preferences in his own measured, subtle, light-footed fiction. It is rich with hanging vignettes of domestic and urban life; the atmosphere is impressionistic, poetic, softly comic. In short novels like “A Strange and Sublime Address” (1991) and “Freedom Song” (1998), Chaudhuri writes superbly about family life in Calcutta, about domestic rhythms and emotional counterpoint. See how patiently he describes cleaning a house:

A house in Calcutta must be swept and scrubbed at least twice a day. Once, in the morning, Saraswati polished the floor with a moist rag, and Mamima religiously dusted the tables and chairs. The dust rose in the air in breathless clouds and seemed to evaporate and disappear. But by evening, it would condense, like moisture, and resettle on the surfaces of things. A little before sunset, a woman called Chhaya came to clean the house a second time, smiling at the boys as they waited impatiently for her to finish. She had a serious cultured face with a serious smile, the face of a kindly and understanding teacher; it was hard to believe she lived across the railway lines, in the clump of huts called the basti, from which whiffs of excrement rose on windy days.
She would sweep the floor—unending expanses, acres and acres of floor—with a short broom called the jhadu, swiping away the dust in an arc with its long tail, which reminded one of the drooping tail of some nameless, exotic bird. She would collect the dust in a corner, and here there would be an accumulation of unlikely treasure that had blown in from outside or had gathered, unnoticed, inside: a single elegant pigeon’s feather, a page lost from a book, a dead spider which ants had forgotten to carry off, the long, black, tender loops of Mamima’s and Sandeep’s mother’s hair.

In his new novel, “Odysseus Abroad” (Knopf), Chaudhuri casts a lingering eye on London in the mid-nineteen-eighties. Nothing very much “happens” in the course of its two hundred-odd pages; the book seems almost to relax itself into real time. But each page notices something freshly, or registers something true. And what is being noticed is of intense significance, because it is being seen by an outsider who also half-belongs, by virtue of a shared language and education, to an English life he is warily studying. Ananda, the novel’s protagonist, is a twenty-two-year-old Indian student and aspiring poet, living in modest squalor in Bloomsbury and studying at an unnamed institution that resembles University College London, where Chaudhuri studied. The book follows Chaudhuri’s early life in other ways: like the author, Ananda is of Bengali-Sylheti ancestry, and encounters a South African writer named Nestor Davidson, as Chaudhuri encountered the South African novelist and teacher Dan Jacobson at University College. “Odysseus Abroad” thus belongs with an earlier novel of Chaudhuri’s, the wonderful “Afternoon Raag” (1993), which explores the experiences of an Indian student at Oxford, where Chaudhuri did graduate work, after his years in London.

In London, Ananda is lonely, anxious, touchy about race and sex. He feels excluded, and partly chooses to be excluded, because it “gave his drift and insignificance meaning in his own eyes.” He self-consciously imagines himself “a young man of letters,” and greedily studies Edward Thomas, Philip Larkin, Geoffrey Hill. But what does it mean to want to write poetry in English, and, moreover, in England, when you are an Indian immigrant? On the one hand, as far as Britain is concerned, “to be a Bengali in London meant being the owner of a Bangladeshi restaurant.” On the other hand, to be a well-educated, comfortably off, English-speaking Bengali student is to arrive with an identity that is already partly British. Ananda’s parents were married in London, in 1955, and he has to remind himself that this apparently foreign city is also their city. And, yet again, this hybrid identity, one that afforded Ananda some distinction when he was growing up in India, has no currency in London. So who is he? Whittled into vulnerability by such dialectical twisting, Ananda feels bereft of identity, raw and frail:

None of the things that defined him—that he was a modern Bengali and Indian, with a cursory but proud knowledge of Bengali literature; that he wrote in English, and had spoken it much of his life; that he used to be served lettuce sandwiches as a teatime snack as a child; that in his early teenage years he’d subsisted on a diet of Agatha Christie and Erle Stanley Gardner; that he’d developed a taste for corduroys over jeans recently—almost none of this counted for anything in London, since everyone here spoke English, ate sandwiches, wore jeans or corduroys.
Ananda feels that he must often “swallow the insult of having been ruled by this nation!” But, if India was colonized by the British, then “how is it that our cities are so different? How come I’m so little prepared for here?” A timid Odysseus, with no obvious Ithaca to return to, and no familiar city to feel transiently at home in (unlike, say, Leopold Bloom in Dublin), Ananda makes his way from his flat, on Warren Street, to the university, on Gower Street, or to a bleak Chinese takeout restaurant on Euston Road, near King’s Cross, where he buys mixed fried rice or Singapore noodles. This unheroic Odysseus is unconvinced, anyway, by the great claims made in the West for Homer. He hasn’t bothered to read the Odyssey. Compared with the great Indian epics, the Ramayana and the Mahabharata, Homer seems minor, “like a Thames to the Ganges.” The Mahabharata, he thinks, is “equal to all of Shakespeare and more.” He had always thought that Sophocles rhymed with “monocles,” until he heard an English student say it properly. He concedes that he is “plainly prejudiced against the West. Then what was he doing in the West, in the English department?” Ananda is most at home not in a single place but in music, when he sits cross-legged on the floor of his flat, the tanpura on his lap, and sings ragas.

As a literary critic (and, indeed, theorist), Amit Chaudhuri has strived to identify and analyze his own kind of postcolonialism—one marked by entanglement, self-division, and mild appropriation, rather than by decisive political opposition or confident theoretical skepticism. In an essay entitled “Huge Baggy Monster: Mimetic Theories of the Indian Novel After Rushdie,” he notes that Indian hybridity need not be flagged in bright colors, and in busy polylingual prose, “with a scattering of untranslated Indian words and phrases and odd sentence constructions.” V. S. Naipaul and Nirad C. Chaudhuri, he adds, have written elegant, formal English, indistinguishable from native formal English; the hybridity of their vision is thus one of sensibility and context—the language seems the same, but has been quietly, even invisibly, borrowed, reinhabited, corrected. Something like this quiet correction occurs often in “Odysseus Abroad,” as Ananda makes his flâneur’s way through the streets of London. On roads that are doubtless eternally wet, he hears “the sabre-like hiss of passing cars”: the phrase is immaculate and exact, and yet usefully estranged, too. Ananda watches the British at play and at work: how they redden in the sunshine and drink outside pubs when the weather warms up; how volubly they conduct politics on television. The local Asian Books and Video store, the kind of shop that most of us would walk past without pausing to notice, has a “tranquil but impoverished air, like a duty-free shop in a socialist country.” Gray London teaches him to cherish light. He considers how peculiar, to the Indian ear, is Shakespeare’s line “Shall I compare thee to a summer’s day?” In India, “summer” is a dead word, and the poem had made no sense to him when he read it in Bombay. Who would want to be compared to a summer’s day in India? “Only after coming to England had he discovered the beauty of the word.”


Ananda has one relative living in London: an uncle Radhesh, his mother’s brother. The two men go walking in the city, though it is not clear whether Uncle Radhesh offers any comfort or merely makes Ananda feel even lonelier. Radhesh arrives in the novel in good time, just as it is in danger of sagging a bit. He is a vivid creation, and dominates the second half of the book—a crank, a shaman, an urban philosopher, an interesting failure and theatrical bore, he is the kind of comic character who properly overpowers good novels. Uncle Radhesh is described as “a genius” by Ananda’s father, but he has little to show for it. A brilliant student, he worked in business in London but has recently been laid off. It seems unlikely that he ever “turned down” a directorship, as he claims. For twenty-one years, he lived in a first-floor bedsit in Belsize Park, near Hampstead; since 1982, he has lived in a basement apartment in the same building.

Much of the pleasure of spending time in Radhesh’s irrepressible presence has to do with how little Chaudhuri forces anything on us—there is no obvious plot, no determined design, no faked “conflict” or other drama. The two men just go walking in North London. Chaudhuri sets Radhesh before us, and seems to follow him with a wide and loyal camera lens, the film haplessly rolling. The effect is closer to documentary than to fiction; gentle artifice—selection, pacing, occasional dialogue—hides overt artifice. The author seems to say, Here he is; what do you think? The literary pleasure is a human pleasure, as we slowly encounter this strolling, musing, forceful self. When Ananda resists Radhesh, he argues with his uncle “not to dispute him but to fend off becoming an accomplice to his vision.”

For Radhesh has many opinions and eccentricities. His breakfast consists of coffee with eleven spoonfuls of sugar, and in winter he wears an old three-piece suit over his pajamas to keep warm. In familiar Indian comic fashion, he is a keen student of his bowel movements, letting Ananda know, when he goes off to the bathroom, if he is leaving for a “small job” or a “big job.” One of the reasons, he maintains, that “I could never have become a director at Philipp Brothers . . . was because I spent too much time in the toilet. . . . The toilet holds up your day. You use a lot of time.” He is shocked by Ananda’s relative ignorance of the Bengali literary tradition, and likes to quote Tagore, with pedagogical insistence. And he has firm opinions about reincarnation. Indians and Pakistanis, he has decided, have lived many lives. His neighbor, a Pakistani nicknamed Shah because of his resemblance to the Shah of Iran, is just such an “old soul”:

“Old soul?” said Ananda. “Yes, born into the world again and again and again. Most Indians and Pakistanis are ‘old souls.’ They’ve been born so many times that they’re tired, they’ve returned to reality so often they take it for granted. If you ask Shah, ‘I gave you ten pounds yesterday for some cigarettes—what happened to the change?,’ he’ll look astonished, and say, Arrey Nandy, I gave it back to you in the afternoon, because he thinks he did. He’s been around for a very, very long time. Small inaccuracies escape him, and minor discrepancies don’t matter. Similarly, if you ask an Indian on the street, ‘Bhai, which way to Camden Town?’ he’ll give you directions even if he’s never heard of Camden Town. Old soul. Tired from having come back repeatedly. No longer mindful of detail, just living out, yet again, the duties and obligations.”
As the two make their way through London, there are wonderfully funny, lightly sketched scenes—the virginal Radhesh confiding with Ananda about his terror of catching syphilis, while the barely experienced Ananda (the beneficiary of just two sessions with a Bombay prostitute) portentously informs the older man that he lives in an innocent and idealistic world because he has never had sex: “Once you’ve had sex, the world goes grey.” Puzzling over this piece of wisdom, Radhesh bows his head. But the levity is tensioned across cavities of anxiety and displacement. We learn that the gap in Radhesh’s teeth comes from an encounter with a racist skinhead. And a gentle scene in a Hampstead tearoom—Ananda’s uncle complaining that in cosmopolitan London you can’t get “treacle tart any more”—is riven with social awkwardness and uncertainty: how to say thank you properly to the English waitress; what kind of tip is appropriate; how to keep one’s voice down, and so on.


Radhesh and Ananda represent two generations in what Naipaul called “that great movement of peoples that was to take place in the second half of the twentieth century.” Each immigrant deals with the loss of his home, and the quest for a new one, in his own way. Ananda, the serious student of English poetry, the aspirant writer, may have a greater chance of belonging, a deeper sense of possession, than Radhesh ever did—Geoffrey Hill for Ananda, only treacle tart for his uncle. But the prospect of deeper possession brings a correspondingly deeper anxiety and a more searching self-scrutiny. Who knows how things will turn out for Ananda? Cavafy ends his poem “Ithaca” with the promise that if you keep Ithaca in your mind, and do not hurry the voyage, you will arrive eventually, “rich with all that you have gained on the way.” The final section of “Odysseus Abroad” is ironically entitled “Ithaca,” and lacks Cavafy’s optimism, though it is not without solace of its own. “Ithaca” appears to be Ananda’s temporary flat in Bloomsbury, from which he set out to visit Radhesh, and to which the two men are now returning, after their urban stroll. Or perhaps it is the nearby Indian restaurant, the Gurkha Tandoori, where they have dinner before finally parting. There the waiter speaks with a Sylheti accent, and Ananda feels that “he was near home”: not home in Bombay, he thinks, because his parents don’t sound like the waiter; certainly not home in Bloomsbury. Perhaps home in some “notion of Sylhet imparted to him inadvertently by his parents and relations—as an emblem of the perennially recognisable . . . and the perennially comic. Sylhet, and Sylheti, made everybody in his family laugh with joy.” 
<-------->
Thomas  Kunkel’s “Man in Profile: Joseph Mitchell of The New Yorker” (Random House) is a book about someone who may seem, except to longtime students of this magazine, an odd and unpromising subject for a full-length biography. Mitchell was a staff writer at The New Yorker from 1938 until his death, in 1996. He had a cultlike following, but for most of his life his books were hard to find, and he never became a household name the way that, say, James Thurber or E. B. White did. He was more esteemed by his peers—revered, it’s not too much to say—than by the public: he was a writer’s writer, or even a writer’s writer’s writer. You can still see unmistakable signs of his influence—blocks of foursquare declarative sentences, a patient layering of detail, passages of precisely rendered dialogue, a tone of quiet amusement—in current New Yorker writers like Alec Wilkinson, Mark Singer, and Ian Frazier.

Mitchell practiced what he called a “wild exactitude,” and his style is hard to describe except by extensive quotation. His writing is at once spare and leisurely, lyrical and precise, funny and a little mournful. He was always on the lookout for oddness, as in his famous description of the bearded lady known as Lady Olga:


Her thick, curly beard measures thirteen and a half inches, which is the longest it has ever been. When she was young and more entranced by life under canvas, she wore it differently every year; in those days there was a variety of style in beards—she remembers the Icicle, the Indian Fighter, the Whisk Broom, and the Billy Goat—and at the beginning of a season she would ask the circus barber to trim hers in the style most popular at the moment. Since it became gray, she has worn it in the untrimmed, House of David fashion.
And he was also capable of something close to poetry, especially when describing the Hudson River:

I like to look at it in midsummer, when it is warm and dirty and drowsy, and I like to look at it in January, when it is carrying ice. I like to look at it when it is stirred up, when a northeast wind is blowing and a strong tide is running—a new-moon tide or a full-moon tide—and I like to look at it when it is slack.
If he could help it, Mitchell never wrote about anyone who was famous or newsworthy. He was drawn to people on the margins: bearded ladies, Gypsies, street preachers, Bowery bums, Mohawk steelworkers, the fishmongers at the Fulton Market. In the nineteen-thirties and forties, The New Yorker couldn’t get enough stories like this—barroom scenes were practically a subgenre—and Mitchell’s work overlaps somewhat with that of A. J. Liebling, his best friend at the magazine, and of elbow-bending New Yorker writers like John McNulty. In general, Mitchell has aged far better than they have. McNulty’s drunks no longer seem charming, and Liebling’s Broadway hustlers and Tin Pan Alley hangers-on sometimes strain credulity. Liebling saw his people as “characters,” and mined them for their colorfulness; Mitchell was genuinely interested in his subjects as human beings, remarkable because they so vividly demonstrate that one way or another we are all a little weird. But Mitchell’s world was vanishing even as he wrote about it, and it now seems almost unimaginably distant from our own. Mitchell meant his stories to be lasting, and they are, but not quite in the way he intended. They resemble a bit the drawer pulls and pickle forks and electrical insulators he liked to collect: evocative, beautifully made artifacts from a bygone age.

Mitchell’s personal life was unexceptional. He was happily married to the same woman for forty-nine years. He liked to take a drink—more so than Kunkel acknowledges—but drinking did not make him boastful or quarrelsome or self-pitying, the way it did so many writers of his generation. It just made him sadder and more nostalgic. Mitchell was shy, courtly, and private, even slightly paranoid. (Once, when I was a young New Yorker editor, he took me aside and said in a near-whisper, “I read something in the Times today. Don’t tell anyone.”) He dressed like a businessman—wing tips, white shirt, Brooks Brothers suit, fedora or, in summer, straw hat—and he kept businessman’s hours, coming in at nine every day and leaving at six. He went into his office and shut the door and sat at his desk, and for thirty-two years, starting in 1964, he did this without publishing a word. In time, Mitchell became a cautionary figure, more famous for not writing than for anything he actually wrote. At The New Yorker, people used to lurk outside his door listening for the sound of typing, and would scurry in when he left, looking for manuscript pages in the wastebasket. He would have hated to think so, but the mystery of his long silence adds an extra shine to what he did write, and it gives his biography shape and poignance.


Kunkel never quite solves the mystery. A former newspaper reporter and now a college president, he is the author of a solid biography of Harold Ross, the founder of The New Yorker, and he got to know and like Mitchell while researching it. The new book is careful, admiring, even adulatory—so uncritical that it sometimes diminishes its subject, draining the life from him. Kunkel doesn’t go in for theorizing or speculation, and there’s not a lot in his account—with one possible exception—that Mitchell fans didn’t already know or guess. In many ways, as the title implies, Kunkel’s Mitchell is a man captured in profile: an elusive figure still visible most clearly in his own writing.

Mitchell was born in Fairmont, North Carolina—farming country, on the coastal plain—in 1908. His father, Averette Nance, was a dour, humorless, self-made man who became a prosperous cotton and tobacco broker and one of the largest landowners in the area. “I very rarely feel altogether at ease with my father and haven’t since I was a child,” Mitchell wrote when he was in his sixties. “He is still able to make an offhand remark and cut me to pieces.” If you wanted to psychologize—something that Kunkel generally avoids—Mitchell’s relationship with his father would surely be the place to start.

Unlike his father, Mitchell’s mother, Elizabeth, had been to college; she was also sweeter and kinder, and from her he developed an early love of reading. To his father’s lasting disappointment, Mitchell had little desire to become a farmer, and, as a sort of fallback, he was sent to the University of North Carolina to study medicine. He was hopeless with numbers, which not only ruled out premed but made it impossible for him to get a regular degree. So for four years Mitchell stayed in Chapel Hill as a so-called “special student,” taking whatever courses he felt like, mostly literature and journalism, and writing for not just the campus publications but some of the better North Carolina newspapers. In 1929, on the eve of the stock-market crash, he decided to move to New York and try his luck with the papers there. On hearing the news, his father looked at him sadly and said, “Son, is that the best you can do, sticking your nose into other people’s business?”

New York in the nineteen-thirties was heaven for Mitchell. He quickly landed a job at the Herald Tribune, and began exploring the city on foot—all his life, he was a tireless walker—and by hanging out in the neighborhood police stations. A great noticer of things from boyhood, he became a careful listener as well, developing a matchless ear for New York speech. In 1931, Mitchell was fired from the Trib after flinging an inkwell at the publisher’s wall in a drunken fit of temper, but after a stint as a deckhand on a freighter sailing to the Soviet Union he landed a job at the World-Telegram, then thought to be “the writer’s paper. ” He soon became such a star that his name was featured on the side of the delivery trucks. He covered the Lindbergh trial; did celebrity profiles of people like Bing Crosby, Noël Coward, and George Bernard Shaw; and brought particular care to stories about New York’s oddballs—strippers, street preachers, voodoo worshippers. Many of these pieces were assembled in a 1938 collection called “My Ears Are Bent,” and they still sparkle. There were a lot of them, moreover. The writer who in later years had trouble finishing anything was sometimes turning out two or three features a day.

Cartoon
BUY THE PRINT »

Mitchell was still prolific when he moved on to The New Yorker, in September, 1938. The slowdown was gradual, and can be explained mostly by the greater length of the stories he was able to do and by the care he was putting into them, cutting and pasting, writing and rewriting. He became a much more deliberate and consciously literary writer than most of his contemporaries at the magazine. The critic Stanley Edgar Hyman first pointed out that the people Mitchell wrote about more and more resembled himself: loners, depressives, nostalgists, haunters of the waterfront, cherishers of arcane information. The characters in his pieces began to share a similar voice; they all sounded a little like Mitchell.

The self-identification became complete in Mitchell’s most famous piece and, as it happens, the last one he published, “Joe Gould’s Secret.” Joe Gould was a Village character who lived on handouts and was famous for doing seagull imitations at parties and for a multivolume compendium of conversations that he had supposedly overheard called the “Oral History of Our Time.” Mitchell published a mostly admiring and unskeptical profile of Gould in 1942, and returned to the subject in 1964 with a revisionist view. Gould was a fraud, he reveals, and the “Oral History” didn’t exist except in a handful of pages that said the same thing over and over again. Then, in a startling about-face, Mitchell not only forgives Gould—on the ground that he probably thought he had the “History” all in his head and just needed to put it down on paper—but confesses that he is guilty of the same thing, having walked around for years with a great Joycean novel of New York in his own head, a book not quite written yet so vivid in his mind that he can practically see the title page. Years later, after Mitchell stopped publishing, the critic Norman Sims asked him why he found Gould so interesting. “Because he is me,” Mitchell replied.

After Mitchell’s death, his fans, just like Salinger’s, hoped that among his papers might be piles of publishable manuscripts. It now appears that there was no such trove. There were letters, notes, diaries, and false starts, but only two and half chapters of an uncompleted memoir (all of which have recently been published in The New Yorker). Yet Mitchell’s drafts and notes allowed Kunkel to make some discoveries that may disturb and disappoint Mitchell admirers. More than we knew, or wanted to know, he made things up.

It’s no secret that the character of Mr. Hugh G. Flood—an elderly, eccentric waterfront dweller who figures in three famous Mitchell stories—was a composite. Mitchell admitted this when the Flood stories were republished in book form, in 1948, though careful readers could have surmised as much on their own. There’s the poetic name (which may be an allusion to Edwin Arlington Robinson’s “Mr. Flood’s Party”), the fact that Flood and Mitchell shared a birthday, and the fablelike quality of some of the scenes. According to Kunkel, though, there were other fabrications: the character of Cockeye Johnny Nikanov, the self-styled King of the Gypsies and the subject of a 1942 Profile so popular that Sidney Sheldon, of all people, wanted to base a musical on it, combined traits from several Gypsies Mitchell had known. (Because Mitchell wanted the rights to a Gypsy musical of his own, he told the magazine’s lawyer, “Cockeye Johnny Nikanov does not exist in real life, and never did.”) And in all likelihood Orvis Diabo, the central figure in a vivid and still revealing Mitchell piece about the Mohawks who worked in high steel, was also a construct.

Mitchell, it should be said, was not the only New Yorker writer in the thirties and forties to take liberties. Composites and invented figures were an old, if not honorable, journalistic tradition and a standby of newspaper feature writing. St. Clair McKelway, a New Yorker editor and writer who recruited Mitchell to the magazine, resorted to it, and, in some of his urban sketches, so did Liebling, barely bothering to disguise what he was up to. (His war reporting seems beyond reproach.) But this biography’s most confounding revelation is that Harold Ross, a famously nitpicking literalist and stickler for accuracy—an early advocate of magazine fact-checking—was apparently not only aware of Mitchell’s composites but encouraged him. It’s as if Kunkel had caught Jonathan Edwards secretly winking at adultery.

Ross must also have known that Mitchell regularly doctored his quotes, stitching together bits of conversation into protracted monologues and sometimes switching chronology. This, too, careful readers could have guessed—no one really talks in paragraphs as long and eloquent as some of Mitchell’s characters do—and Kunkel has found notes and revisions that prove it. As journalistic offenses go, fudging a quote is a far lesser offense than making up a character, and here again Mitchell was not alone at The New Yorker or elsewhere. Well into the era of Ross’s successor, William Shawn, one of the magazine’s hallmarks was long, unbroken quotations that now seem suspiciously articulate. Writers regularly transposed conversations, turning something said at lunch, say, into a remark made at dinner. The practice wasn’t officially condoned, but some of the magazine’s very best writers did it. There were even New Yorker writers who didn’t take notes or use tape recorders but reconstructed (or reimagined) long quotations from memory. The subjects seldom, if ever, objected, because, even if they didn’t recognize what they were quoted as saying, it usually sounded like something they might wish they had said.

Kunkel defends Mitchell’s practice on the somewhat wobbly ground that it allows for a higher truthfulness, a faithfulness that goes beyond mere factual accuracy. He also says that the results are more literary and artful than an untouched transcription might be, and in this he is certainly right. Mitchell’s best work is lovely and stirring in a way that a documentary or a recorded interview could never be. George Hunter, an elderly black man and Staten Island resident, and the subject of a story that is probably Mitchell’s masterpiece, would be less interesting if we had to read what he actually said. And yet the piece gains immeasurably from being presented as factual, an account of scenes and conversations that really took place. If we read it as fiction, which it is, in part, some of the air goes out.

As inglorious examples like Jayson Blair demonstrate, invention is often easier than reporting—you can do it without even leaving home—and requires no special talent other than nerve. But fabricators are now the exception; the rules of journalism have changed, and it’s widely accepted—or ought to be—that what appears between quotation marks is a reasonably accurate representation of what someone has said at the time he or she is supposed to have said it. It’s tempting to think that this represents a new scrupulousness and high-mindedness on the part of journalists. But it may be that more people don’t try fabricating only because now, when we’re more used to reading real speech (and when what people say is so easily picked up on smartphones and video cameras), it’s harder to get away with. Mitchell’s best defense is that he wrote what he did out of affection and empathy for his subjects, not a wish to deceive.

Mitchell appears to have felt uneasy about the composites, but to judge from Kunkel’s account he saw nothing wrong with massaging his quotes. The most likely explanation for his block wasn’t any sense of guilt but that he was a depressive by nature and became imprisoned by his own reputation—by the mythology that had grown up around him. The longer he went without producing anything, the more pressure he felt to come up with a masterpiece, and his standards by then had transcended those of mere magazine journalism.

In the late nineteen-eighties, I became Mitchell’s editor at The New Yorker, a strictly nominal post by then. (I was interviewed by Kunkel, and am quoted a few times.) Once a year, Mitchell would report to my office and bring me up to date on what he was doing. In the beginning, as I recall, he talked about working on a memoir about his family and growing up in North Carolina. Then, in what seemed to me a shift in enthusiasm, he talked about his early newspapering days in New York, and in particular about his friendship with a woman named Ann Honeycutt. Honey, as she was called, really would make a great magazine piece. She was a funny, pretty, hard-drinking Louisiana-born blonde who became something like the collective girlfriend of a whole generation of New Yorker writers and editors. Wolcott Gibbs, Thurber, and Geoffrey Hellman were all in love with her at various times; McKelway married her, though only briefly; and Liebling and Mitchell couldn’t get enough of her company. Whenever he spoke of her to me, Mitchell’s spirits seemed to lift.


According to Kunkel, Mitchell’s papers suggest that as early as the nineteen-seventies he was planning a large-scale autobiographical work that would toggle back and forth between North Carolina and New York, which is what the unfinished memoir does, though Honeycutt never makes an appearance. The existing chapters seem rather obsessive, circling around and around the same themes, and the last, incomplete section begins with an admission that Mitchell is now helplessly “living in the past.” Something has also happened to the writing. It’s mannered, self-conscious in a way that classic Mitchell never was, with the sentences growing longer and longer and more intricate. They feel like Penelope’s web, woven to stave off an ending.

In Kunkel’s telling, Mitchell’s silence is sad, not tragic. Mitchell was clearly pained and embarrassed by his failure to finish anything, but he also made peace with it. Years after he stopped publishing, he even asked for a raise—believing, probably rightly, that he had been underpaid during the years when he was producing—and Shawn gave him one. Ultimately, what happened to Mitchell is an extreme version of what happens to most writers: your powers decline, the vision of what you want to attain becomes more and more distant, the words dry up or don’t come out the way you want. In Mitchell’s case, the diminuendo was especially long, and his continued expectations for himself much higher. But he never pitied himself, or expected anyone else to. Almost to the end, he kept hoping that inspiration might yet strike. 
<-------->
In his poem “An Urban Convalescence,” James Merrill wrote of the “dull need to make some kind of house / Out of the life lived, out of the love spent.” It is a classic Merrill formulation. People are supposed to “make” houses out of blossoming aspirations and love, before busting them up in resignation and defeat. Merrill, rich since birth, as he said, “whether I liked it or not,” and one of the greatest formalist poets this country has ever produced, had seen this pattern play out in his family’s houses (especially the Orchard, his father’s Southampton summer estate, designed by McKim, Mead & White) and, painfully, in his own, in Key West; Stonington, Connecticut; and Athens, Greece. But poetry starts building when love starts dying; it erects its structures durably on emptiness. Rupture and conflict are aesthetic necessities: they turn the broken home into “The Broken Home,” Merrill’s great poem about his childhood. Though his poems can be as grand and spacious as the houses he knew, they are founded on loss. To say that Merrill is among our finest poets of interiors is merely to pick up on a pun implicit in all his work.

Langdon Hammer’s extraordinary biography of the poet, “James Merrill: Life and Art” (Knopf), suggests that “life” and “art” were for Merrill a feedback loop, not at all Yeats’s zero-sum choice between “perfection of the life, or of the work.” Merrill maintained that he sometimes wrote “lest he think / Of the reasons why he writes— / Boredom, fear, mixed vanities and shames; / Also love.” But he lived in conscious pursuit of his own “chills and fever, passions and betrayals, / Chiefly in order to make song of them.” He compared the poet off the page to an “empty hive”; he said, as many say, that he “lived to write.” Merrill, whose income came from a trust set up by his father, could afford the leisure to do anything he pleased, or nothing at all. Instead, he filled his days with work, transforming stitch by stitch the endless idle hours into measured intervals of language that, in turn, measured his days. “The years lay open before him,” Hammer writes, “a book of fresh, blank pages.”


Biographers are sometimes chastised for drowning their readers in trivia. Merrill’s work exists in part to reverse our bias against trivia. We need to know the origins and the importance of the prisms and cups and mirrors and kimonos that Merrill collected on his travels abroad and his rambles closer to home and then preserved in his poems. These constituted, as Hammer puts it, “a lexicon he used for self-expression”; the objects Merrill selected used him, in turn, “to express themselves.” His work is replete with the transfigured commonplace, bits of the world reclaimed in his daily imaginative raids: an “Atari dragonfly” on the Connecticut River, a joint smoked on a courthouse lawn, a trip to the gym, a Tyvek 

